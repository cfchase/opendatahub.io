{"componentChunkName":"component---src-templates-docs-page-tsx","path":"/docs/working-with-distributed-workloads/","result":{"data":{"allFile":{"edges":[{"node":{"childAsciidoc":{"fields":{"slug":"/docs/README/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/getting-started-with-open-data-hub/"},"sections":[{"parentId":null,"name":"Overview","level":1,"index":0,"id":"overview-for-getting-started_get-started"},{"parentId":"overview-for-getting-started_get-started","name":"Data science workflow","level":2,"index":0,"id":"_data_science_workflow"},{"parentId":"overview-for-getting-started_get-started","name":"About this guide","level":2,"index":1,"id":"_about_this_guide"},{"parentId":null,"name":"Logging in to Open Data Hub","level":1,"index":1,"id":"logging-in_get-started"},{"parentId":null,"name":"Creating a data science project","level":1,"index":2,"id":"creating-a-data-science-project_get-started"},{"parentId":null,"name":"Creating a workbench and selecting an IDE","level":1,"index":3,"id":"creating-a-workbench-select-ide_get-started"},{"parentId":"creating-a-workbench-select-ide_get-started","name":"About workbench images","level":2,"index":0,"id":"about-workbench-images_get-started"},{"parentId":"creating-a-workbench-select-ide_get-started","name":"Creating a workbench","level":2,"index":1,"id":"creating-a-project-workbench_get-started"},{"parentId":null,"name":"Next steps","level":1,"index":4,"id":"next-steps_get-started"},{"parentId":"next-steps_get-started","name":"Additional resources","level":2,"index":0,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/installing-open-data-hub/"},"sections":[{"parentId":null,"name":"Installing Open Data Hub version 2","level":1,"index":0,"id":"installing-odh-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing the Open Data Hub Operator version 2","level":2,"index":0,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing Open Data Hub components","level":2,"index":1,"id":"installing-odh-components_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Accessing the Open Data Hub dashboard","level":2,"index":2,"id":"accessing-the-odh-dashboard_installv2"},{"parentId":null,"name":"Installing Open Data Hub version 1","level":1,"index":1,"id":"installing-odh-v1_installv1"},{"parentId":"installing-odh-v1_installv1","name":"Installing the Open Data Hub Operator version 1","level":2,"index":0,"id":"installing-the-odh-operator-v1_installv1"},{"parentId":"installing-odh-v1_installv1","name":"Creating a new project for your Open Data Hub instance","level":2,"index":1,"id":"creating-a-new-project-for-your-odh-instance_installv1"},{"parentId":"installing-odh-v1_installv1","name":"Adding an Open Data Hub instance","level":2,"index":2,"id":"adding-an-odh-instance_installv1"},{"parentId":"installing-odh-v1_installv1","name":"Accessing the Open Data Hub dashboard","level":2,"index":3,"id":"accessing-the-odh-dashboard_installv1"},{"parentId":null,"name":"Configuring the Open Data Hub Operator logger","level":0,"index":2,"id":"configuring-the-odh-operator-logger_operator-log"},{"parentId":"configuring-the-odh-operator-logger_operator-log","name":"Configuring the Open Data Hub Operator logger","level":1,"index":0,"id":"configuring-the-operator-logger_operator-log"},{"parentId":"configuring-the-operator-logger_operator-log","name":"Viewing the Open Data Hub Operator log","level":2,"index":0,"id":"_viewing_the_open_data_hub_operator_log"},{"parentId":"configuring-the-odh-operator-logger_operator-log","name":"Working with certificates","level":1,"index":1,"id":"working-with-certificates_certs"},{"parentId":"working-with-certificates_certs","name":"Understanding certificates in Open Data Hub","level":2,"index":0,"id":"understanding-certificates_certs"},{"parentId":"understanding-certificates_certs","name":"How CA bundles are injected","level":3,"index":0,"id":"_how_ca_bundles_are_injected"},{"parentId":"understanding-certificates_certs","name":"How the ConfigMap is managed","level":3,"index":1,"id":"_how_the_configmap_is_managed"},{"parentId":"working-with-certificates_certs","name":"Adding a CA bundle","level":2,"index":1,"id":"adding-a-ca-bundle_certs"},{"parentId":"working-with-certificates_certs","name":"Removing a CA bundle","level":2,"index":2,"id":"removing-a-ca-bundle_certs"},{"parentId":"working-with-certificates_certs","name":"Removing a CA bundle from a namespace","level":2,"index":3,"id":"removing-a-ca-bundle-from-a-namespace_certs"},{"parentId":"working-with-certificates_certs","name":"Managing certificates","level":2,"index":4,"id":"managing-certificates_certs"},{"parentId":"working-with-certificates_certs","name":"Using self-signed certificates with Open Data Hub components","level":2,"index":5,"id":"_using_self_signed_certificates_with_open_data_hub_components"},{"parentId":"_using_self_signed_certificates_with_open_data_hub_components","name":"Using certificates with data science pipelines","level":3,"index":0,"id":"using-certificates-with-data-science-pipelines_certs"},{"parentId":"using-certificates-with-data-science-pipelines_certs","name":"Providing a CA bundle only for data science pipelines","level":4,"index":0,"id":"_providing_a_ca_bundle_only_for_data_science_pipelines"},{"parentId":"_using_self_signed_certificates_with_open_data_hub_components","name":"Using certificates with workbenches","level":3,"index":1,"id":"using-certificates-with-workbenches_certs"},{"parentId":"using-certificates-with-workbenches_certs","name":"Creating data science pipelines with Elyra and self-signed certificates","level":4,"index":0,"id":"_creating_data_science_pipelines_with_elyra_and_self_signed_certificates"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/managing-resources/"},"sections":[{"parentId":null,"name":"Customizing the dashboard","level":1,"index":0,"id":"customizing-the-dashboard"},{"parentId":"customizing-the-dashboard","name":"Editing the dashboard configuration file","level":2,"index":0,"id":"editing-the-dashboard-configuration-file_dashboard"},{"parentId":"customizing-the-dashboard","name":"Dashboard configuration options","level":2,"index":1,"id":"ref-dashboard-configuration-options_dashboard"},{"parentId":null,"name":"Managing applications that show in the dashboard","level":1,"index":1,"id":"managing-applications-that-show-in-the-dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Adding an application to the dashboard","level":2,"index":0,"id":"adding-an-application-to-the-dashboard_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Preventing users from adding applications to the dashboard","level":2,"index":1,"id":"preventing-users-from-adding-applications-to-the-dashboard_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Disabling applications connected to Open Data Hub","level":2,"index":2,"id":"disabling-applications-connected_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Showing or hiding information about enabled applications","level":2,"index":3,"id":"showing-hiding-information-about-enabled-applications_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Hiding the default Jupyter application","level":2,"index":4,"id":"hiding-the-default-jupyter-application_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Troubleshooting common problems in Jupyter for administrators","level":2,"index":5,"id":"troubleshooting-common-problems-in-jupyter-for-administrators_dashboard"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_dashboard","name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":3,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_dashboard","name":"A user&#8217;s notebook server does not start","level":3,"index":1,"id":"_a_users_notebook_server_does_not_start"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_dashboard","name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":3,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"},{"parentId":null,"name":"Managing cluster resources","level":1,"index":2,"id":"managing-cluster-resources"},{"parentId":"managing-cluster-resources","name":"Configuring the default PVC size for your cluster","level":2,"index":0,"id":"configuring-the-default-pvc-size-for-your-cluster_managing-resources"},{"parentId":"managing-cluster-resources","name":"Restoring the default PVC size for your cluster","level":2,"index":1,"id":"restoring-the-default-pvc-size-for-your-cluster_managing-resources"},{"parentId":"managing-cluster-resources","name":"Overview of accelerators","level":2,"index":2,"id":"overview-of-accelerators_managing-resources"},{"parentId":"overview-of-accelerators_managing-resources","name":"Enabling NVIDIA GPUs","level":3,"index":0,"id":"enabling-nvidia-gpus_managing-resources"},{"parentId":"overview-of-accelerators_managing-resources","name":"Enabling Intel Gaudi AI accelerators","level":3,"index":1,"id":"enabling-intel-gaudi-ai-accelerators_managing-resources"},{"parentId":"managing-cluster-resources","name":"Allocating additional resources to Open Data Hub users","level":2,"index":3,"id":"allocating-additional-resources-to-data-science-users_managing-resources"},{"parentId":"managing-cluster-resources","name":"Troubleshooting common problems with distributed workloads for administrators","level":2,"index":4,"id":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-resources"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-resources","name":"A user&#8217;s Ray cluster is in a suspended state","level":3,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-resources","name":"A user&#8217;s Ray cluster is in a failed state","level":3,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-resources","name":"A user receives a <strong>failed to call webhook</strong> error message for the CodeFlare Operator","level":3,"index":2,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-resources","name":"A user receives a <strong>failed to call webhook</strong> error message for Kueue","level":3,"index":3,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-resources","name":"A user&#8217;s Ray cluster does not start","level":3,"index":4,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-resources","name":"A user receives a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":3,"index":5,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-resources","name":"A user receives a <strong>local_queue provided does not exist</strong> error message","level":3,"index":6,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-resources","name":"A user cannot create a Ray cluster or submit jobs","level":3,"index":7,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-resources","name":"The user&#8217;s pod provisioned by Kueue is terminated before the user&#8217;s image is pulled","level":3,"index":8,"id":"_the_users_pod_provisioned_by_kueue_is_terminated_before_the_users_image_is_pulled"},{"parentId":null,"name":"Customizing component deployment resources","level":1,"index":3,"id":"customizing-component-deployment-resources_managing-resources"},{"parentId":"customizing-component-deployment-resources_managing-resources","name":"Overview of component resource customization","level":2,"index":0,"id":"overview-of-component-resource-customization_managing-resources"},{"parentId":"customizing-component-deployment-resources_managing-resources","name":"Customizing component resources","level":2,"index":1,"id":"customizing-component-resources_managing-resources"},{"parentId":"customizing-component-deployment-resources_managing-resources","name":"Disabling component resource customization","level":2,"index":2,"id":"disabling-component-resource-customization_managing-resources"},{"parentId":"customizing-component-deployment-resources_managing-resources","name":"Re-enabling component resource customization","level":2,"index":3,"id":"reenabling-component-resource-customization_managing-resources"},{"parentId":null,"name":"Managing Jupyter notebook servers","level":1,"index":4,"id":"managing-notebook-servers"},{"parentId":"managing-notebook-servers","name":"Accessing the Jupyter administration interface","level":2,"index":0,"id":"accessing-the-jupyter-administration-interface_managing-resources"},{"parentId":"managing-notebook-servers","name":"Starting notebook servers owned by other users","level":2,"index":1,"id":"starting-notebook-servers-owned-by-other-users_managing-resources"},{"parentId":"managing-notebook-servers","name":"Accessing notebook servers owned by other users","level":2,"index":2,"id":"accessing-notebook-servers-owned-by-other-users_managing-resources"},{"parentId":"managing-notebook-servers","name":"Stopping notebook servers owned by other users","level":2,"index":3,"id":"stopping-notebook-servers-owned-by-other-users_managing-resources"},{"parentId":"managing-notebook-servers","name":"Stopping idle notebooks","level":2,"index":4,"id":"stopping-idle-notebooks_managing-resources"},{"parentId":"managing-notebook-servers","name":"Adding notebook pod tolerations","level":2,"index":5,"id":"adding-notebook-pod-tolerations_managing-resources"},{"parentId":"managing-notebook-servers","name":"Configuring a custom notebook image","level":2,"index":6,"id":"configuring-a-custom-notebook-image_managing-resources"},{"parentId":null,"name":"Backing up storage data","level":1,"index":5,"id":"backing-up-storage-data_managing-resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/managing-users/"},"sections":[{"parentId":null,"name":"Adding users","level":1,"index":0,"id":"adding-users"},{"parentId":"adding-users","name":"Overview of user types and permissions","level":2,"index":0,"id":"overview-of-user-types-and-permissions_managing-users"},{"parentId":"adding-users","name":"Defining Open Data Hub administrator and user groups","level":2,"index":1,"id":"defining-data-science-admin-and-user-groups_managing-users"},{"parentId":"adding-users","name":"Adding users to specialized Open Data Hub user groups","level":2,"index":2,"id":"adding-users-to-specialized-data-science-user-groups_managing-users"},{"parentId":"adding-users","name":"Viewing Open Data Hub users","level":2,"index":3,"id":"viewing-data-science-users_managing-users"},{"parentId":null,"name":"Deleting users and their resources","level":1,"index":1,"id":"deleting-users"},{"parentId":"deleting-users","name":"About deleting users and their resources","level":2,"index":0,"id":"about-deleting-users-and-resources_managing-users"},{"parentId":"deleting-users","name":"Backing up storage data","level":2,"index":1,"id":"backing-up-storage-data_managing-users"},{"parentId":"deleting-users","name":"Stopping notebook servers owned by other users","level":2,"index":2,"id":"stopping-notebook-servers-owned-by-other-users_managing-users"},{"parentId":"deleting-users","name":"Revoking user access to Jupyter","level":2,"index":3,"id":"revoking-user-access-to-jupyter_managing-users"},{"parentId":"deleting-users","name":"Cleaning up after deleting users","level":2,"index":4,"id":"cleaning-up-after-deleting-users_managing-users"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/serving-models/"},"sections":[{"parentId":null,"name":"About model serving","level":1,"index":0,"id":"about-model-serving_about-model-serving"},{"parentId":null,"name":"Serving small and medium-sized models","level":1,"index":1,"id":"serving-small-and-medium-sized-models_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Configuring model servers","level":2,"index":0,"id":"_configuring_model_servers"},{"parentId":"_configuring_model_servers","name":"Enabling the multi-model serving platform","level":3,"index":0,"id":"enabling-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a custom model-serving runtime for the multi-model serving platform","level":3,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a model server for the multi-model serving platform","level":3,"index":2,"id":"adding-a-model-server-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Deleting a model server","level":3,"index":3,"id":"deleting-a-model-server_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Working with deployed models","level":2,"index":1,"id":"_working_with_deployed_models"},{"parentId":"_working_with_deployed_models","name":"Deploying a model by using the multi-model serving platform","level":3,"index":0,"id":"deploying-a-model-using-the-multi-model-serving-platform_model-serving"},{"parentId":"_working_with_deployed_models","name":"Viewing a deployed model","level":3,"index":1,"id":"viewing-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Updating the deployment properties of a deployed model","level":3,"index":2,"id":"updating-the-deployment-properties-of-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Deleting a deployed model","level":3,"index":3,"id":"deleting-a-deployed-model_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Configuring monitoring for the multi-model serving platform","level":2,"index":2,"id":"configuring-monitoring-for-the-multi-model-serving-platform_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Viewing model-serving runtime metrics for the multi-model serving platform","level":2,"index":3,"id":"viewing-metrics-for-the-multi-model-serving-platform_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Monitoring model performance","level":2,"index":4,"id":"_monitoring_model_performance"},{"parentId":"_monitoring_model_performance","name":"Viewing performance metrics for all models on a model server","level":3,"index":0,"id":"viewing-performance-metrics-for-model-server_model-serving"},{"parentId":"_monitoring_model_performance","name":"Viewing HTTP request metrics for a deployed model","level":3,"index":1,"id":"viewing-http-request-metrics-for-a-deployed-model_model-serving"},{"parentId":null,"name":"Serving large models","level":1,"index":2,"id":"serving-large-models_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"About the single-model serving platform","level":2,"index":0,"id":"about-the-single-model-serving-platform_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"About KServe deployment modes","level":2,"index":1,"id":"about-kserve-deployment-modes_serving-large-models"},{"parentId":"about-kserve-deployment-modes_serving-large-models","name":"Serverless mode","level":3,"index":0,"id":"_serverless_mode"},{"parentId":"about-kserve-deployment-modes_serving-large-models","name":"Raw deployment mode","level":3,"index":1,"id":"_raw_deployment_mode"},{"parentId":"serving-large-models_serving-large-models","name":"Installing KServe","level":2,"index":2,"id":"installing-kserve_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Deploying models by using the single-model serving platform","level":2,"index":3,"id":"deploying-models-using-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Enabling the single-model serving platform","level":3,"index":0,"id":"enabling-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Adding a custom model-serving runtime for the single-model serving platform","level":3,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Deploying models on the single-model serving platform","level":3,"index":2,"id":"deploying-models-on-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Accessing the inference endpoint for a deployed model","level":3,"index":3,"id":"accessing-inference-endpoint-for-deployed-model_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Configuring monitoring for the single-model serving platform","level":2,"index":4,"id":"configuring-monitoring-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Viewing model-serving runtime metrics for the single-model serving platform","level":2,"index":5,"id":"viewing-metrics-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Monitoring model performance","level":2,"index":6,"id":"_monitoring_model_performance_2"},{"parentId":"_monitoring_model_performance_2","name":"Viewing performance metrics for a deployed model","level":3,"index":0,"id":"viewing-performance-metrics-for-deployed-model_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Optimizing model-serving runtimes","level":2,"index":7,"id":"_optimizing_model_serving_runtimes"},{"parentId":"_optimizing_model_serving_runtimes","name":"Optimizing the vLLM model-serving runtime","level":3,"index":0,"id":"optimizing-the-vllm-runtime_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Performance tuning on the single-model serving platform","level":2,"index":8,"id":"_performance_tuning_on_the_single_model_serving_platform"},{"parentId":"_performance_tuning_on_the_single_model_serving_platform","name":"Resolving CUDA out-of-memory errors","level":3,"index":0,"id":"resolving-cuda-oom-errors-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Supported model-serving runtimes","level":2,"index":9,"id":"supported-model-serving-runtimes_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Inference endpoints","level":2,"index":10,"id":"inference-endpoints_serving-large-models"},{"parentId":"inference-endpoints_serving-large-models","name":"Example commands","level":3,"index":0,"id":"_example_commands"},{"parentId":"inference-endpoints_serving-large-models","name":"Additional resources","level":3,"index":1,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/monitoring-data-science-models/"},"sections":[{"parentId":null,"name":"Overview of model monitoring","level":1,"index":0,"id":"overview-of-model-monitoring_monitor"},{"parentId":null,"name":"Configuring TrustyAI","level":1,"index":1,"id":"configuring-trustyai_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Configuring monitoring for the multi-model serving platform","level":2,"index":0,"id":"configuring-monitoring-for-the-multi-model-serving-platform_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Enabling the TrustyAI component","level":2,"index":1,"id":"enabling-trustyai-component_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Configuring TrustyAI with a database","level":2,"index":2,"id":"configuring-trustyai-with-a-database_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Installing the TrustyAI service for a project","level":2,"index":3,"id":"installing-trustyai-service_monitor"},{"parentId":"installing-trustyai-service_monitor","name":"Installing the TrustyAI service by using the dashboard","level":3,"index":0,"id":"installing-trustyai-service-using-dashboard_monitor"},{"parentId":"installing-trustyai-service_monitor","name":"Installing the TrustyAI service by using the CLI","level":3,"index":1,"id":"installing-trustyai-service-using-cli_monitor"},{"parentId":null,"name":"Setting up TrustyAI for your project","level":1,"index":2,"id":"setting-up-trustyai-for-your-project_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Authenticating the TrustyAI service","level":2,"index":0,"id":"authenticating-trustyai-service_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Sending training data to TrustyAI","level":2,"index":1,"id":"sending-training-data-to-trustyai_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Labeling data fields","level":2,"index":2,"id":"labeling-data-fields_monitor"},{"parentId":null,"name":"Monitoring model bias","level":1,"index":3,"id":"monitoring-model-bias_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Creating a bias metric","level":2,"index":0,"id":"creating-a-bias-metric_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the dashboard","level":3,"index":0,"id":"creating-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the CLI","level":3,"index":1,"id":"creating-a-bias-metric-using-cli_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Duplicating a bias metric","level":3,"index":2,"id":"duplicating-a-bias-metric_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Deleting a bias metric","level":2,"index":1,"id":"deleting-a-bias-metric_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the dashboard","level":3,"index":0,"id":"deleting-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the CLI","level":3,"index":1,"id":"deleting-a-bias-metric-using-cli_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Viewing bias metrics for a model","level":2,"index":2,"id":"viewing-bias-metrics_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Supported bias metrics","level":2,"index":3,"id":"supported-bias-metrics_bias-monitoring"},{"parentId":null,"name":"Monitoring data drift","level":1,"index":4,"id":"monitoring-data-drift_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Creating a drift metric","level":2,"index":0,"id":"creating-a-drift-metric_drift-monitoring"},{"parentId":"creating-a-drift-metric_drift-monitoring","name":"Creating a drift metric by using the CLI","level":3,"index":0,"id":"creating-a-drift-metric-using-cli_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Deleting a drift metric by using the CLI","level":2,"index":1,"id":"deleting-a-drift-metric-using-cli_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Viewing data drift metrics for a model","level":2,"index":2,"id":"viewing-drift-metrics_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Supported drift metrics","level":2,"index":3,"id":"supported-drift-metrics_drift-monitoring"},{"parentId":null,"name":"Using explainability","level":1,"index":5,"id":"using-explainability_explainers"},{"parentId":"using-explainability_explainers","name":"Requesting a LIME explanation","level":2,"index":0,"id":"requesting-a-lime-explanation_explainers"},{"parentId":"requesting-a-lime-explanation_explainers","name":"Requesting a LIME explanation by using the CLI","level":3,"index":0,"id":"requesting-a-lime-explanation-using-CLI_explainers"},{"parentId":"using-explainability_explainers","name":"Requesting a SHAP explanation","level":2,"index":1,"id":"requesting-a-shap-explanation_explainers"},{"parentId":"requesting-a-shap-explanation_explainers","name":"Requesting a SHAP explanation by using the CLI","level":3,"index":0,"id":"requesting-a-shap-explanation-using-CLI_explainers"},{"parentId":"using-explainability_explainers","name":"Supported explainers","level":2,"index":2,"id":"supported-explainers_explainers"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/upgrading-open-data-hub/"},"sections":[{"parentId":null,"name":"Overview of upgrading Open Data Hub","level":1,"index":0,"id":"overview-of-upgrading-odh_upgrade"},{"parentId":null,"name":"Requirements for upgrading Open Data Hub","level":1,"index":1,"id":"requirements-for-upgrading-odh_upgrade"},{"parentId":null,"name":"Upgrading Open Data Hub version 1 to version 2","level":1,"index":2,"id":"upgrading-odh-v1-to-v2_upgradev1"},{"parentId":"upgrading-odh-v1-to-v2_upgradev1","name":"Upgrading the Open Data Hub Operator version 1","level":2,"index":0,"id":"upgrading-the-odh-operator-v1_upgradev1"},{"parentId":"upgrading-odh-v1-to-v2_upgradev1","name":"Installing Open Data Hub components","level":2,"index":1,"id":"installing-odh-components_upgradev1"},{"parentId":"upgrading-odh-v1-to-v2_upgradev1","name":"Accessing the Open Data Hub dashboard","level":2,"index":2,"id":"accessing-the-odh-dashboard_upgradev1"},{"parentId":null,"name":"Upgrading Open Data Hub version 2.0 to version 2.2","level":1,"index":3,"id":"upgrading-odh-v2_upgradev2"},{"parentId":"upgrading-odh-v2_upgradev2","name":"Installing Open Data Hub version 2","level":2,"index":0,"id":"installing-odh-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing the Open Data Hub Operator version 2","level":3,"index":0,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing Open Data Hub components","level":3,"index":1,"id":"installing-odh-components_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Accessing the Open Data Hub dashboard","level":3,"index":2,"id":"accessing-the-odh-dashboard_installv2"},{"parentId":"upgrading-odh-v2_upgradev2","name":"Adding a CA bundle after upgrading","level":2,"index":1,"id":"adding-a-ca-bundle-after-upgrading_upgradev2"},{"parentId":null,"name":"Adding a CA bundle after upgrading","level":1,"index":4,"id":"adding-a-ca-bundle-after-upgrading_upgradev2"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-in-your-data-science-ide/"},"sections":[{"parentId":null,"name":"Accessing your workbench IDE","level":1,"index":0,"id":"accessing-your-workbench-ide_ide"},{"parentId":null,"name":"Working in JupyterLab","level":1,"index":1,"id":"_working_in_jupyterlab"},{"parentId":"_working_in_jupyterlab","name":"Creating and importing notebooks","level":2,"index":0,"id":"creating-and-importing-notebooks_ide"},{"parentId":"creating-and-importing-notebooks_ide","name":"Creating a new notebook","level":3,"index":0,"id":"creating-a-new-notebook_ide"},{"parentId":"creating-and-importing-notebooks_ide","name":"Uploading an existing notebook file from local storage","level":3,"index":1,"id":"uploading-an-existing-notebook-file-from-local-storage_ide"},{"parentId":"creating-and-importing-notebooks_ide","name":"Additional resources","level":3,"index":2,"id":"_additional_resources"},{"parentId":"_working_in_jupyterlab","name":"Collaborating on notebooks by using Git","level":2,"index":1,"id":"collaborating-on-notebooks-by-using-git_ide"},{"parentId":"collaborating-on-notebooks-by-using-git_ide","name":"Uploading an existing notebook file from a Git repository by using JupyterLab","level":3,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_ide"},{"parentId":"collaborating-on-notebooks-by-using-git_ide","name":"Uploading an existing notebook file from a Git repository by using the command line interface","level":3,"index":1,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_ide"},{"parentId":"collaborating-on-notebooks-by-using-git_ide","name":"Updating your project with changes from a remote Git repository","level":3,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_ide"},{"parentId":"collaborating-on-notebooks-by-using-git_ide","name":"Pushing project changes to a Git repository","level":3,"index":3,"id":"pushing-project-changes-to-a-git-repository_ide"},{"parentId":"_working_in_jupyterlab","name":"Managing Python packages","level":2,"index":2,"id":"managing-python-packages_ide"},{"parentId":"managing-python-packages_ide","name":"Viewing Python packages installed on your notebook server","level":3,"index":0,"id":"viewing-python-packages-installed-on-your-notebook-server_ide"},{"parentId":"managing-python-packages_ide","name":"Installing Python packages on your notebook server","level":3,"index":1,"id":"installing-python-packages-on-your-notebook-server_ide"},{"parentId":"_working_in_jupyterlab","name":"Troubleshooting common problems in Jupyter for users","level":2,"index":3,"id":"troubleshooting-common-problems-in-jupyter-for-users_ide"},{"parentId":null,"name":"Working in code-server","level":1,"index":2,"id":"working-in-code-server_ide"},{"parentId":"working-in-code-server_ide","name":"Installing extensions with code-server","level":2,"index":0,"id":"_installing_extensions_with_code_server"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-on-data-science-projects/"},"sections":[{"parentId":null,"name":"Using data science projects","level":1,"index":0,"id":"using-data-science-projects_projects"},{"parentId":"using-data-science-projects_projects","name":"Creating a data science project","level":2,"index":0,"id":"creating-a-data-science-project_projects"},{"parentId":"using-data-science-projects_projects","name":"Updating a data science project","level":2,"index":1,"id":"updating-a-data-science-project_projects"},{"parentId":"using-data-science-projects_projects","name":"Deleting a data science project","level":2,"index":2,"id":"deleting-a-data-science-project_projects"},{"parentId":null,"name":"Using project workbenches","level":1,"index":1,"id":"using-project-workbenches_projects"},{"parentId":"using-project-workbenches_projects","name":"Creating a workbench and selecting an IDE","level":2,"index":0,"id":"creating-a-workbench-select-ide_projects"},{"parentId":"creating-a-workbench-select-ide_projects","name":"About workbench images","level":3,"index":0,"id":"about-workbench-images_projects"},{"parentId":"creating-a-workbench-select-ide_projects","name":"Creating a workbench","level":3,"index":1,"id":"creating-a-project-workbench_projects"},{"parentId":"using-project-workbenches_projects","name":"Starting a workbench","level":2,"index":1,"id":"starting-a-workbench_projects"},{"parentId":"using-project-workbenches_projects","name":"Updating a project workbench","level":2,"index":2,"id":"updating-a-project-workbench_projects"},{"parentId":"using-project-workbenches_projects","name":"Deleting a workbench from a data science project","level":2,"index":3,"id":"deleting-a-workbench-from-a-data-science-project_projects"},{"parentId":null,"name":"Using data connections","level":1,"index":2,"id":"using-data-connections_projects"},{"parentId":"using-data-connections_projects","name":"Adding a data connection to your data science project","level":2,"index":0,"id":"adding-a-data-connection-to-your-data-science-project_projects"},{"parentId":"using-data-connections_projects","name":"Deleting a data connection","level":2,"index":1,"id":"deleting-a-data-connection_projects"},{"parentId":"using-data-connections_projects","name":"Updating a connected data source","level":2,"index":2,"id":"updating-a-connected-data-source_projects"},{"parentId":null,"name":"Configuring cluster storage","level":1,"index":3,"id":"configuring-cluster-storage_projects"},{"parentId":"configuring-cluster-storage_projects","name":"Adding cluster storage to your data science project","level":2,"index":0,"id":"adding-cluster-storage-to-your-data-science-project_projects"},{"parentId":"configuring-cluster-storage_projects","name":"Updating cluster storage","level":2,"index":1,"id":"updating-cluster-storage_projects"},{"parentId":"configuring-cluster-storage_projects","name":"Deleting cluster storage from a data science project","level":2,"index":2,"id":"deleting-cluster-storage-from-a-data-science-project_projects"},{"parentId":null,"name":"Managing access to data science projects","level":1,"index":4,"id":"managing-access-to-data-science-projects_projects"},{"parentId":"managing-access-to-data-science-projects_projects","name":"Configuring access to a data science project","level":2,"index":0,"id":"configuring-access-to-a-data-science-project_projects"},{"parentId":"managing-access-to-data-science-projects_projects","name":"Sharing access to a data science project","level":2,"index":1,"id":"sharing-access-to-a-data-science-project_projects"},{"parentId":"managing-access-to-data-science-projects_projects","name":"Updating access to a data science project","level":2,"index":2,"id":"updating-access-to-a-data-science-project_projects"},{"parentId":"managing-access-to-data-science-projects_projects","name":"Removing access to a data science project","level":2,"index":3,"id":"removing-access-to-a-data-science-project_projects"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-accelerators/"},"sections":[{"parentId":null,"name":"Overview of accelerators","level":1,"index":0,"id":"overview-of-accelerators_accelerators"},{"parentId":null,"name":"Enabling NVIDIA GPUs","level":1,"index":1,"id":"enabling-nvidia-gpus_accelerators"},{"parentId":null,"name":"Intel Gaudi AI Accelerator integration","level":1,"index":2,"id":"intel-gaudi-ai-accelerator-integration_accelerators"},{"parentId":"intel-gaudi-ai-accelerator-integration_accelerators","name":"Enabling Intel Gaudi AI accelerators","level":2,"index":0,"id":"enabling-intel-gaudi-ai-accelerators_accelerators"},{"parentId":null,"name":"Working with accelerator profiles","level":1,"index":3,"id":"working-with-accelerator-profiles_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Creating an accelerator profile","level":2,"index":0,"id":"creating-an-accelerator-profile_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Updating an accelerator profile","level":2,"index":1,"id":"updating-an-accelerator-profile_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Deleting an accelerator profile","level":2,"index":2,"id":"deleting-an-accelerator-profile_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Viewing accelerator profiles","level":2,"index":3,"id":"viewing-accelerator-profiles_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Configuring a recommended accelerator for notebook images","level":2,"index":4,"id":"configuring-a-recommended-accelerator-for-notebook-images_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Configuring a recommended accelerator for serving runtimes","level":2,"index":5,"id":"configuring-a-recommended-accelerator-for-serving-runtimes_accelerators"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-connected-applications/"},"sections":[{"parentId":null,"name":"Viewing applications that are connected to Open Data Hub","level":1,"index":0,"id":"viewing-connected-applications_connected-apps"},{"parentId":null,"name":"Enabling applications that are connected to Open Data Hub","level":1,"index":1,"id":"enabling-applications-connected_connected-apps"},{"parentId":null,"name":"Removing disabled applications from the dashboard","level":1,"index":2,"id":"removing-disabled-applications_connected-apps"},{"parentId":null,"name":"Using the Jupyter application","level":1,"index":3,"id":"_using_the_jupyter_application"},{"parentId":"_using_the_jupyter_application","name":"Starting a Jupyter notebook server","level":2,"index":0,"id":"starting-a-jupyter-notebook-server_connected-apps"},{"parentId":"_using_the_jupyter_application","name":"Creating and importing notebooks","level":2,"index":1,"id":"creating-and-importing-notebooks_connected-apps"},{"parentId":"creating-and-importing-notebooks_connected-apps","name":"Creating a new notebook","level":3,"index":0,"id":"creating-a-new-notebook_connected-apps"},{"parentId":"creating-and-importing-notebooks_connected-apps","name":"Uploading an existing notebook file from local storage","level":3,"index":1,"id":"uploading-an-existing-notebook-file-from-local-storage_connected-apps"},{"parentId":"creating-and-importing-notebooks_connected-apps","name":"Additional resources","level":3,"index":2,"id":"_additional_resources"},{"parentId":"_using_the_jupyter_application","name":"Collaborating on notebooks by using Git","level":2,"index":2,"id":"collaborating-on-notebooks-by-using-git_connected-apps"},{"parentId":"collaborating-on-notebooks-by-using-git_connected-apps","name":"Uploading an existing notebook file from a Git repository by using JupyterLab","level":3,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_connected-apps"},{"parentId":"collaborating-on-notebooks-by-using-git_connected-apps","name":"Uploading an existing notebook file from a Git repository by using the command line interface","level":3,"index":1,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_connected-apps"},{"parentId":"collaborating-on-notebooks-by-using-git_connected-apps","name":"Updating your project with changes from a remote Git repository","level":3,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_connected-apps"},{"parentId":"collaborating-on-notebooks-by-using-git_connected-apps","name":"Pushing project changes to a Git repository","level":3,"index":3,"id":"pushing-project-changes-to-a-git-repository_connected-apps"},{"parentId":"_using_the_jupyter_application","name":"Managing Python packages","level":2,"index":3,"id":"managing-python-packages_connected-apps"},{"parentId":"managing-python-packages_connected-apps","name":"Viewing Python packages installed on your notebook server","level":3,"index":0,"id":"viewing-python-packages-installed-on-your-notebook-server_connected-apps"},{"parentId":"managing-python-packages_connected-apps","name":"Installing Python packages on your notebook server","level":3,"index":1,"id":"installing-python-packages-on-your-notebook-server_connected-apps"},{"parentId":"_using_the_jupyter_application","name":"Updating notebook server settings by restarting your server","level":2,"index":4,"id":"updating-notebook-server-settings-by-restarting-your-server_connected-apps"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-distributed-workloads/"},"sections":[{"parentId":null,"name":"Overview of distributed workloads","level":1,"index":0,"id":"overview-of-distributed-workloads_distributed-workloads"},{"parentId":"overview-of-distributed-workloads_distributed-workloads","name":"Overview of Kueue resources","level":2,"index":0,"id":"overview-of-kueue-resources_distributed-workloads"},{"parentId":"overview-of-kueue-resources_distributed-workloads","name":"Resource flavor","level":3,"index":0,"id":"_resource_flavor"},{"parentId":"overview-of-kueue-resources_distributed-workloads","name":"Cluster queue","level":3,"index":1,"id":"_cluster_queue"},{"parentId":"overview-of-kueue-resources_distributed-workloads","name":"Local queue","level":3,"index":2,"id":"_local_queue"},{"parentId":null,"name":"Configuring distributed workloads","level":1,"index":1,"id":"configuring-distributed-workloads_distributed-workloads"},{"parentId":"configuring-distributed-workloads_distributed-workloads","name":"Configuring the distributed workloads components","level":2,"index":0,"id":"configuring-the-distributed-workloads-components_distributed-workloads"},{"parentId":"configuring-distributed-workloads_distributed-workloads","name":"Configuring quota management for distributed workloads","level":2,"index":1,"id":"configuring-quota-management-for-distributed-workloads_distributed-workloads"},{"parentId":"configuring-distributed-workloads_distributed-workloads","name":"Configuring the CodeFlare Operator","level":2,"index":2,"id":"configuring-the-codeflare-operator_distributed-workloads"},{"parentId":null,"name":"Running distributed workloads","level":1,"index":2,"id":"running-distributed-workloads_distributed-workloads"},{"parentId":"running-distributed-workloads_distributed-workloads","name":"Running distributed data science workloads from notebooks","level":2,"index":0,"id":"running-distributed-data-science-workloads-from-notebooks_distributed-workloads"},{"parentId":"running-distributed-workloads_distributed-workloads","name":"Running distributed data science workloads from data science pipelines","level":2,"index":1,"id":"running-distributed-data-science-workloads-from-ds-pipelines_distributed-workloads"},{"parentId":null,"name":"Monitoring distributed workloads","level":1,"index":3,"id":"monitoring-distributed-workloads_distributed-workloads"},{"parentId":"monitoring-distributed-workloads_distributed-workloads","name":"Viewing project metrics for distributed workloads","level":2,"index":0,"id":"viewing-project-metrics-for-distributed-workloads_distributed-workloads"},{"parentId":"monitoring-distributed-workloads_distributed-workloads","name":"Viewing the status of distributed workloads","level":2,"index":1,"id":"viewing-the-status-of-distributed-workloads_distributed-workloads"},{"parentId":null,"name":"Tuning a model by using the Training Operator","level":1,"index":4,"id":"tuning-a-model-by-using-the-training-operator_distributed-workloads"},{"parentId":"tuning-a-model-by-using-the-training-operator_distributed-workloads","name":"Configuring the Training Operator permissions when not using Kueue","level":2,"index":0,"id":"configuring-the-training-operator-permissions-when-not-using-kueue_distributed-workloads"},{"parentId":"tuning-a-model-by-using-the-training-operator_distributed-workloads","name":"Configuring the training job","level":2,"index":1,"id":"configuring-the-training-job_distributed-workloads"},{"parentId":"tuning-a-model-by-using-the-training-operator_distributed-workloads","name":"Running the training job","level":2,"index":2,"id":"running-the-training-job_distributed-workloads"},{"parentId":"tuning-a-model-by-using-the-training-operator_distributed-workloads","name":"Monitoring the training job","level":2,"index":3,"id":"monitoring-the-training-job_distributed-workloads"},{"parentId":null,"name":"Troubleshooting common problems with distributed workloads for users","level":1,"index":5,"id":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My Ray cluster is in a suspended state","level":2,"index":0,"id":"_my_ray_cluster_is_in_a_suspended_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My Ray cluster is in a failed state","level":2,"index":1,"id":"_my_ray_cluster_is_in_a_failed_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a <strong>failed to call webhook</strong> error message for the CodeFlare Operator","level":2,"index":2,"id":"_i_see_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a <strong>failed to call webhook</strong> error message for Kueue","level":2,"index":3,"id":"_i_see_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My Ray cluster doesn&#8217;t start","level":2,"index":4,"id":"_my_ray_cluster_doesnt_start"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":2,"index":5,"id":"_i_see_a_default_local_queue_not_found_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a <strong>local_queue provided does not exist</strong> error message","level":2,"index":6,"id":"_i_see_a_local_queue_provided_does_not_exist_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I cannot create a Ray cluster or submit jobs","level":2,"index":7,"id":"_i_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My pod provisioned by Kueue is terminated before my image is pulled","level":2,"index":8,"id":"_my_pod_provisioned_by_kueue_is_terminated_before_my_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/_artifacts/document-attributes-global/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Enabling data science pipelines 2.0","level":1,"index":0,"id":"enabling-data-science-pipelines-2_ds-pipelines"},{"parentId":"enabling-data-science-pipelines-2_ds-pipelines","name":"Installing Open Data Hub with data science pipelines 2.0","level":2,"index":0,"id":"_installing_open_data_hub_with_data_science_pipelines_2_0"},{"parentId":"enabling-data-science-pipelines-2_ds-pipelines","name":"Upgrading to data science pipelines 2.0","level":2,"index":1,"id":"_upgrading_to_data_science_pipelines_2_0"},{"parentId":null,"name":"Managing data science pipelines","level":1,"index":1,"id":"managing-data-science-pipelines_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Configuring a pipeline server","level":2,"index":0,"id":"configuring-a-pipeline-server_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Defining a pipeline","level":2,"index":1,"id":"defining-a-pipeline_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Importing a data science pipeline","level":2,"index":2,"id":"importing-a-data-science-pipeline_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Deleting a data science pipeline","level":2,"index":3,"id":"deleting-a-data-science-pipeline_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Deleting a pipeline server","level":2,"index":4,"id":"deleting-a-pipeline-server_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Viewing the details of a pipeline server","level":2,"index":5,"id":"viewing-the-details-of-a-pipeline-server_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Viewing existing pipelines","level":2,"index":6,"id":"viewing-existing-pipelines_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Overview of pipeline versions","level":2,"index":7,"id":"overview-of-pipeline-versions_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Uploading a pipeline version","level":2,"index":8,"id":"uploading-a-pipeline-version_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Deleting a pipeline version","level":2,"index":9,"id":"deleting-a-pipeline-version_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Viewing the details of a pipeline version","level":2,"index":10,"id":"viewing-the-details-of-a-pipeline-version_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Downloading a data science pipeline version","level":2,"index":11,"id":"downloading-a-data-science-pipeline-version_ds-pipelines"},{"parentId":null,"name":"Managing pipeline experiments","level":1,"index":2,"id":"managing-pipeline-experiments_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Overview of pipeline experiments","level":2,"index":0,"id":"overview-of-pipeline-experiments_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Creating a pipeline experiment","level":2,"index":1,"id":"creating-a-pipeline-experiment_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Archiving a pipeline experiment","level":2,"index":2,"id":"archiving-a-pipeline-experiment_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Deleting an archived pipeline experiment","level":2,"index":3,"id":"deleting-an-archived-pipeline-experiment_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Restoring an archived pipeline experiment","level":2,"index":4,"id":"restoring-an-archived-pipeline-experiment_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Viewing pipeline task executions","level":2,"index":5,"id":"viewing-pipeline-task-executions_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Viewing pipeline artifacts","level":2,"index":6,"id":"viewing-pipeline-artifacts_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Comparing runs","level":2,"index":7,"id":"comparing-runs_ds-pipelines"},{"parentId":null,"name":"Managing pipeline runs","level":1,"index":3,"id":"managing-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Overview of pipeline runs","level":2,"index":0,"id":"overview-of-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Storing data with data science pipelines","level":2,"index":1,"id":"storing-data-with-data-science-pipelines_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Viewing active pipeline runs","level":2,"index":2,"id":"viewing-active-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Executing a pipeline run","level":2,"index":3,"id":"executing-a-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Stopping an active pipeline run","level":2,"index":4,"id":"stopping-an-active-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Duplicating an active pipeline run","level":2,"index":5,"id":"duplicating-an-active-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Viewing scheduled pipeline runs","level":2,"index":6,"id":"viewing-scheduled-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Scheduling a pipeline run using a cron job","level":2,"index":7,"id":"scheduling-a-pipeline-run-using-a-cron-job_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Scheduling a pipeline run","level":2,"index":8,"id":"scheduling-a-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Duplicating a scheduled pipeline run","level":2,"index":9,"id":"duplicating-a-scheduled-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Deleting a scheduled pipeline run","level":2,"index":10,"id":"deleting-a-scheduled-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Viewing the details of a pipeline run","level":2,"index":11,"id":"viewing-the-details-of-a-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Viewing archived pipeline runs","level":2,"index":12,"id":"viewing-archived-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Archiving a pipeline run","level":2,"index":13,"id":"archiving-a-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Restoring an archived pipeline run","level":2,"index":14,"id":"restoring-an-archived-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Deleting an archived pipeline run","level":2,"index":15,"id":"deleting-an-archived-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Duplicating an archived pipeline run","level":2,"index":16,"id":"duplicating-an-archived-pipeline-run_ds-pipelines"},{"parentId":null,"name":"Working with pipeline logs","level":1,"index":4,"id":"working-with-pipeline-logs_ds-pipelines"},{"parentId":"working-with-pipeline-logs_ds-pipelines","name":"About pipeline logs","level":2,"index":0,"id":"about-pipeline-logs_ds-pipelines"},{"parentId":"working-with-pipeline-logs_ds-pipelines","name":"Viewing pipeline step logs","level":2,"index":1,"id":"viewing-pipeline-step-logs_ds-pipelines"},{"parentId":"working-with-pipeline-logs_ds-pipelines","name":"Downloading pipeline step logs","level":2,"index":2,"id":"downloading-pipeline-step-logs_ds-pipelines"},{"parentId":null,"name":"Working with pipelines in JupyterLab","level":1,"index":5,"id":"working-with-pipelines-in-jupyterlab_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Overview of pipelines in JupyterLab","level":2,"index":0,"id":"overview-of-pipelines-in-jupyterlab_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Accessing the pipeline editor","level":2,"index":1,"id":"accessing-the-pipeline-editor_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Creating a runtime configuration","level":2,"index":2,"id":"creating-a-runtime-configuration_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Updating a runtime configuration","level":2,"index":3,"id":"updating-a-runtime-configuration_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Deleting a runtime configuration","level":2,"index":4,"id":"deleting-a-runtime-configuration_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Duplicating a runtime configuration","level":2,"index":5,"id":"duplicating-a-runtime-configuration_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Running a pipeline in JupyterLab","level":2,"index":6,"id":"running-a-pipeline-in-jupyterlab_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Exporting a pipeline in JupyterLab","level":2,"index":7,"id":"exporting-a-pipeline-in-jupyterlab_ds-pipelines"},{"parentId":null,"name":"Additional resources","level":1,"index":6,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/adding-users/"},"sections":[{"parentId":null,"name":"Overview of user types and permissions","level":1,"index":0,"id":"overview-of-user-types-and-permissions_{context}"},{"parentId":null,"name":"Defining {productname-short} administrator and user groups","level":1,"index":1,"id":"defining-data-science-admin-and-user-groups_{context}"},{"parentId":null,"name":"Adding users to specialized {productname-short} user groups","level":1,"index":2,"id":"adding-users-to-specialized-data-science-user-groups_{context}"},{"parentId":null,"name":"Viewing {productname-short} users","level":1,"index":3,"id":"viewing-data-science-users_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/collaborating-on-notebooks-by-using-git/"},"sections":[{"parentId":null,"name":"Uploading an existing notebook file from a Git repository by using JupyterLab","level":1,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_{context}"},{"parentId":null,"name":"Uploading an existing notebook file from a Git repository by using the command line interface","level":1,"index":1,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_{context}"},{"parentId":null,"name":"Updating your project with changes from a remote Git repository","level":1,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_{context}"},{"parentId":null,"name":"Pushing project changes to a Git repository","level":1,"index":3,"id":"pushing-project-changes-to-a-git-repository_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-cluster-storage/"},"sections":[{"parentId":null,"name":"Adding cluster storage to your data science project","level":1,"index":0,"id":"adding-cluster-storage-to-your-data-science-project_{context}"},{"parentId":null,"name":"Updating cluster storage","level":1,"index":1,"id":"updating-cluster-storage_{context}"},{"parentId":null,"name":"Deleting cluster storage from a data science project","level":1,"index":2,"id":"deleting-cluster-storage-from-a-data-science-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-distributed-workloads/"},"sections":[{"parentId":null,"name":"Configuring the distributed workloads components","level":1,"index":0,"id":"configuring-the-distributed-workloads-components_{context}"},{"parentId":null,"name":"Configuring quota management for distributed workloads","level":1,"index":1,"id":"configuring-quota-management-for-distributed-workloads_{context}"},{"parentId":null,"name":"Configuring the CodeFlare Operator","level":1,"index":2,"id":"configuring-the-codeflare-operator_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-the-odh-operator-logger/"},"sections":[{"parentId":null,"name":"Configuring the {productname-short} Operator logger","level":1,"index":0,"id":"configuring-the-operator-logger_operator-log"},{"parentId":"configuring-the-operator-logger_operator-log","name":"Viewing the {productname-short} Operator log","level":2,"index":0,"id":"_viewing_the_productname_short_operator_log"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/creating-and-importing-notebooks/"},"sections":[{"parentId":null,"name":"Creating a new notebook","level":1,"index":0,"id":"creating-a-new-notebook_{context}"},{"parentId":null,"name":"Uploading an existing notebook file from local storage","level":1,"index":1,"id":"uploading-an-existing-notebook-file-from-local-storage_{context}"},{"parentId":null,"name":"Additional resources","level":1,"index":2,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-trustyai/"},"sections":[{"parentId":null,"name":"Configuring monitoring for the multi-model serving platform","level":1,"index":0,"id":"configuring-monitoring-for-the-multi-model-serving-platform_{context}"},{"parentId":null,"name":"Enabling the TrustyAI component","level":1,"index":1,"id":"enabling-trustyai-component_{context}"},{"parentId":null,"name":"Configuring TrustyAI with a database","level":1,"index":2,"id":"configuring-trustyai-with-a-database_{context}"},{"parentId":null,"name":"Installing the TrustyAI service for a project","level":1,"index":3,"id":"installing-trustyai-service_{context}"},{"parentId":"installing-trustyai-service_{context}","name":"Installing the TrustyAI service by using the dashboard","level":2,"index":0,"id":"installing-trustyai-service-using-dashboard_{context}"},{"parentId":"installing-trustyai-service_{context}","name":"Installing the TrustyAI service by using the CLI","level":2,"index":1,"id":"installing-trustyai-service-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/customizing-component-deployment-resources/"},"sections":[{"parentId":null,"name":"Overview of component resource customization","level":1,"index":0,"id":"overview-of-component-resource-customization_managing-resources"},{"parentId":null,"name":"Customizing component resources","level":1,"index":1,"id":"customizing-component-resources_managing-resources"},{"parentId":null,"name":"Disabling component resource customization","level":1,"index":2,"id":"disabling-component-resource-customization_managing-resources"},{"parentId":null,"name":"Re-enabling component resource customization","level":1,"index":3,"id":"reenabling-component-resource-customization_managing-resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/customizing-the-dashboard/"},"sections":[{"parentId":null,"name":"Editing the dashboard configuration file","level":1,"index":0,"id":"editing-the-dashboard-configuration-file_dashboard"},{"parentId":null,"name":"Dashboard configuration options","level":1,"index":1,"id":"ref-dashboard-configuration-options_dashboard"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/deleting-users/"},"sections":[{"parentId":null,"name":"About deleting users and their resources","level":1,"index":0,"id":"about-deleting-users-and-resources_{context}"},{"parentId":null,"name":"Backing up storage data","level":1,"index":1,"id":"backing-up-storage-data_{context}"},{"parentId":null,"name":"Stopping notebook servers owned by other users","level":1,"index":2,"id":"stopping-notebook-servers-owned-by-other-users_{context}"},{"parentId":null,"name":"Revoking user access to Jupyter","level":1,"index":3,"id":"revoking-user-access-to-jupyter_{context}"},{"parentId":null,"name":"Cleaning up after deleting users","level":1,"index":4,"id":"cleaning-up-after-deleting-users_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/installing-odh-v1/"},"sections":[{"parentId":null,"name":"Installing the Open Data Hub Operator version 1","level":1,"index":0,"id":"installing-the-odh-operator-v1_installv1"},{"parentId":null,"name":"Creating a new project for your Open Data Hub instance","level":1,"index":1,"id":"creating-a-new-project-for-your-odh-instance_installv1"},{"parentId":null,"name":"Adding an Open Data Hub instance","level":1,"index":2,"id":"adding-an-odh-instance_installv1"},{"parentId":null,"name":"Accessing the Open Data Hub dashboard","level":1,"index":3,"id":"accessing-the-odh-dashboard_installv1"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/installing-odh-v2/"},"sections":[{"parentId":null,"name":"Installing the Open Data Hub Operator version 2","level":1,"index":0,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":null,"name":"Installing Open Data Hub components","level":1,"index":1,"id":"installing-odh-components_installv2"},{"parentId":null,"name":"Accessing the Open Data Hub dashboard","level":1,"index":2,"id":"accessing-the-odh-dashboard_installv2"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-access-to-data-science-projects/"},"sections":[{"parentId":null,"name":"Configuring access to a data science project","level":1,"index":0,"id":"configuring-access-to-a-data-science-project_{context}"},{"parentId":null,"name":"Sharing access to a data science project","level":1,"index":1,"id":"sharing-access-to-a-data-science-project_{context}"},{"parentId":null,"name":"Updating access to a data science project","level":1,"index":2,"id":"updating-access-to-a-data-science-project_{context}"},{"parentId":null,"name":"Removing access to a data science project","level":1,"index":3,"id":"removing-access-to-a-data-science-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-applications-that-show-in-the-dashboard/"},"sections":[{"parentId":null,"name":"Adding an application to the dashboard","level":1,"index":0,"id":"adding-an-application-to-the-dashboard_dashboard"},{"parentId":null,"name":"Preventing users from adding applications to the dashboard","level":1,"index":1,"id":"preventing-users-from-adding-applications-to-the-dashboard_dashboard"},{"parentId":null,"name":"Disabling applications connected to {productname-short}","level":1,"index":2,"id":"disabling-applications-connected_dashboard"},{"parentId":null,"name":"Showing or hiding information about enabled applications","level":1,"index":3,"id":"showing-hiding-information-about-enabled-applications_dashboard"},{"parentId":null,"name":"Hiding the default Jupyter application","level":1,"index":4,"id":"hiding-the-default-jupyter-application_dashboard"},{"parentId":null,"name":"Troubleshooting common problems in Jupyter for administrators","level":1,"index":5,"id":"troubleshooting-common-problems-in-jupyter-for-administrators_dashboard"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_dashboard","name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":2,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_dashboard","name":"A user&#8217;s notebook server does not start","level":2,"index":1,"id":"_a_users_notebook_server_does_not_start"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_dashboard","name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":2,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-cluster-resources/"},"sections":[{"parentId":null,"name":"Configuring the default PVC size for your cluster","level":1,"index":0,"id":"configuring-the-default-pvc-size-for-your-cluster_{context}"},{"parentId":null,"name":"Restoring the default PVC size for your cluster","level":1,"index":1,"id":"restoring-the-default-pvc-size-for-your-cluster_{context}"},{"parentId":null,"name":"Overview of accelerators","level":1,"index":2,"id":"overview-of-accelerators_{context}"},{"parentId":"overview-of-accelerators_{context}","name":"Enabling NVIDIA GPUs","level":2,"index":0,"id":"enabling-nvidia-gpus_{context}"},{"parentId":"overview-of-accelerators_{context}","name":"Enabling Intel Gaudi AI accelerators","level":2,"index":1,"id":"enabling-intel-gaudi-ai-accelerators_{context}"},{"parentId":null,"name":"Allocating additional resources to {productname-short} users","level":1,"index":3,"id":"allocating-additional-resources-to-data-science-users_{context}"},{"parentId":null,"name":"Troubleshooting common problems with distributed workloads for administrators","level":1,"index":4,"id":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user&#8217;s Ray cluster is in a suspended state","level":2,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user&#8217;s Ray cluster is in a failed state","level":2,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user receives a <strong>failed to call webhook</strong> error message for the CodeFlare Operator","level":2,"index":2,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user receives a <strong>failed to call webhook</strong> error message for Kueue","level":2,"index":3,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user&#8217;s Ray cluster does not start","level":2,"index":4,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user receives a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":2,"index":5,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user receives a <strong>local_queue provided does not exist</strong> error message","level":2,"index":6,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user cannot create a Ray cluster or submit jobs","level":2,"index":7,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"The user&#8217;s pod provisioned by Kueue is terminated before the user&#8217;s image is pulled","level":2,"index":8,"id":"_the_users_pod_provisioned_by_kueue_is_terminated_before_the_users_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Configuring a pipeline server","level":1,"index":0,"id":"configuring-a-pipeline-server_{context}"},{"parentId":null,"name":"Defining a pipeline","level":1,"index":1,"id":"defining-a-pipeline_{context}"},{"parentId":null,"name":"Importing a data science pipeline","level":1,"index":2,"id":"importing-a-data-science-pipeline_{context}"},{"parentId":null,"name":"Deleting a data science pipeline","level":1,"index":3,"id":"deleting-a-data-science-pipeline_{context}"},{"parentId":null,"name":"Deleting a pipeline server","level":1,"index":4,"id":"deleting-a-pipeline-server_{context}"},{"parentId":null,"name":"Viewing the details of a pipeline server","level":1,"index":5,"id":"viewing-the-details-of-a-pipeline-server_{context}"},{"parentId":null,"name":"Viewing existing pipelines","level":1,"index":6,"id":"viewing-existing-pipelines_{context}"},{"parentId":null,"name":"Overview of pipeline versions","level":1,"index":7,"id":"overview-of-pipeline-versions_{context}"},{"parentId":null,"name":"Uploading a pipeline version","level":1,"index":8,"id":"uploading-a-pipeline-version_{context}"},{"parentId":null,"name":"Deleting a pipeline version","level":1,"index":9,"id":"deleting-a-pipeline-version_{context}"},{"parentId":null,"name":"Viewing the details of a pipeline version","level":1,"index":10,"id":"viewing-the-details-of-a-pipeline-version_{context}"},{"parentId":null,"name":"Downloading a data science pipeline version","level":1,"index":11,"id":"downloading-a-data-science-pipeline-version_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-notebook-servers/"},"sections":[{"parentId":null,"name":"Accessing the Jupyter administration interface","level":1,"index":0,"id":"accessing-the-jupyter-administration-interface_{context}"},{"parentId":null,"name":"Starting notebook servers owned by other users","level":1,"index":1,"id":"starting-notebook-servers-owned-by-other-users_{context}"},{"parentId":null,"name":"Accessing notebook servers owned by other users","level":1,"index":2,"id":"accessing-notebook-servers-owned-by-other-users_{context}"},{"parentId":null,"name":"Stopping notebook servers owned by other users","level":1,"index":3,"id":"stopping-notebook-servers-owned-by-other-users_{context}"},{"parentId":null,"name":"Stopping idle notebooks","level":1,"index":4,"id":"stopping-idle-notebooks_{context}"},{"parentId":null,"name":"Adding notebook pod tolerations","level":1,"index":5,"id":"adding-notebook-pod-tolerations_{context}"},{"parentId":null,"name":"Configuring a custom notebook image","level":1,"index":6,"id":"configuring-a-custom-notebook-image_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-pipeline-experiments/"},"sections":[{"parentId":null,"name":"Overview of pipeline experiments","level":1,"index":0,"id":"overview-of-pipeline-experiments_{context}"},{"parentId":null,"name":"Creating a pipeline experiment","level":1,"index":1,"id":"creating-a-pipeline-experiment_{context}"},{"parentId":null,"name":"Archiving a pipeline experiment","level":1,"index":2,"id":"archiving-a-pipeline-experiment_{context}"},{"parentId":null,"name":"Deleting an archived pipeline experiment","level":1,"index":3,"id":"deleting-an-archived-pipeline-experiment_{context}"},{"parentId":null,"name":"Restoring an archived pipeline experiment","level":1,"index":4,"id":"restoring-an-archived-pipeline-experiment_{context}"},{"parentId":null,"name":"Viewing pipeline task executions","level":1,"index":5,"id":"viewing-pipeline-task-executions_{context}"},{"parentId":null,"name":"Viewing pipeline artifacts","level":1,"index":6,"id":"viewing-pipeline-artifacts_{context}"},{"parentId":null,"name":"Comparing runs","level":1,"index":7,"id":"comparing-runs_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-pipeline-runs/"},"sections":[{"parentId":null,"name":"Overview of pipeline runs","level":1,"index":0,"id":"overview-of-pipeline-runs_{context}"},{"parentId":null,"name":"Storing data with data science pipelines","level":1,"index":1,"id":"storing-data-with-data-science-pipelines_{context}"},{"parentId":null,"name":"Viewing active pipeline runs","level":1,"index":2,"id":"viewing-active-pipeline-runs_{context}"},{"parentId":null,"name":"Executing a pipeline run","level":1,"index":3,"id":"executing-a-pipeline-run_{context}"},{"parentId":null,"name":"Stopping an active pipeline run","level":1,"index":4,"id":"stopping-an-active-pipeline-run_{context}"},{"parentId":null,"name":"Duplicating an active pipeline run","level":1,"index":5,"id":"duplicating-an-active-pipeline-run_{context}"},{"parentId":null,"name":"Viewing scheduled pipeline runs","level":1,"index":6,"id":"viewing-scheduled-pipeline-runs_{context}"},{"parentId":null,"name":"Scheduling a pipeline run using a cron job","level":1,"index":7,"id":"scheduling-a-pipeline-run-using-a-cron-job_{context}"},{"parentId":null,"name":"Scheduling a pipeline run","level":1,"index":8,"id":"scheduling-a-pipeline-run_{context}"},{"parentId":null,"name":"Duplicating a scheduled pipeline run","level":1,"index":9,"id":"duplicating-a-scheduled-pipeline-run_{context}"},{"parentId":null,"name":"Deleting a scheduled pipeline run","level":1,"index":10,"id":"deleting-a-scheduled-pipeline-run_{context}"},{"parentId":null,"name":"Viewing the details of a pipeline run","level":1,"index":11,"id":"viewing-the-details-of-a-pipeline-run_{context}"},{"parentId":null,"name":"Viewing archived pipeline runs","level":1,"index":12,"id":"viewing-archived-pipeline-runs_{context}"},{"parentId":null,"name":"Archiving a pipeline run","level":1,"index":13,"id":"archiving-a-pipeline-run_{context}"},{"parentId":null,"name":"Restoring an archived pipeline run","level":1,"index":14,"id":"restoring-an-archived-pipeline-run_{context}"},{"parentId":null,"name":"Deleting an archived pipeline run","level":1,"index":15,"id":"deleting-an-archived-pipeline-run_{context}"},{"parentId":null,"name":"Duplicating an archived pipeline run","level":1,"index":16,"id":"duplicating-an-archived-pipeline-run_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-python-packages/"},"sections":[{"parentId":null,"name":"Viewing Python packages installed on your notebook server","level":1,"index":0,"id":"viewing-python-packages-installed-on-your-notebook-server_{context}"},{"parentId":null,"name":"Installing Python packages on your notebook server","level":1,"index":1,"id":"installing-python-packages-on-your-notebook-server_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-data-drift/"},"sections":[{"parentId":null,"name":"Creating a drift metric","level":1,"index":0,"id":"creating-a-drift-metric_drift-monitoring"},{"parentId":"creating-a-drift-metric_drift-monitoring","name":"Creating a drift metric by using the CLI","level":2,"index":0,"id":"creating-a-drift-metric-using-cli_drift-monitoring"},{"parentId":null,"name":"Deleting a drift metric by using the CLI","level":1,"index":1,"id":"deleting-a-drift-metric-using-cli_drift-monitoring"},{"parentId":null,"name":"Viewing data drift metrics for a model","level":1,"index":2,"id":"viewing-drift-metrics_drift-monitoring"},{"parentId":null,"name":"Supported drift metrics","level":1,"index":3,"id":"supported-drift-metrics_drift-monitoring"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-distributed-workloads/"},"sections":[{"parentId":null,"name":"Viewing project metrics for distributed workloads","level":1,"index":0,"id":"viewing-project-metrics-for-distributed-workloads_{context}"},{"parentId":null,"name":"Viewing the status of distributed workloads","level":1,"index":1,"id":"viewing-the-status-of-distributed-workloads_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-model-bias/"},"sections":[{"parentId":null,"name":"Creating a bias metric","level":1,"index":0,"id":"creating-a-bias-metric_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the dashboard","level":2,"index":0,"id":"creating-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the CLI","level":2,"index":1,"id":"creating-a-bias-metric-using-cli_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Duplicating a bias metric","level":2,"index":2,"id":"duplicating-a-bias-metric_bias-monitoring"},{"parentId":null,"name":"Deleting a bias metric","level":1,"index":1,"id":"deleting-a-bias-metric_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the dashboard","level":2,"index":0,"id":"deleting-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the CLI","level":2,"index":1,"id":"deleting-a-bias-metric-using-cli_bias-monitoring"},{"parentId":null,"name":"Viewing bias metrics for a model","level":1,"index":2,"id":"viewing-bias-metrics_bias-monitoring"},{"parentId":null,"name":"Supported bias metrics","level":1,"index":3,"id":"supported-bias-metrics_bias-monitoring"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-model-performance/"},"sections":[{"parentId":null,"name":"Viewing performance metrics for all models on a model server","level":1,"index":0,"id":"viewing-performance-metrics-for-model-server_monitoring-model-performance"},{"parentId":null,"name":"Viewing HTTP request metrics for a deployed model","level":1,"index":1,"id":"viewing-http-request-metrics-for-a-deployed-model_monitoring-model-performance"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/running-distributed-workloads/"},"sections":[{"parentId":null,"name":"Running distributed data science workloads from notebooks","level":1,"index":0,"id":"running-distributed-data-science-workloads-from-notebooks_{context}"},{"parentId":null,"name":"Running distributed data science workloads from data science pipelines","level":1,"index":1,"id":"running-distributed-data-science-workloads-from-ds-pipelines_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/serving-large-models/"},"sections":[{"parentId":null,"name":"About the single-model serving platform","level":1,"index":0,"id":"about-the-single-model-serving-platform_serving-large-models"},{"parentId":null,"name":"About KServe deployment modes","level":1,"index":1,"id":"about-kserve-deployment-modes_serving-large-models"},{"parentId":"about-kserve-deployment-modes_serving-large-models","name":"Serverless mode","level":2,"index":0,"id":"_serverless_mode"},{"parentId":"about-kserve-deployment-modes_serving-large-models","name":"Raw deployment mode","level":2,"index":1,"id":"_raw_deployment_mode"},{"parentId":null,"name":"Installing KServe","level":1,"index":2,"id":"installing-kserve_serving-large-models"},{"parentId":null,"name":"Deploying models by using the single-model serving platform","level":1,"index":3,"id":"deploying-models-using-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Enabling the single-model serving platform","level":2,"index":0,"id":"enabling-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Adding a custom model-serving runtime for the single-model serving platform","level":2,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Deploying models on the single-model serving platform","level":2,"index":2,"id":"deploying-models-on-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Accessing the inference endpoint for a deployed model","level":2,"index":3,"id":"accessing-inference-endpoint-for-deployed-model_serving-large-models"},{"parentId":null,"name":"Configuring monitoring for the single-model serving platform","level":1,"index":4,"id":"configuring-monitoring-for-the-single-model-serving-platform_serving-large-models"},{"parentId":null,"name":"Viewing model-serving runtime metrics for the single-model serving platform","level":1,"index":5,"id":"viewing-metrics-for-the-single-model-serving-platform_serving-large-models"},{"parentId":null,"name":"Monitoring model performance","level":1,"index":6,"id":"_monitoring_model_performance"},{"parentId":"_monitoring_model_performance","name":"Viewing performance metrics for a deployed model","level":2,"index":0,"id":"viewing-performance-metrics-for-deployed-model_serving-large-models"},{"parentId":null,"name":"Optimizing model-serving runtimes","level":1,"index":7,"id":"_optimizing_model_serving_runtimes"},{"parentId":"_optimizing_model_serving_runtimes","name":"Optimizing the vLLM model-serving runtime","level":2,"index":0,"id":"optimizing-the-vllm-runtime_serving-large-models"},{"parentId":null,"name":"Performance tuning on the single-model serving platform","level":1,"index":8,"id":"_performance_tuning_on_the_single_model_serving_platform"},{"parentId":"_performance_tuning_on_the_single_model_serving_platform","name":"Resolving CUDA out-of-memory errors","level":2,"index":0,"id":"resolving-cuda-oom-errors-for-the-single-model-serving-platform_serving-large-models"},{"parentId":null,"name":"Supported model-serving runtimes","level":1,"index":9,"id":"supported-model-serving-runtimes_serving-large-models"},{"parentId":null,"name":"Inference endpoints","level":1,"index":10,"id":"inference-endpoints_serving-large-models"},{"parentId":"inference-endpoints_serving-large-models","name":"Example commands","level":2,"index":0,"id":"_example_commands"},{"parentId":"inference-endpoints_serving-large-models","name":"Additional resources","level":2,"index":1,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/serving-small-and-medium-sized-models/"},"sections":[{"parentId":null,"name":"Configuring model servers","level":1,"index":0,"id":"_configuring_model_servers"},{"parentId":"_configuring_model_servers","name":"Enabling the multi-model serving platform","level":2,"index":0,"id":"enabling-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a custom model-serving runtime for the multi-model serving platform","level":2,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a model server for the multi-model serving platform","level":2,"index":2,"id":"adding-a-model-server-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Deleting a model server","level":2,"index":3,"id":"deleting-a-model-server_model-serving"},{"parentId":null,"name":"Working with deployed models","level":1,"index":1,"id":"_working_with_deployed_models"},{"parentId":"_working_with_deployed_models","name":"Deploying a model by using the multi-model serving platform","level":2,"index":0,"id":"deploying-a-model-using-the-multi-model-serving-platform_model-serving"},{"parentId":"_working_with_deployed_models","name":"Viewing a deployed model","level":2,"index":1,"id":"viewing-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Updating the deployment properties of a deployed model","level":2,"index":2,"id":"updating-the-deployment-properties-of-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Deleting a deployed model","level":2,"index":3,"id":"deleting-a-deployed-model_model-serving"},{"parentId":null,"name":"Configuring monitoring for the multi-model serving platform","level":1,"index":2,"id":"configuring-monitoring-for-the-multi-model-serving-platform_model-serving"},{"parentId":null,"name":"Viewing model-serving runtime metrics for the multi-model serving platform","level":1,"index":3,"id":"viewing-metrics-for-the-multi-model-serving-platform_model-serving"},{"parentId":null,"name":"Monitoring model performance","level":1,"index":4,"id":"_monitoring_model_performance"},{"parentId":"_monitoring_model_performance","name":"Viewing performance metrics for all models on a model server","level":2,"index":0,"id":"viewing-performance-metrics-for-model-server_model-serving"},{"parentId":"_monitoring_model_performance","name":"Viewing HTTP request metrics for a deployed model","level":2,"index":1,"id":"viewing-http-request-metrics-for-a-deployed-model_model-serving"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/setting-up-trustyai-for-your-project/"},"sections":[{"parentId":null,"name":"Authenticating the TrustyAI service","level":1,"index":0,"id":"authenticating-trustyai-service_{context}"},{"parentId":null,"name":"Sending training data to TrustyAI","level":1,"index":1,"id":"sending-training-data-to-trustyai_{context}"},{"parentId":null,"name":"Labeling data fields","level":1,"index":2,"id":"labeling-data-fields_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/tuning-a-model-by-using-the-training-operator/"},"sections":[{"parentId":null,"name":"Configuring the Training Operator permissions when not using Kueue","level":1,"index":0,"id":"configuring-the-training-operator-permissions-when-not-using-kueue_{context}"},{"parentId":null,"name":"Configuring the training job","level":1,"index":1,"id":"configuring-the-training-job_{context}"},{"parentId":null,"name":"Running the training job","level":1,"index":2,"id":"running-the-training-job_{context}"},{"parentId":null,"name":"Monitoring the training job","level":1,"index":3,"id":"monitoring-the-training-job_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/upgrading-odh-v1-to-v2/"},"sections":[{"parentId":null,"name":"Upgrading the Open Data Hub Operator version 1","level":1,"index":0,"id":"upgrading-the-odh-operator-v1_upgradev1"},{"parentId":null,"name":"Installing Open Data Hub components","level":1,"index":1,"id":"installing-odh-components_upgradev1"},{"parentId":null,"name":"Accessing the Open Data Hub dashboard","level":1,"index":2,"id":"accessing-the-odh-dashboard_upgradev1"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-data-science-projects/"},"sections":[{"parentId":null,"name":"Creating a data science project","level":1,"index":0,"id":"creating-a-data-science-project_{context}"},{"parentId":null,"name":"Updating a data science project","level":1,"index":1,"id":"updating-a-data-science-project_{context}"},{"parentId":null,"name":"Deleting a data science project","level":1,"index":2,"id":"deleting-a-data-science-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/upgrading-odh-v2/"},"sections":[{"parentId":null,"name":"Installing Open Data Hub version 2","level":1,"index":0,"id":"installing-odh-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing the Open Data Hub Operator version 2","level":2,"index":0,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing Open Data Hub components","level":2,"index":1,"id":"installing-odh-components_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Accessing the Open Data Hub dashboard","level":2,"index":2,"id":"accessing-the-odh-dashboard_installv2"},{"parentId":null,"name":"Adding a CA bundle after upgrading","level":1,"index":1,"id":"adding-a-ca-bundle-after-upgrading_upgradev2"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-data-connections/"},"sections":[{"parentId":null,"name":"Adding a data connection to your data science project","level":1,"index":0,"id":"adding-a-data-connection-to-your-data-science-project_{context}"},{"parentId":null,"name":"Deleting a data connection","level":1,"index":1,"id":"deleting-a-data-connection_{context}"},{"parentId":null,"name":"Updating a connected data source","level":1,"index":2,"id":"updating-a-connected-data-source_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-explainability/"},"sections":[{"parentId":null,"name":"Requesting a LIME explanation","level":1,"index":0,"id":"requesting-a-lime-explanation_explainers"},{"parentId":"requesting-a-lime-explanation_explainers","name":"Requesting a LIME explanation by using the CLI","level":2,"index":0,"id":"requesting-a-lime-explanation-using-CLI_explainers"},{"parentId":null,"name":"Requesting a SHAP explanation","level":1,"index":1,"id":"requesting-a-shap-explanation_explainers"},{"parentId":"requesting-a-shap-explanation_explainers","name":"Requesting a SHAP explanation by using the CLI","level":2,"index":0,"id":"requesting-a-shap-explanation-using-CLI_explainers"},{"parentId":null,"name":"Supported explainers","level":1,"index":2,"id":"supported-explainers_explainers"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-project-workbenches/"},"sections":[{"parentId":null,"name":"Creating a workbench and selecting an IDE","level":1,"index":0,"id":"creating-a-workbench-select-ide_{context}"},{"parentId":"creating-a-workbench-select-ide_{context}","name":"About workbench images","level":2,"index":0,"id":"about-workbench-images_{context}"},{"parentId":"creating-a-workbench-select-ide_{context}","name":"Creating a workbench","level":2,"index":1,"id":"creating-a-project-workbench_{context}"},{"parentId":null,"name":"Starting a workbench","level":1,"index":1,"id":"starting-a-workbench_{context}"},{"parentId":null,"name":"Updating a project workbench","level":1,"index":2,"id":"updating-a-project-workbench_{context}"},{"parentId":null,"name":"Deleting a workbench from a data science project","level":1,"index":3,"id":"deleting-a-workbench-from-a-data-science-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-certificates/"},"sections":[{"parentId":null,"name":"Understanding certificates in {productname-short}","level":1,"index":0,"id":"understanding-certificates_certs"},{"parentId":"understanding-certificates_certs","name":"How CA bundles are injected","level":2,"index":0,"id":"_how_ca_bundles_are_injected"},{"parentId":"understanding-certificates_certs","name":"How the ConfigMap is managed","level":2,"index":1,"id":"_how_the_configmap_is_managed"},{"parentId":null,"name":"Adding a CA bundle","level":1,"index":1,"id":"adding-a-ca-bundle_certs"},{"parentId":null,"name":"Removing a CA bundle","level":1,"index":2,"id":"removing-a-ca-bundle_certs"},{"parentId":null,"name":"Removing a CA bundle from a namespace","level":1,"index":3,"id":"removing-a-ca-bundle-from-a-namespace_certs"},{"parentId":null,"name":"Managing certificates","level":1,"index":4,"id":"managing-certificates_certs"},{"parentId":null,"name":"Using self-signed certificates with {productname-short} components","level":1,"index":5,"id":"_using_self_signed_certificates_with_productname_short_components"},{"parentId":"_using_self_signed_certificates_with_productname_short_components","name":"Using certificates with data science pipelines","level":2,"index":0,"id":"using-certificates-with-data-science-pipelines_certs"},{"parentId":"using-certificates-with-data-science-pipelines_certs","name":"Providing a CA bundle only for data science pipelines","level":3,"index":0,"id":"_providing_a_ca_bundle_only_for_data_science_pipelines"},{"parentId":"_using_self_signed_certificates_with_productname_short_components","name":"Using certificates with workbenches","level":2,"index":1,"id":"using-certificates-with-workbenches_certs"},{"parentId":"using-certificates-with-workbenches_certs","name":"Creating data science pipelines with Elyra and self-signed certificates","level":3,"index":0,"id":"_creating_data_science_pipelines_with_elyra_and_self_signed_certificates"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-pipelines-in-jupyterlab/"},"sections":[{"parentId":null,"name":"Overview of pipelines in JupyterLab","level":1,"index":0,"id":"overview-of-pipelines-in-jupyterlab_{context}"},{"parentId":null,"name":"Accessing the pipeline editor","level":1,"index":1,"id":"accessing-the-pipeline-editor_{context}"},{"parentId":null,"name":"Creating a runtime configuration","level":1,"index":2,"id":"creating-a-runtime-configuration_{context}"},{"parentId":null,"name":"Updating a runtime configuration","level":1,"index":3,"id":"updating-a-runtime-configuration_{context}"},{"parentId":null,"name":"Deleting a runtime configuration","level":1,"index":4,"id":"deleting-a-runtime-configuration_{context}"},{"parentId":null,"name":"Duplicating a runtime configuration","level":1,"index":5,"id":"duplicating-a-runtime-configuration_{context}"},{"parentId":null,"name":"Running a pipeline in JupyterLab","level":1,"index":6,"id":"running-a-pipeline-in-jupyterlab_{context}"},{"parentId":null,"name":"Exporting a pipeline in JupyterLab","level":1,"index":7,"id":"exporting-a-pipeline-in-jupyterlab_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-pipeline-logs/"},"sections":[{"parentId":null,"name":"About pipeline logs","level":1,"index":0,"id":"about-pipeline-logs_{context}"},{"parentId":null,"name":"Viewing pipeline step logs","level":1,"index":1,"id":"viewing-pipeline-step-logs_{context}"},{"parentId":null,"name":"Downloading pipeline step logs","level":1,"index":2,"id":"downloading-pipeline-step-logs_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/a-new-file-test2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-deleting-users-and-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-kserve-deployment-modes/"},"sections":[{"parentId":null,"name":"Serverless mode","level":1,"index":0,"id":"_serverless_mode"},{"parentId":null,"name":"Raw deployment mode","level":1,"index":1,"id":"_raw_deployment_mode"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-model-serving/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-pipeline-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-authorization-token-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-workbench-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-inference-endpoint-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-jupyter-administration-interface/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-odh-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-pipeline-editor/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-your-workbench-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-ca-bundle-after-upgrading/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-data-connection-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-an-application-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-model-server-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-an-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-cluster-storage-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-users-to-specialized-data-science-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-notebook-pod-tolerations/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/allocating-additional-resources-to-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/authenticating-trustyai-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/backing-up-storage-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/cleaning-up-after-deleting-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/cleaning-up-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/comparing-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-custom-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-recommended-accelerator-for-notebook-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-recommended-accelerator-for-serving-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-monitoring-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-monitoring-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-quota-management-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-codeflare-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-distributed-workloads-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-operator-logger/"},"sections":[{"parentId":null,"name":"Viewing the {productname-short} Operator log","level":1,"index":0,"id":"_viewing_the_productname_short_operator_log"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-training-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-training-operator-permissions-when-not-using-kueue/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-trustyai-with-a-database/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric/"},"sections":[{"parentId":null,"name":"Creating a bias metric by using the dashboard","level":1,"index":0,"id":"creating-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Creating a bias metric by using the CLI","level":1,"index":1,"id":"creating-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-drift-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-drift-metric/"},"sections":[{"parentId":null,"name":"Creating a drift metric by using the CLI","level":1,"index":0,"id":"creating-a-drift-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-new-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-new-project-for-your-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-workbench-select-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/customizing-component-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/defining-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/defining-data-science-admin-and-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric/"},"sections":[{"parentId":null,"name":"Deleting a bias metric by using the dashboard","level":1,"index":0,"id":"deleting-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Deleting a bias metric by using the CLI","level":1,"index":1,"id":"deleting-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-data-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-drift-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-workbench-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-cluster-storage-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-model-using-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-on-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-on-single-node-openshift-using-kserve-raw-deployment-mode/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-using-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/disabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/disabling-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-a-data-science-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-bias-metric/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-the-dashboard-configuration-file/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-intel-gaudi-ai-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-nvidia-gpus/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-data-science-pipelines-2/"},"sections":[{"parentId":null,"name":"Installing {productname-short} with data science pipelines 2.0","level":1,"index":0,"id":"_installing_productname_short_with_data_science_pipelines_2_0"},{"parentId":null,"name":"Upgrading to data science pipelines 2.0","level":1,"index":1,"id":"_upgrading_to_data_science_pipelines_2_0"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-trustyai-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/executing-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/exporting-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/hiding-information-about-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/hiding-the-default-jupyter-application/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/importing-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-python-packages-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-odh-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-the-odh-operator-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-trustyai-service-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-trustyai-service-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/intel-gaudi-ai-accelerator-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-trustyai-service/"},"sections":[{"parentId":null,"name":"Installing the TrustyAI service by using the dashboard","level":1,"index":0,"id":"installing-trustyai-service-using-dashboard_{context}"},{"parentId":null,"name":"Installing the TrustyAI service by using the CLI","level":1,"index":1,"id":"installing-trustyai-service-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/labeling-data-fields/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/logging-in/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/making-inference-requests-to-models-deployed-on-single-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/managing-certificates/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/next-steps-getting-started/"},"sections":[{"parentId":null,"name":"Additional resources","level":1,"index":0,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/monitoring-the-training-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/nvidia-gpu-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-for-getting-started/"},"sections":[{"parentId":null,"name":"Data science workflow","level":1,"index":0,"id":"_data_science_workflow"},{"parentId":null,"name":"About this guide","level":1,"index":1,"id":"_about_this_guide"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/optimizing-the-vllm-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-kueue-resources/"},"sections":[{"parentId":null,"name":"Resource flavor","level":1,"index":0,"id":"_resource_flavor"},{"parentId":null,"name":"Cluster queue","level":1,"index":1,"id":"_cluster_queue"},{"parentId":null,"name":"Local queue","level":1,"index":2,"id":"_local_queue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-model-monitoring/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-experiments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipelines-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/preventing-users-from-adding-applications-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/pushing-project-changes-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-user-types-and-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/reenabling-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-dashboard-configuration-options/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-inference-endpoints/"},"sections":[{"parentId":null,"name":"Example commands","level":1,"index":0,"id":"_example_commands"},{"parentId":null,"name":"Additional resources","level":1,"index":1,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-disabled-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-supported-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-a-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requesting-a-lime-explanation/"},"sections":[{"parentId":null,"name":"Requesting a LIME explanation by using the CLI","level":1,"index":0,"id":"requesting-a-lime-explanation-using-CLI_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requesting-a-lime-explanation-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requesting-a-shap-explanation-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requesting-a-shap-explanation/"},"sections":[{"parentId":null,"name":"Requesting a SHAP explanation by using the CLI","level":1,"index":0,"id":"requesting-a-shap-explanation-using-CLI_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requirements-for-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/resolving-cuda-oom-errors/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/revoking-user-access-to-jupyter/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-from-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-from-ds-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-disconnected-env/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/scheduling-a-pipeline-run-using-a-cron-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-the-training-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/scheduling-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/sharing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/sending-training-data-to-trustyai/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-a-jupyter-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/showing-hiding-information-about-enabled-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-a-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-idle-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/supported-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/storing-data-with-data-science-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/supported-drift-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-in-jupyter-for-administrators/"},"sections":[{"parentId":null,"name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":1,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":null,"name":"A user&#8217;s notebook server does not start","level":1,"index":1,"id":"_a_users_notebook_server_does_not_start"},{"parentId":null,"name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":1,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/supported-explainers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-in-jupyter-for-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-with-distributed-workloads-for-users/"},"sections":[{"parentId":null,"name":"My Ray cluster is in a suspended state","level":1,"index":0,"id":"_my_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"My Ray cluster is in a failed state","level":1,"index":1,"id":"_my_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"I see a <strong>failed to call webhook</strong> error message for the CodeFlare Operator","level":1,"index":2,"id":"_i_see_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":null,"name":"I see a <strong>failed to call webhook</strong> error message for Kueue","level":1,"index":3,"id":"_i_see_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"My Ray cluster doesn&#8217;t start","level":1,"index":4,"id":"_my_ray_cluster_doesnt_start"},{"parentId":null,"name":"I see a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":1,"index":5,"id":"_i_see_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"I see a <strong>local_queue provided does not exist</strong> error message","level":1,"index":6,"id":"_i_see_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"I cannot create a Ray cluster or submit jobs","level":1,"index":7,"id":"_i_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"My pod provisioned by Kueue is terminated before my image is pulled","level":1,"index":8,"id":"_my_pod_provisioned_by_kueue_is_terminated_before_my_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/understanding-certificates/"},"sections":[{"parentId":null,"name":"How CA bundles are injected","level":1,"index":0,"id":"_how_ca_bundles_are_injected"},{"parentId":null,"name":"How the ConfigMap is managed","level":1,"index":1,"id":"_how_the_configmap_is_managed"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-with-distributed-workloads-for-administrators/"},"sections":[{"parentId":null,"name":"A user&#8217;s Ray cluster is in a suspended state","level":1,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"A user&#8217;s Ray cluster is in a failed state","level":1,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"A user receives a <strong>failed to call webhook</strong> error message for the CodeFlare Operator","level":1,"index":2,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":null,"name":"A user receives a <strong>failed to call webhook</strong> error message for Kueue","level":1,"index":3,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"A user&#8217;s Ray cluster does not start","level":1,"index":4,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":null,"name":"A user receives a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":1,"index":5,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"A user receives a <strong>local_queue provided does not exist</strong> error message","level":1,"index":6,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"A user cannot create a Ray cluster or submit jobs","level":1,"index":7,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"The user&#8217;s pod provisioned by Kueue is terminated before the user&#8217;s image is pulled","level":1,"index":8,"id":"_the_users_pod_provisioned_by_kueue_is_terminated_before_the_users_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-connected-data-source/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-cluster-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-notebook-server-settings-by-restarting-your-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-the-deployment-properties-of-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-your-project-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/upgrading-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-certificates-with-workbenches/"},"sections":[{"parentId":null,"name":"Creating data science pipelines with Elyra and self-signed certificates","level":1,"index":0,"id":"_creating_data_science_pipelines_with_elyra_and_self_signed_certificates"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-certificates-with-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Providing a CA bundle only for data science pipelines","level":1,"index":0,"id":"_providing_a_ca_bundle_only_for_data_science_pipelines"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-active-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-archived-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-connected-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-drift-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-existing-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-metrics-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-http-request-metrics-for-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-metrics-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-performance-metrics-for-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-performance-metrics-for-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-pipeline-artifacts/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-pipeline-task-executions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-project-metrics-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-python-packages-installed-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-scheduled-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-status-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/working-with-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/working-in-code-server/"},"sections":[{"parentId":null,"name":"Installing extensions with code-server","level":1,"index":0,"id":"_installing_extensions_with_code_server"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/a-new-file-test2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-deleting-users-and-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-kserve-deployment-modes/"},"sections":[{"parentId":null,"name":"Serverless mode","level":1,"index":0,"id":"_serverless_mode"},{"parentId":null,"name":"Raw deployment mode","level":1,"index":1,"id":"_raw_deployment_mode"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-model-serving/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-pipeline-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-authorization-token-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-workbench-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-inference-endpoint-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-odh-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-jupyter-administration-interface/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-pipeline-editor/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-your-workbench-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-ca-bundle-after-upgrading/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-data-connection-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-an-application-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-an-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-model-server-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-cluster-storage-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-notebook-pod-tolerations/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-users-to-specialized-data-science-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/allocating-additional-resources-to-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/backing-up-storage-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/authenticating-trustyai-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/cleaning-up-after-deleting-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/cleaning-up-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/comparing-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-custom-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-recommended-accelerator-for-notebook-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-recommended-accelerator-for-serving-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-monitoring-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-monitoring-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-codeflare-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-quota-management-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-distributed-workloads-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-operator-logger/"},"sections":[{"parentId":null,"name":"Viewing the {productname-short} Operator log","level":1,"index":0,"id":"_viewing_the_productname_short_operator_log"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-training-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-training-operator-permissions-when-not-using-kueue/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-trustyai-with-a-database/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric/"},"sections":[{"parentId":null,"name":"Creating a bias metric by using the dashboard","level":1,"index":0,"id":"creating-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Creating a bias metric by using the CLI","level":1,"index":1,"id":"creating-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-new-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-drift-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-drift-metric/"},"sections":[{"parentId":null,"name":"Creating a drift metric by using the CLI","level":1,"index":0,"id":"creating-a-drift-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-new-project-for-your-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-workbench-select-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/defining-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/customizing-component-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/defining-data-science-admin-and-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric/"},"sections":[{"parentId":null,"name":"Deleting a bias metric by using the dashboard","level":1,"index":0,"id":"deleting-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Deleting a bias metric by using the CLI","level":1,"index":1,"id":"deleting-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-data-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-drift-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-workbench-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-cluster-storage-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-model-using-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-using-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-on-single-node-openshift-using-kserve-raw-deployment-mode/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/disabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-on-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/disabling-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-a-data-science-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-bias-metric/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-the-dashboard-configuration-file/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-intel-gaudi-ai-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-nvidia-gpus/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-trustyai-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/executing-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/exporting-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-data-science-pipelines-2/"},"sections":[{"parentId":null,"name":"Installing {productname-short} with data science pipelines 2.0","level":1,"index":0,"id":"_installing_productname_short_with_data_science_pipelines_2_0"},{"parentId":null,"name":"Upgrading to data science pipelines 2.0","level":1,"index":1,"id":"_upgrading_to_data_science_pipelines_2_0"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/hiding-the-default-jupyter-application/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/hiding-information-about-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/importing-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-odh-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-python-packages-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-the-odh-operator-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-trustyai-service-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-trustyai-service-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-trustyai-service/"},"sections":[{"parentId":null,"name":"Installing the TrustyAI service by using the dashboard","level":1,"index":0,"id":"installing-trustyai-service-using-dashboard_{context}"},{"parentId":null,"name":"Installing the TrustyAI service by using the CLI","level":1,"index":1,"id":"installing-trustyai-service-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/labeling-data-fields/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/logging-in/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/intel-gaudi-ai-accelerator-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-a-ca-bundle-from-a-namespace/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/making-inference-requests-to-models-deployed-on-single-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/monitoring-the-training-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/nvidia-gpu-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/optimizing-the-vllm-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-for-getting-started/"},"sections":[{"parentId":null,"name":"Data science workflow","level":1,"index":0,"id":"_data_science_workflow"},{"parentId":null,"name":"About this guide","level":1,"index":1,"id":"_about_this_guide"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-kueue-resources/"},"sections":[{"parentId":null,"name":"Resource flavor","level":1,"index":0,"id":"_resource_flavor"},{"parentId":null,"name":"Cluster queue","level":1,"index":1,"id":"_cluster_queue"},{"parentId":null,"name":"Local queue","level":1,"index":2,"id":"_local_queue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-model-monitoring/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-experiments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipelines-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-user-types-and-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/preventing-users-from-adding-applications-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/reenabling-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/pushing-project-changes-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-dashboard-configuration-options/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-inference-endpoints/"},"sections":[{"parentId":null,"name":"Example commands","level":1,"index":0,"id":"_example_commands"},{"parentId":null,"name":"Additional resources","level":1,"index":1,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-supported-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-a-ca-bundle-from-a-namespace/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-a-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-disabled-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requesting-a-lime-explanation-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requesting-a-lime-explanation/"},"sections":[{"parentId":null,"name":"Requesting a LIME explanation by using the CLI","level":1,"index":0,"id":"requesting-a-lime-explanation-using-CLI_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requesting-a-shap-explanation/"},"sections":[{"parentId":null,"name":"Requesting a SHAP explanation by using the CLI","level":1,"index":0,"id":"requesting-a-shap-explanation-using-CLI_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requesting-a-shap-explanation-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requirements-for-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/resolving-cuda-oom-errors/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/revoking-user-access-to-jupyter/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-disconnected-env/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-from-ds-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-from-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-the-training-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/scheduling-a-pipeline-run-using-a-cron-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/scheduling-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/sending-training-data-to-trustyai/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/sharing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-a-jupyter-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/showing-hiding-information-about-enabled-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-a-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-idle-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/storing-data-with-data-science-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/supported-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/supported-explainers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-in-jupyter-for-administrators/"},"sections":[{"parentId":null,"name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":1,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":null,"name":"A user&#8217;s notebook server does not start","level":1,"index":1,"id":"_a_users_notebook_server_does_not_start"},{"parentId":null,"name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":1,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/supported-drift-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-in-jupyter-for-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-with-distributed-workloads-for-administrators/"},"sections":[{"parentId":null,"name":"A user&#8217;s Ray cluster is in a suspended state","level":1,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"A user&#8217;s Ray cluster is in a failed state","level":1,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"A user receives a <strong>failed to call webhook</strong> error message for the CodeFlare Operator","level":1,"index":2,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":null,"name":"A user receives a <strong>failed to call webhook</strong> error message for Kueue","level":1,"index":3,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"A user&#8217;s Ray cluster does not start","level":1,"index":4,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":null,"name":"A user receives a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":1,"index":5,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"A user receives a <strong>local_queue provided does not exist</strong> error message","level":1,"index":6,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"A user cannot create a Ray cluster or submit jobs","level":1,"index":7,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"The user&#8217;s pod provisioned by Kueue is terminated before the user&#8217;s image is pulled","level":1,"index":8,"id":"_the_users_pod_provisioned_by_kueue_is_terminated_before_the_users_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-with-distributed-workloads-for-users/"},"sections":[{"parentId":null,"name":"My Ray cluster is in a suspended state","level":1,"index":0,"id":"_my_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"My Ray cluster is in a failed state","level":1,"index":1,"id":"_my_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"I see a <strong>failed to call webhook</strong> error message for the CodeFlare Operator","level":1,"index":2,"id":"_i_see_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":null,"name":"I see a <strong>failed to call webhook</strong> error message for Kueue","level":1,"index":3,"id":"_i_see_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"My Ray cluster doesn&#8217;t start","level":1,"index":4,"id":"_my_ray_cluster_doesnt_start"},{"parentId":null,"name":"I see a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":1,"index":5,"id":"_i_see_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"I see a <strong>local_queue provided does not exist</strong> error message","level":1,"index":6,"id":"_i_see_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"I cannot create a Ray cluster or submit jobs","level":1,"index":7,"id":"_i_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"My pod provisioned by Kueue is terminated before my image is pulled","level":1,"index":8,"id":"_my_pod_provisioned_by_kueue_is_terminated_before_my_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/understanding-certificates/"},"sections":[{"parentId":null,"name":"How CA bundles are injected","level":1,"index":0,"id":"_how_ca_bundles_are_injected"},{"parentId":null,"name":"How the ConfigMap is managed","level":1,"index":1,"id":"_how_the_configmap_is_managed"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-connected-data-source/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-cluster-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-notebook-server-settings-by-restarting-your-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-the-deployment-properties-of-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-your-project-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/upgrading-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-certificates-with-workbenches/"},"sections":[{"parentId":null,"name":"Creating data science pipelines with Elyra and self-signed certificates","level":1,"index":0,"id":"_creating_data_science_pipelines_with_elyra_and_self_signed_certificates"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-certificates-with-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Providing a CA bundle only for data science pipelines","level":1,"index":0,"id":"_providing_a_ca_bundle_only_for_data_science_pipelines"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-active-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-archived-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-connected-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-existing-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-drift-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-http-request-metrics-for-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-metrics-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-metrics-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-performance-metrics-for-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-performance-metrics-for-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-pipeline-artifacts/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-pipeline-task-executions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-project-metrics-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-python-packages-installed-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-scheduled-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-status-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/working-in-code-server/"},"sections":[{"parentId":null,"name":"Installing extensions with code-server","level":1,"index":0,"id":"_installing_extensions_with_code_server"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/working-with-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/next-steps-getting-started/"},"sections":[{"parentId":null,"name":"Additional resources","level":1,"index":0,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/managing-certificates/"},"sections":null}}}]},"asciidoc":{"html":"<div id=\"toc\" class=\"toc\">\n<div id=\"toctitle\">Table of Contents</div>\n<ul class=\"sectlevel1\">\n<li><a href=\"#overview-of-distributed-workloads_distributed-workloads\">Overview of distributed workloads</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#overview-of-kueue-resources_distributed-workloads\">Overview of Kueue resources</a></li>\n</ul>\n</li>\n<li><a href=\"#configuring-distributed-workloads_distributed-workloads\">Configuring distributed workloads</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#configuring-the-distributed-workloads-components_distributed-workloads\">Configuring the distributed workloads components</a></li>\n<li><a href=\"#configuring-quota-management-for-distributed-workloads_distributed-workloads\">Configuring quota management for distributed workloads</a></li>\n<li><a href=\"#configuring-the-codeflare-operator_distributed-workloads\">Configuring the CodeFlare Operator</a></li>\n</ul>\n</li>\n<li><a href=\"#running-distributed-workloads_distributed-workloads\">Running distributed workloads</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#running-distributed-data-science-workloads-from-notebooks_distributed-workloads\">Running distributed data science workloads from notebooks</a></li>\n<li><a href=\"#running-distributed-data-science-workloads-from-ds-pipelines_distributed-workloads\">Running distributed data science workloads from data science pipelines</a></li>\n</ul>\n</li>\n<li><a href=\"#monitoring-distributed-workloads_distributed-workloads\">Monitoring distributed workloads</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#viewing-project-metrics-for-distributed-workloads_distributed-workloads\">Viewing project metrics for distributed workloads</a></li>\n<li><a href=\"#viewing-the-status-of-distributed-workloads_distributed-workloads\">Viewing the status of distributed workloads</a></li>\n</ul>\n</li>\n<li><a href=\"#tuning-a-model-by-using-the-training-operator_distributed-workloads\">Tuning a model by using the Training Operator</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#configuring-the-training-operator-permissions-when-not-using-kueue_distributed-workloads\">Configuring the Training Operator permissions when not using Kueue</a></li>\n<li><a href=\"#configuring-the-training-job_distributed-workloads\">Configuring the training job</a></li>\n<li><a href=\"#running-the-training-job_distributed-workloads\">Running the training job</a></li>\n<li><a href=\"#monitoring-the-training-job_distributed-workloads\">Monitoring the training job</a></li>\n</ul>\n</li>\n<li><a href=\"#troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads\">Troubleshooting common problems with distributed workloads for users</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#_my_ray_cluster_is_in_a_suspended_state\">My Ray cluster is in a suspended state</a></li>\n<li><a href=\"#_my_ray_cluster_is_in_a_failed_state\">My Ray cluster is in a failed state</a></li>\n<li><a href=\"#_i_see_a_failed_to_call_webhook_error_message_for_the_codeflare_operator\">I see a <strong>failed to call webhook</strong> error message for the CodeFlare Operator</a></li>\n<li><a href=\"#_i_see_a_failed_to_call_webhook_error_message_for_kueue\">I see a <strong>failed to call webhook</strong> error message for Kueue</a></li>\n<li><a href=\"#_my_ray_cluster_doesnt_start\">My Ray cluster doesn&#8217;t start</a></li>\n<li><a href=\"#_i_see_a_default_local_queue_not_found_error_message\">I see a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message</a></li>\n<li><a href=\"#_i_see_a_local_queue_provided_does_not_exist_error_message\">I see a <strong>local_queue provided does not exist</strong> error message</a></li>\n<li><a href=\"#_i_cannot_create_a_ray_cluster_or_submit_jobs\">I cannot create a Ray cluster or submit jobs</a></li>\n<li><a href=\"#_my_pod_provisioned_by_kueue_is_terminated_before_my_image_is_pulled\">My pod provisioned by Kueue is terminated before my image is pulled</a></li>\n</ul>\n</li>\n</ul>\n</div>\n<div id=\"preamble\">\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>To train complex machine-learning models or process data more quickly, data scientists can use the distributed workloads feature to run their jobs on multiple OpenShift worker nodes in parallel.\nThis approach significantly reduces the task completion time, and enables the use of larger datasets and more complex models.</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"overview-of-distributed-workloads_distributed-workloads\">Overview of distributed workloads</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>You can use the distributed workloads feature to queue, scale, and manage the resources required to run data science workloads across multiple nodes in an OpenShift cluster simultaneously.\nTypically, data science workloads include several types of artificial intelligence (AI) workloads, including machine learning (ML) and Python workloads.</p>\n</div>\n<div class=\"paragraph\">\n<p>Distributed workloads provide the following benefits:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>You can iterate faster and experiment more frequently because of the reduced processing time.</p>\n</li>\n<li>\n<p>You can use larger datasets, which can lead to more accurate models.</p>\n</li>\n<li>\n<p>You can use complex models that could not be trained on a single node.</p>\n</li>\n<li>\n<p>You can submit distributed workloads at any time, and the system then schedules the distributed workload when the required resources are available.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>The distributed workloads infrastructure includes the following components:</p>\n</div>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">CodeFlare Operator</dt>\n<dd>\n<p>Secures deployed Ray clusters and grants access to their URLs</p>\n</dd>\n<dt class=\"hdlist1\">CodeFlare SDK</dt>\n<dd>\n<p>Defines and controls the remote distributed compute jobs and infrastructure for any Python-based environment</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>The CodeFlare SDK is not installed as part of Open Data Hub, but it is contained in some of the notebook images provided by Open Data Hub.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</dd>\n<dt class=\"hdlist1\">KubeRay</dt>\n<dd>\n<p>Manages remote Ray clusters on OpenShift for running distributed compute workloads</p>\n</dd>\n<dt class=\"hdlist1\">Kueue</dt>\n<dd>\n<p>Manages quotas and how distributed workloads consume them, and manages the queueing of distributed workloads with respect to quotas</p>\n</dd>\n</dl>\n</div>\n<div class=\"paragraph\">\n<p>You can run distributed workloads from data science pipelines, from Jupyter notebooks, or from Microsoft Visual Studio Code files.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Data science pipelines workloads are not managed by the distributed workloads feature, and are not included in the distributed workloads metrics.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"sect2\">\n<h3 id=\"overview-of-kueue-resources_distributed-workloads\">Overview of Kueue resources</h3>\n<div class=\"paragraph _abstract\">\n<p>Cluster administrators can configure Kueue resource flavors, cluster queues, and local queues to manage distributed workload resources across multiple nodes in an OpenShift cluster.</p>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_resource_flavor\">Resource flavor</h4>\n<div class=\"paragraph\">\n<p>The Kueue <code>ResourceFlavor</code> object describes the resource variations that are available in a cluster.</p>\n</div>\n<div class=\"paragraph\">\n<p>Resources in a cluster can be <em>homogenous</em> or <em>heterogeneous</em>:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Homogeneous resources are identical across the cluster: same node type, CPUs, memory, accelerators, and so on.</p>\n</li>\n<li>\n<p>Heterogeneous resources have variations across the cluster.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>If a cluster has homogeneous resources, or if it is not necessary to manage separate quotas for different flavors of a resource, a cluster administrator can create an empty <code>ResourceFlavor</code> object named <code>default-flavor</code>, without any labels or taints, as follows:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Empty Kueue resource flavor for homegeneous resources</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">apiVersion: kueue.x-k8s.io/v1beta1\nkind: ResourceFlavor\nmetadata:\n  name: default-flavor</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>If a cluster has heterogeneous resources, cluster administrators can define a different resource flavor for each variation in the resources available.\nExample variations include different CPUs, different memory, or different accelerators.\nCluster administrators can then associate the resource flavors with cluster nodes by using labels, taints, and tolerations, as shown in the following example.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Example Kueue resource flavor for heterogeneous resources</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">apiVersion: kueue.x-k8s.io/v1beta1\nkind: ResourceFlavor\nmetadata:\n  name: \"spot\"\nspec:\n  nodeLabels:\n    instance-type: spot\n  nodeTaints:\n  - effect: NoSchedule\n    key: spot\n    value: \"true\"\n  tolerations:\n  - key: \"spot-taint\"\n    operator: \"Exists\"\n    effect: \"NoSchedule\"</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>For more information about configuring resource flavors, see <a href=\"https://kueue.sigs.k8s.io/docs/concepts/resource_flavor/\">Resource Flavor</a> in the Kueue documentation.</p>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_cluster_queue\">Cluster queue</h4>\n<div class=\"paragraph\">\n<p>The Kueue <code>ClusterQueue</code> object manages a pool of cluster resources such as pods, CPUs, memory, and accelerators.\nA cluster can have multiple cluster queues, and each cluster queue can reference multiple resource flavors.</p>\n</div>\n<div class=\"paragraph\">\n<p>Cluster administrators can configure cluster queues to define the resource flavors that the queue manages, and assign a quota for each resource in each resource flavor.\nCluster administrators can also configure usage limits and queueing strategies to apply fair sharing rules across multiple cluster queues in a cluster.</p>\n</div>\n<div class=\"paragraph\">\n<p>The following example configures a cluster queue to assign a quota of 9 CPUs, 36 GiB memory, 5 pods, and 5 NVIDIA GPUs.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Example cluster queue</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">apiVersion: kueue.x-k8s.io/v1beta1\nkind: ClusterQueue\nmetadata:\n  name: \"cluster-queue\"\nspec:\n  namespaceSelector: {} # match all.\n  resourceGroups:\n  - coveredResources: [\"cpu\", \"memory\", \"pods\", \"nvidia.com/gpu\"]\n    flavors:\n    - name: \"default-flavor\"\n      resources:\n      - name: \"cpu\"\n        nominalQuota: 9\n      - name: \"memory\"\n        nominalQuota: 36Gi\n      - name: \"pods\"\n        nominalQuota: 5\n      - name: \"nvidia.com/gpu\"\n        nominalQuota: '5'</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The cluster queue starts a distributed workload only if the total required resources are within these quota limits.\nIf the sum of the requests for a resource in a distributed workload is greater than the specified quota for that resource in the cluster queue, the cluster queue does not admit the distributed workload.</p>\n</div>\n<div class=\"paragraph\">\n<p>For more information about configuring cluster queues, see <a href=\"https://kueue.sigs.k8s.io/docs/concepts/cluster_queue/\">Cluster Queue</a> in the Kueue documentation.</p>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_local_queue\">Local queue</h4>\n<div class=\"paragraph\">\n<p>The Kueue <code>LocalQueue</code> object groups closely related distributed workloads in a project.\nCluster administrators can configure local queues to specify the project name and the associated cluster queue.\nEach local queue then grants access to the resources that its specified cluster queue manages.\nA cluster administrator can optionally define one local queue in a project as the default local queue for that project.</p>\n</div>\n<div class=\"paragraph\">\n<p>When configuring a distributed workload, the user specifies the local queue name.\nIf a cluster administrator configured a default local queue, the user can omit the local queue specification from the distributed workload code.</p>\n</div>\n<div class=\"paragraph\">\n<p>Kueue allocates the resources for a distributed workload from the cluster queue that is associated with the local queue, if the total requested resources are within the quota limits specified in that cluster queue.</p>\n</div>\n<div class=\"paragraph\">\n<p>The following example configures a local queue called <code>team-a-queue</code> for the <code>team-a</code> project, and specifies <code>cluster-queue</code> as the associated cluster queue.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Example local queue</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">apiVersion: kueue.x-k8s.io/v1beta1\nkind: LocalQueue\nmetadata:\n  namespace: team-a\n  name: team-a-queue\n  annotations:\n    kueue.x-k8s.io/default-queue: \"true\"\nspec:\n  clusterQueue: cluster-queue</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>In this example, the <code>kueue.x-k8s.io/default-queue: \"true\"</code> annotation defines this local queue as the default local queue for the <code>team-a</code> project.\nIf a user submits a distributed workload in the <code>team-a</code> project and that distributed workload does not specify a local queue in the cluster configuration, Kueue automatically routes the distributed workload to the <code>team-a-queue</code> local queue.\nThe distributed workload can then access the resources that the <code>cluster-queue</code> cluster queue manages.</p>\n</div>\n<div class=\"paragraph\">\n<p>For more information about configuring local queues, see <a href=\"https://kueue.sigs.k8s.io/docs/concepts/local_queue/\">Local Queue</a> in the Kueue documentation.</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"configuring-distributed-workloads_distributed-workloads\">Configuring distributed workloads</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>To configure the distributed workloads feature for your data scientists to use in Open Data Hub, you must enable several components in the Open Data Hub Operator, create the required Kueue resources, and optionally configure the CodeFlare Operator.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"configuring-the-distributed-workloads-components_distributed-workloads\">Configuring the distributed workloads components</h3>\n<div class=\"paragraph _abstract\">\n<p>To configure the distributed workloads feature for your data scientists to use in Open Data Hub, you must enable several components.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to OpenShift Container Platform with the <code>cluster-admin</code> role.</p>\n</li>\n<li>\n<p>You have access to the data science cluster.</p>\n</li>\n<li>\n<p>You have installed Open Data Hub.</p>\n</li>\n<li>\n<p>You have sufficient resources. In addition to the minimum Open Data Hub resources described in <a href=\"https://opendatahub.io/docs/installing-open-data-hub/#installing-the-odh-operator-v2_installv2\">Installing the Open Data Hub Operator version 2</a>, you need 1.6 vCPU and 2 GiB memory to deploy the distributed workloads infrastructure.</p>\n</li>\n<li>\n<p>You have access to a Ray cluster image. For information about how to create a Ray cluster, see the <a href=\"https://docs.ray.io/en/latest/cluster/getting-started.html\">Ray Clusters documentation</a>.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Mutual Transport Layer Security (mTLS) is enabled by default in the CodeFlare component in Open Data Hub.\nOpen Data Hub 2 does not support the <code>submissionMode=K8sJobMode</code> setting in the Ray job specification, so the KubeRay Operator cannot create a submitter Kubernetes Job to submit the Ray job.\nInstead, users must configure the Ray job specification to set <code>submissionMode=HTTPMode</code> only, so that the KubeRay Operator sends a request to the RayCluster to create a Ray job.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>You have access to the data sets and models that the distributed workload uses.</p>\n</li>\n<li>\n<p>You have access to the Python dependencies for the distributed workload.</p>\n</li>\n<li>\n<p>You have removed any previously installed instances of the CodeFlare Operator.</p>\n</li>\n<li>\n<p>If you want to use graphics processing units (GPUs), you have enabled GPU support.\nThis process includes installing the Node Feature Discovery Operator and the NVIDIA GPU Operator.\nFor more information, see <a href=\"https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html\" target=\"_blank\" rel=\"noopener\">NVIDIA GPU Operator on Red&#160;Hat OpenShift Container Platform</a> in the NVIDIA documentation.</p>\n</li>\n<li>\n<p>If you want to use self-signed certificates, you have added them to a central Certificate Authority (CA) bundle as described in <a href=\"https://opendatahub.io/docs/installing-open-data-hub/#understanding-certificates_certs\">Understanding certificates in Open Data Hub</a>.\nNo additional configuration is necessary to use those certificates with distributed workloads.\nThe centrally configured self-signed certificates are automatically available in the workload pods at the following mount points:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Cluster-wide CA bundle:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">/etc/pki/tls/certs/odh-trusted-ca-bundle.crt\n/etc/ssl/certs/odh-trusted-ca-bundle.crt</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Custom CA bundle:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">/etc/pki/tls/certs/odh-ca-bundle.crt\n/etc/ssl/certs/odh-ca-bundle.crt</code></pre>\n</div>\n</div>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift Container Platform console, click <strong>Operators</strong> &#8594; <strong>Installed Operators</strong>.</p>\n</li>\n<li>\n<p>Search for the <strong>Open Data Hub Operator</strong>, and then click the Operator name to open the Operator details page.</p>\n</li>\n<li>\n<p>Click the <strong>Data Science Cluster</strong> tab.</p>\n</li>\n<li>\n<p>Click the default instance name (for example, <strong>default-dsc</strong>) to open the instance details page.</p>\n</li>\n<li>\n<p>Click the <strong>YAML</strong> tab to show the instance specifications.</p>\n</li>\n<li>\n<p>Enable the required distributed workloads components.\nIn the <code>spec:components</code> section, set the <code>managementState</code> field correctly for the required components.\nThe <code>trainingoperator</code> component is required only if you want to use the Kubeflow Training Operator to tune models.\nThe list of required components depends on whether the distributed workload is run from a pipeline or notebook or both, as shown in the following table.</p>\n<table class=\"tableblock frame-all grid-all stretch\">\n<caption class=\"title\">Table 1. Components required for distributed workloads</caption>\n<colgroup>\n<col style=\"width: 36%;\">\n<col style=\"width: 19%;\">\n<col style=\"width: 19%;\">\n<col style=\"width: 26%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Component</th>\n<th class=\"tableblock halign-left valign-top\">Pipelines only</th>\n<th class=\"tableblock halign-left valign-top\">Notebooks only</th>\n<th class=\"tableblock halign-left valign-top\">Pipelines and notebooks</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>codeflare</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>dashboard</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>datasciencepipelines</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Removed</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>kueue</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>ray</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>trainingoperator</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>workbenches</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Removed</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n</tr>\n</tbody>\n</table>\n</li>\n<li>\n<p>Click <strong>Save</strong>.\nAfter a short time, the components with a <code>Managed</code> state are ready.</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>Check the status of the <strong>codeflare-operator-manager</strong>, <strong>kuberay-operator</strong>, and <strong>kueue-controller-manager</strong> pods, as follows:</p>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift Container Platform console, from the <strong>Project</strong> list, select <strong>odh</strong>.</p>\n</li>\n<li>\n<p>Click <strong>Workloads</strong> &#8594; <strong>Deployments</strong>.</p>\n</li>\n<li>\n<p>Search for the <strong>codeflare-operator-manager</strong>, <strong>kuberay-operator</strong>, and <strong>kueue-controller-manager</strong> deployments.\nIn each case, check the status as follows:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click the deployment name to open the deployment details page.</p>\n</li>\n<li>\n<p>Click the <strong>Pods</strong> tab.</p>\n</li>\n<li>\n<p>Check the pod status.</p>\n<div class=\"paragraph\">\n<p>When the status of the <strong>codeflare-operator-manager-<em>&lt;pod-id&gt;</em></strong>, <strong>kuberay-operator-<em>&lt;pod-id&gt;</em></strong>, and <strong>kueue-controller-manager-<em>&lt;pod-id&gt;</em></strong> pods is <strong>Running</strong>, the pods are ready to use.</p>\n</div>\n</li>\n<li>\n<p>To see more information about each pod, click the pod name to open the pod details page, and then click the <strong>Logs</strong> tab.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"configuring-quota-management-for-distributed-workloads_distributed-workloads\">Configuring quota management for distributed workloads</h3>\n<div class=\"paragraph _abstract\">\n<p>Configure quotas for distributed workloads on a cluster, so that you can share resources between several data science projects.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have enabled the required distributed workloads components as described in <a href=\"https://opendatahub.io/docs/working-with-distributed-workloads/#configuring-the-distributed-workloads-components_distributed-workloads\">Configuring the distributed workloads components</a>.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench, and the workbench is running a default notebook image that contains the CodeFlare SDK, for example, the <strong>Standard Data Science</strong> notebook. For information about how to create a project, see <a href=\"https://opendatahub.io/docs/working-on-data-science-projects/#creating-a-data-science-project_projects\">Creating a data science project</a>.</p>\n</li>\n<li>\n<p>You have sufficient resources. In addition to the base Open Data Hub resources, you need 1.6 vCPU and 2 GiB memory to deploy the distributed workloads infrastructure.</p>\n</li>\n<li>\n<p>The resources are physically available in the cluster.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>For more information about Kueue resources, see <a href=\"https://opendatahub.io/docs/working-with-distributed-workloads/#overview-of-kueue-resources_distributed-workloads\">Overview of Kueue resources</a>.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>If you want to use graphics processing units (GPUs), you have enabled GPU support.\nThis process includes installing the Node Feature Discovery Operator and the NVIDIA GPU Operator.\nFor more information, see <a href=\"https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html\" target=\"_blank\" rel=\"noopener\">NVIDIA GPU Operator on Red&#160;Hat OpenShift Container Platform</a> in the NVIDIA documentation.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In a terminal window, if you are not already logged in to your OpenShift cluster as a cluster administrator, log in to the OpenShift CLI as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc login <em>&lt;openshift_cluster_url&gt;</em> -u <em>&lt;admin_username&gt;</em> -p <em>&lt;password&gt;</em></code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Create an empty Kueue resource flavor, as follows:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Create a file called <code>default_flavor.yaml</code> and populate it with the following content:</p>\n<div class=\"listingblock\">\n<div class=\"title\">Empty Kueue resource flavor</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">apiVersion: kueue.x-k8s.io/v1beta1\nkind: ResourceFlavor\nmetadata:\n  name: default-flavor</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Apply the configuration to create the <code>default-flavor</code> object:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">$ oc apply -f default_flavor.yaml</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Create a cluster queue to manage the empty Kueue resource flavor, as follows:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Create a file called <code>cluster_queue.yaml</code> and populate it with the following content:</p>\n<div class=\"listingblock\">\n<div class=\"title\">Example cluster queue</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">apiVersion: kueue.x-k8s.io/v1beta1\nkind: ClusterQueue\nmetadata:\n  name: \"cluster-queue\"\nspec:\n  namespaceSelector: {}  # match all.\n  resourceGroups:\n  - coveredResources: [\"cpu\", \"memory\", \"nvidia.com/gpu\"]\n    flavors:\n    - name: \"default-flavor\"\n      resources:\n      - name: \"cpu\"\n        nominalQuota: 9\n      - name: \"memory\"\n        nominalQuota: 36Gi\n      - name: \"nvidia.com/gpu\"\n        nominalQuota: 5</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Replace the example quota values (9 CPUs, 36 GiB memory, and 5 NVIDIA GPUs) with the appropriate values for your cluster queue.\nThe cluster queue will start a distributed workload only if the total required resources are within these quota limits.</p>\n<div class=\"paragraph\">\n<p>You must specify a quota for each resource that the user can request, even if the requested value is 0, by updating the <code>spec.resourceGroups</code> section as follows:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Include the resource name in the <code>coveredResources</code> list.</p>\n</li>\n<li>\n<p>Specify the resource <code>name</code> and <code>nominalQuota</code> in the <code>flavors.resources</code> section, even if the <code>nominalQuota</code> value is 0.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Apply the configuration to create the <code>cluster-queue</code> object:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">$ oc apply -f cluster_queue.yaml</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Create a local queue that points to your cluster queue, as follows:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Create a file called <code>local_queue.yaml</code> and populate it with the following content:</p>\n<div class=\"listingblock\">\n<div class=\"title\">Example local queue</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">apiVersion: kueue.x-k8s.io/v1beta1\nkind: LocalQueue\nmetadata:\n  namespace: test\n  name: local-queue-test\n  annotations:\n    kueue.x-k8s.io/default-queue: 'true'\nspec:\n  clusterQueue: cluster-queue</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The <code>kueue.x-k8s.io/default-queue: 'true'</code> annotation defines this queue as the default queue.\nDistributed workloads are submitted to this queue if no <code>local_queue</code> value is specified in the <code>ClusterConfiguration</code> section of the data science pipeline or Jupyter notebook or Microsoft Visual Studio Code file.</p>\n</div>\n</li>\n<li>\n<p>Update the <code>namespace</code> value to specify the same namespace as in the <code>ClusterConfiguration</code> section that creates the Ray cluster.</p>\n</li>\n<li>\n<p>Optional: Update the <code>name</code> value accordingly.</p>\n</li>\n<li>\n<p>Apply the configuration to create the local-queue object:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">$ oc apply -f local_queue.yaml</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The cluster queue allocates the resources to run distributed workloads in the local queue.</p>\n</div>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>Check the status of the local queue in a project, as follows:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc get -n <em>&lt;project-name&gt;</em> localqueues</code></pre>\n</div>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://kueue.sigs.k8s.io/docs/concepts/\">Kueue documentation</a></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"configuring-the-codeflare-operator_distributed-workloads\">Configuring the CodeFlare Operator</h3>\n<div class=\"paragraph _abstract\">\n<p>If you want to change the default configuration of the CodeFlare Operator for distributed workloads in Open Data Hub, you can edit the associated config map.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to OpenShift Container Platform with the <code>cluster-admin</code> role.</p>\n</li>\n<li>\n<p>You have enabled the required distributed workloads components as described in <a href=\"https://opendatahub.io/docs/working-with-distributed-workloads/#configuring-the-distributed-workloads-components_distributed-workloads\">Configuring the distributed workloads components</a>.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift Container Platform console, click <strong>Workloads</strong> &#8594; <strong>ConfigMaps</strong>.</p>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select <strong>odh</strong>.</p>\n</li>\n<li>\n<p>Search for the <strong>codeflare-operator-config</strong> config map, and click the config map name to open the <strong>ConfigMap details</strong> page.</p>\n</li>\n<li>\n<p>Click the <strong>YAML</strong> tab to show the config map specifications.</p>\n</li>\n<li>\n<p>In the <code>data:config.yaml:kuberay</code> section, you can edit the following entries:</p>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">ingressDomain</dt>\n<dd>\n<p>This configuration option is null (<code>ingressDomain: \"\"</code>) by default.\nDo not change this option unless the Ingress Controller is not running on OpenShift.\nOpen Data Hub uses this value to generate the dashboard and client routes for every Ray Cluster, as shown in the following examples:</p>\n<div class=\"listingblock\">\n<div class=\"title\">Example dashboard and client routes</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">ray-dashboard-<em>&lt;clustername&gt;</em>-<em>&lt;namespace&gt;</em>.<em>&lt;your.ingress.domain&gt;</em>\nray-client-<em>&lt;clustername&gt;</em>-<em>&lt;namespace&gt;</em>.<em>&lt;your.ingress.domain&gt;</em></code></pre>\n</div>\n</div>\n</dd>\n<dt class=\"hdlist1\">mTLSEnabled</dt>\n<dd>\n<p>This configuration option is enabled (<code>mTLSEnabled: true</code>) by default.\nWhen this option is enabled, the Ray Cluster pods create certificates that are used for mutual Transport Layer Security (mTLS), a form of mutual authentication, between Ray Cluster nodes.\nWhen this option is enabled, Ray clients cannot connect to the Ray head node unless they download the generated certificates from the <code>ca-secret-_&lt;cluster_name&gt;_</code> secret, generate the necessary certificates for mTLS communication, and then set the required Ray environment variables.\nUsers must then re-initialize the Ray clients to apply the changes.\nThe CodeFlare SDK provides the following functions to simplify the authentication process for Ray clients:</p>\n<div class=\"listingblock\">\n<div class=\"title\">Example Ray client authentication code</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">from codeflare_sdk import generate_cert\n\ngenerate_cert.generate_tls_cert(cluster.config.name, cluster.config.namespace)\ngenerate_cert.export_env(cluster.config.name, cluster.config.namespace)\n\nray.init(cluster.cluster_uri())</code></pre>\n</div>\n</div>\n</dd>\n<dt class=\"hdlist1\">rayDashboardOauthEnabled</dt>\n<dd>\n<p>This configuration option is enabled (<code>rayDashboardOAuthEnabled: true</code>) by default.\nWhen this option is enabled, Open Data Hub places an OpenShift OAuth proxy in front of the Ray Cluster head node.\nUsers must then authenticate by using their OpenShift cluster login credentials when accessing the Ray Dashboard through the browser.\nIf users want to access the Ray Dashboard in another way (for example, by using the Ray <code>JobSubmissionClient</code> class), they must set an authorization header as part of their request, as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"title\">Example authorization header</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">{Authorization: \"Bearer <em>&lt;your-openshift-token&gt;</em>\"}</code></pre>\n</div>\n</div>\n</dd>\n</dl>\n</div>\n</li>\n<li>\n<p>To save your changes, click <strong>Save</strong>.</p>\n</li>\n<li>\n<p>To apply your changes, delete the pod:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click <strong>Workloads</strong> &#8594; <strong>Pods</strong>.</p>\n</li>\n<li>\n<p>Find the <strong>codeflare-operator-manager-<em>&lt;pod-id&gt;</em></strong> pod.</p>\n</li>\n<li>\n<p>Click the options menu (&#8942;) for that pod, and then click <strong>Delete Pod</strong>.\nThe pod restarts with your changes applied.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>Check the status of the <strong>codeflare-operator-manager</strong> pod, as follows:</p>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift Container Platform console, click <strong>Workloads</strong> &#8594; <strong>Deployments</strong>.</p>\n</li>\n<li>\n<p>Search for the <strong>codeflare-operator-manager</strong> deployment, and then click the deployment name to open the deployment details page.</p>\n</li>\n<li>\n<p>Click the <strong>Pods</strong> tab.\nWhen the status of the <strong>codeflare-operator-manager-<em>&lt;pod-id&gt;</em></strong> pod is <strong>Running</strong>, the pod is ready to use.\nTo see more information about the pod, click the pod name to open the pod details page, and then click the <strong>Logs</strong> tab.</p>\n</li>\n</ol>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"running-distributed-workloads_distributed-workloads\">Running distributed workloads</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>In Open Data Hub, you can run a distributed workload from a notebook or from a pipeline.\nYou can also run distributed workloads in a disconnected environment if you have access to all of the required software.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"running-distributed-data-science-workloads-from-notebooks_distributed-workloads\">Running distributed data science workloads from notebooks</h3>\n<div class=\"paragraph _abstract\">\n<p>To run a distributed data science workload from a notebook, you must first provide the link to your Ray cluster image.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have access to a data science cluster that is configured to run distributed workloads as described in <a href=\"https://opendatahub.io/docs/working-with-distributed-workloads/#configuring-distributed-workloads_distributed-workloads\">Configuring distributed workloads</a>.</p>\n</li>\n<li>\n<p>Your cluster administrator has created the required Kueue resources as described in <a href=\"https://opendatahub.io/docs/working-with-distributed-workloads/#configuring-quota-management-for-distributed-workloads_distributed-workloads\">Configuring quota management for distributed workloads</a>.</p>\n</li>\n<li>\n<p>Optional: Your cluster administrator has defined a <em>default</em> local queue for the Ray cluster by creating a <code>LocalQueue</code> resource and adding the following annotation to its configuration details, as described in <a href=\"https://opendatahub.io/docs/working-with-distributed-workloads/#configuring-quota-management-for-distributed-workloads_distributed-workloads\">Configuring quota management for distributed workloads</a>:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">\"kueue.x-k8s.io/default-queue\": \"true\"</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If your cluster administrator does not define a default local queue, you must specify a local queue in each notebook.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench, and the workbench is running a default notebook image that contains the CodeFlare SDK, for example, the <strong>Standard Data Science</strong> notebook.\nFor information about how to create a project, see <a href=\"https://opendatahub.io/docs/working-on-data-science-projects/#creating-a-data-science-project_projects\">Creating a data science project</a>.</p>\n</li>\n<li>\n<p>You have Admin access for the data science project.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>If you created the project, you automatically have Admin access.</p>\n</li>\n<li>\n<p>If you did not create the project, your cluster administrator must give you Admin access.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>You have launched your notebook server and logged in to your notebook editor.\nThe examples in this procedure refer to the JupyterLab integrated development environment (IDE).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Download the demo notebooks provided by the CodeFlare SDK.\nThe demo notebooks provide guidelines for how to use the CodeFlare stack in your own notebooks.</p>\n<div class=\"paragraph\">\n<p>To access the demo notebooks, run the <code>codeflare_sdk.copy_demo_nbs()</code> function in a Jupyter notebook.\nThe function copies the demo notebooks that are packaged with the currently installed version of the CodeFlare SDK, and clones them into the target location.</p>\n</div>\n<div class=\"paragraph\">\n<p>The <code>codeflare_sdk.copy_demo_nbs()</code> function accepts two arguments:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>dir: str = \"./demo-notebooks\"</code> specifies the target location for the cloned demo notebooks.\nIf the leading character is a forward slash (/), the path is absolute.\nOtherwise, the path is relative to the current directory.\nThe default value is <code>\"./demo-notebooks\"</code>.</p>\n</li>\n<li>\n<p><code>overwrite: bool = False</code> specifies whether the function should overwrite the contents of an existing directory.\nWhen this argument is set to <code>True</code>, the function overwrites your customizations in the original demo files.\nThe default value is <code>False</code>.</p>\n<div class=\"paragraph\">\n<p>Examples:</p>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>codeflare_sdk.copy_demo_nbs(\"my-dir\")\ncodeflare_sdk.copy_demo_nbs(\"/absolute/path/to/target-dir\")\ncodeflare_sdk.copy_demo_nbs(\"my-dir\", True)\ncodeflare_sdk.copy_demo_nbs(dir=\"my-dir\", overwrite=True)</pre>\n</div>\n</div>\n</li>\n<li>\n<p>Locate the downloaded demo notebooks in the JupyterLab interface.\nIn the left navigation pane, double-click <strong>guided-demos</strong>.</p>\n</li>\n<li>\n<p>Update each example demo notebook as follows:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>If not already specified, update the <code>import</code> section to import the <code>generate_cert</code> component:</p>\n<div class=\"listingblock\">\n<div class=\"title\">Updated import section</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">from codeflare_sdk import generate_cert</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Replace the default namespace value with the name of your data science project.</p>\n</li>\n<li>\n<p>In the <code>TokenAuthentication</code> section of your notebook code, provide the token and server details to authenticate to the OpenShift cluster by using the CodeFlare SDK.</p>\n</li>\n<li>\n<p>Replace the link to the example community image with a link to your Ray cluster image.</p>\n</li>\n<li>\n<p>Ensure that the following Ray cluster authentication code is included after the Ray cluster creation section.</p>\n<div class=\"listingblock\">\n<div class=\"title\">Ray cluster authentication code</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">generate_cert.generate_tls_cert(cluster.config.name, cluster.config.namespace)\ngenerate_cert.export_env(cluster.config.name, cluster.config.namespace)</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Mutual Transport Layer Security (mTLS) is enabled by default in the CodeFlare component in Open Data Hub.\nYou must include the Ray cluster authentication code to enable the Ray client that runs within a notebook to connect to a secure Ray cluster that has mTLS enabled.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>If you have not configured a default local queue by including the <code>kueue.x-k8s.io/default-queue: 'true'</code> annotation as described in <a href=\"https://opendatahub.io/docs/working-with-distributed-workloads/#configuring-quota-management-for-distributed-workloads_distributed-workloads\">Configuring quota management for distributed workloads</a>, update the <code>ClusterConfiguration</code> section to specify the local queue for the Ray cluster, as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"title\">Example local queue assignment</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">local_queue=\"your_local_queue_name\"</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Optional: In the <code>ClusterConfiguration</code> section, assign a dictionary of <code>labels</code> parameters to the Ray cluster for identification and management purposes, as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"title\">Example labels assignment</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">labels = {\"exampleLabel1\": \"exampleLabel1Value\", \"exampleLabel2\": \"exampleLabel2Value\"}</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Run the notebooks.</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>The notebooks run to completion without errors. In the notebooks, the output from the <code>cluster.status()</code> function or <code>cluster.details()</code> function indicates that the Ray cluster is <code>Active</code>.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"running-distributed-data-science-workloads-from-ds-pipelines_distributed-workloads\">Running distributed data science workloads from data science pipelines</h3>\n<div class=\"paragraph _abstract\">\n<p>To run a distributed data science workload from a data science pipeline, you must first update the pipeline to include a link to your Ray cluster image.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have access to a data science cluster that is configured to run distributed workloads as described in <a href=\"https://opendatahub.io/docs/working-with-distributed-workloads/#configuring-distributed-workloads_distributed-workloads\">Configuring distributed workloads</a>.</p>\n</li>\n<li>\n<p>Your cluster administrator has created the required Kueue resources as described in <a href=\"https://opendatahub.io/docs/working-with-distributed-workloads/#configuring-quota-management-for-distributed-workloads_distributed-workloads\">Configuring quota management for distributed workloads</a>.</p>\n</li>\n<li>\n<p>Optional: Your cluster administrator has defined a <em>default</em> local queue for the Ray cluster by creating a <code>LocalQueue</code> resource and adding the following annotation to the configuration details for that <code>LocalQueue</code> resource, as described in <a href=\"https://opendatahub.io/docs/working-with-distributed-workloads/#configuring-quota-management-for-distributed-workloads_distributed-workloads\">Configuring quota management for distributed workloads</a>:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">\"kueue.x-k8s.io/default-queue\": \"true\"</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If your cluster administrator does not define a default local queue, you must specify a local queue in each pipeline.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>You have access to S3-compatible object storage.</p>\n</li>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench, and the workbench is running a default notebook image that contains the CodeFlare SDK, for example, the <strong>Standard Data Science</strong> notebook. For information about how to create a project, see <a href=\"https://opendatahub.io/docs/working-on-data-science-projects/#creating-a-data-science-project_projects\">Creating a data science project</a>.</p>\n</li>\n<li>\n<p>You have Admin access for the data science project.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>If you created the project, you automatically have Admin access.</p>\n</li>\n<li>\n<p>If you did not create the project, your cluster administrator must give you Admin access.</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Create a data connection to connect the object storage to your data science project, as described in <a href=\"https://opendatahub.io/docs/working-on-data-science-projects/#adding-a-data-connection-to-your-data-science-project_projects\">Adding a data connection to your data science project</a>.</p>\n</li>\n<li>\n<p>Configure a pipeline server to use the data connection, as described in <a href=\"https://opendatahub.io/docs/working-with-data-science-pipelines/#configuring-a-pipeline-server_ds-pipelines\">Configuring a pipeline server</a>.</p>\n</li>\n<li>\n<p>Create the data science pipeline as follows:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Install the <code>kfp</code> Python package, which is required for all pipelines:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">$ pip install kfp</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Install any other dependencies that are required for your pipeline.</p>\n</li>\n<li>\n<p>Build your data science pipeline in Python code.</p>\n<div class=\"paragraph\">\n<p>For example, create a file named <code>compile_example.py</code> with the following content:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-Python\" data-lang=\"Python\">from kfp import dsl\n\n\n@dsl.component(\n    base_image=\"registry.redhat.io/ubi8/python-39:latest\",\n    packages_to_install=['codeflare-sdk']\n)\n\n\ndef ray_fn():\n   import ray <b class=\"conum\">(1)</b>\n   from codeflare_sdk import Cluster, ClusterConfiguration, generate_cert <b class=\"conum\">(2)</b>\n\n\n   cluster = Cluster( <b class=\"conum\">(3)</b>\n       ClusterConfiguration(\n           namespace=\"my_project\", <b class=\"conum\">(4)</b>\n           name=\"raytest\",\n           num_workers=1,\n           head_cpus=\"500m\",\n           min_memory=1,\n           max_memory=1,\n           worker_extended_resource_requests={nvidia.com/gpu: 1}, <b class=\"conum\">(5)</b>\n           image=\"quay.io/rhoai/ray:2.23.0-py39-cu121\", <b class=\"conum\">(6)</b>\n           local_queue=\"local_queue_name\", <b class=\"conum\">(7)</b>\n       )\n   )\n\n\n   print(cluster.status())\n   cluster.up() <b class=\"conum\">(8)</b>\n   cluster.wait_ready() <b class=\"conum\">(9)</b>\n   print(cluster.status())\n   print(cluster.details())\n\n\n   ray_dashboard_uri = cluster.cluster_dashboard_uri()\n   ray_cluster_uri = cluster.cluster_uri()\n   print(ray_dashboard_uri, ray_cluster_uri)\n\n   # Enable Ray client to connect to secure Ray cluster that has mTLS enabled\n   generate_cert.generate_tls_cert(cluster.config.name, cluster.config.namespace) <b class=\"conum\">(10)</b>\n   generate_cert.export_env(cluster.config.name, cluster.config.namespace)\n\n\n   ray.init(address=ray_cluster_uri)\n   print(\"Ray cluster is up and running: \", ray.is_initialized())\n\n\n   @ray.remote\n   def train_fn(): <b class=\"conum\">(11)</b>\n       # complex training function\n       return 100\n\n\n   result = ray.get(train_fn.remote())\n   assert 100 == result\n   ray.shutdown()\n   cluster.down() <b class=\"conum\">(12)</b>\n   auth.logout()\n   return result\n\n\n@dsl.pipeline( <b class=\"conum\">(13)</b>\n   name=\"Ray Simple Example\",\n   description=\"Ray Simple Example\",\n)\n\n\ndef ray_integration():\n   ray_fn()\n\n\nif __name__ == '__main__': <b class=\"conum\">(14)</b>\n    from kfp.compiler import Compiler\n    Compiler().compile(ray_integration, 'compiled-example.yaml')</code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<ol>\n<li>\n<p>Imports Ray.</p>\n</li>\n<li>\n<p>Imports packages from the CodeFlare SDK to define the cluster functions.</p>\n</li>\n<li>\n<p>Specifies the Ray cluster configuration: replace these example values with the values for your Ray cluster.</p>\n</li>\n<li>\n<p>Optional: Specifies the project where the Ray cluster is created. Replace the example value with the name of your project. If you omit this line, the Ray cluster is created in the current project.</p>\n</li>\n<li>\n<p>Optional: Specifies the requested accelerators for the Ray cluster (in this example, 1 NVIDIA GPU).\nIf no accelerators are required, set the value to 0 or omit the line.\nNote: To specify the requested accelerators for the Ray cluster, use the <code>worker_extended_resource_requests</code> parameter instead of the deprecated <code>num_gpus</code> parameter.\nFor more details, see the <a href=\"https://github.com/project-codeflare/codeflare-sdk/blob/v0.18.0/src/codeflare_sdk/cluster/config.py#L43-L73\">CodeFlare SDK documentation</a>.</p>\n</li>\n<li>\n<p>Specifies the location of the Ray cluster image. If you are running this code in a disconnected environment, replace the default value with the location for your environment.</p>\n</li>\n<li>\n<p>Specifies the local queue to which the Ray cluster will be submitted. If a default local queue is configured, you can omit this line.</p>\n</li>\n<li>\n<p>Creates a Ray cluster by using the specified image and configuration.</p>\n</li>\n<li>\n<p>Waits until the Ray cluster is ready before proceeding.</p>\n</li>\n<li>\n<p>Enables the Ray client to connect to a secure Ray cluster that has mutual Transport Layer Security (mTLS) enabled. mTLS is enabled by default in the CodeFlare component in Open Data Hub.</p>\n</li>\n<li>\n<p>Replace the example details in this section with the details for your workload.</p>\n</li>\n<li>\n<p>Removes the Ray cluster when your workload is finished.</p>\n</li>\n<li>\n<p>Replace the example name and description with the values for your workload.</p>\n</li>\n<li>\n<p>Compiles the Python code and saves the output in a YAML file.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Compile the Python file (in this example, the <code>compile_example.py</code> file):</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">$ python compile_example.py</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>This command creates a YAML file (in this example, <code>compiled-example.yaml</code>), which you can import in the next step.</p>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Import your data science pipeline, as described in <a href=\"https://opendatahub.io/docs/working-with-data-science-pipelines/#importing-a-data-science-pipeline_ds-pipelines\">Importing a data science pipeline</a>.</p>\n</li>\n<li>\n<p>Schedule the pipeline run, as described in <a href=\"https://opendatahub.io/docs/working-with-data-science-pipelines/#scheduling-a-pipeline-run_ds-pipelines\">Scheduling a pipeline run</a>.</p>\n</li>\n<li>\n<p>When the pipeline run is complete, confirm that it is included in the list of triggered pipeline runs, as described in <a href=\"https://opendatahub.io/docs/working-with-data-science-pipelines/#viewing-the-details-of-a-pipeline-run_ds-pipelines\">Viewing the details of a pipeline run</a>.</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>The YAML file is created and the pipeline run completes without errors.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can view the run details, as described in <a href=\"https://opendatahub.io/docs/working-with-data-science-pipelines/#viewing-the-details-of-a-pipeline-run_ds-pipelines\">Viewing the details of a pipeline run</a>.</p>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://opendatahub.io/docs/working-with-data-science-pipelines/\">Working with data science pipelines</a></p>\n</li>\n<li>\n<p><a href=\"https://docs.ray.io/en/latest/cluster/getting-started.html\">Ray Clusters documentation</a></p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"monitoring-distributed-workloads_distributed-workloads\">Monitoring distributed workloads</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>In Open Data Hub, you can view project metrics for distributed workloads, and view the status of all distributed workloads in the selected project.\nYou can use these metrics to monitor the resources used by distributed workloads, assess whether project resources are allocated correctly, track the progress of distributed workloads, and identify corrective action when necessary.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Data science pipelines workloads are not managed by the distributed workloads feature, and are not included in the distributed workloads metrics.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-project-metrics-for-distributed-workloads_distributed-workloads\">Viewing project metrics for distributed workloads</h3>\n<div class=\"paragraph _abstract\">\n<p>In Open Data Hub, you can view the following project metrics for distributed workloads:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>CPU</strong> - The number of CPU cores that are currently being used by all distributed workloads in the selected project.</p>\n</li>\n<li>\n<p><strong>Memory</strong> - The amount of memory in gibibytes (GiB) that is currently being used by all distributed workloads in the selected project.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>You can use these metrics to monitor the resources used by the distributed workloads, and assess whether project resources are allocated correctly.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed Open Data Hub.</p>\n</li>\n<li>\n<p>On the OpenShift cluster where Open Data Hub is installed, user workload monitoring is enabled.</p>\n</li>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>Your data science project contains distributed workloads.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the Open Data Hub left navigation pane, click <strong>Distributed Workloads Metrics</strong>.</p>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project that contains the distributed workloads that you want to monitor.</p>\n</li>\n<li>\n<p>Click the <strong>Project metrics</strong> tab.</p>\n</li>\n<li>\n<p>Optional: From the <strong>Refresh interval</strong> list, select a value to specify how frequently the graphs on the metrics page are refreshed to show the latest data.</p>\n<div class=\"paragraph\">\n<p>You can select one of these values: <strong>15 seconds</strong>, <strong>30 seconds</strong>, <strong>1 minute</strong>, <strong>5 minutes</strong>, <strong>15 minutes</strong>, <strong>30 minutes</strong>, <strong>1 hour</strong>, <strong>2 hours</strong>, or <strong>1 day</strong>.</p>\n</div>\n</li>\n<li>\n<p>In the <strong>Requested resources</strong> section, review the <strong>CPU</strong> and <strong>Memory</strong> graphs to identify the resources requested by distributed workloads as follows:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Requested by the selected project</p>\n</li>\n<li>\n<p>Requested by all projects, including the selected project and projects that you cannot access</p>\n</li>\n<li>\n<p>Total shared quota for all projects, as provided by the cluster queue</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>For each resource type (<strong>CPU</strong> and <strong>Memory</strong>), subtract the <strong>Requested by all projects</strong> value from the <strong>Total shared quota</strong> value to calculate how much of that resource quota has not been requested and is available for all projects.</p>\n</div>\n</li>\n<li>\n<p>Scroll down to the <strong>Top resource-consuming distributed workloads</strong> section to review the following graphs:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Top 5 distributed workloads that are consuming the most CPU resources</p>\n</li>\n<li>\n<p>Top 5 distributed workloads that are consuming the most memory</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>You can also identify how much CPU or memory is used in each case.</p>\n</div>\n</li>\n<li>\n<p>Scroll down to view the <strong>Distributed workload resource metrics</strong> table, which lists all of the distributed workloads in the selected project, and indicates the current resource usage and the status of each distributed workload.</p>\n<div class=\"paragraph\">\n<p>In each table entry, progress bars indicate how much of the requested CPU and memory is currently being used by this distributed workload.\nTo see numeric values for the actual usage and requested usage for CPU (measured in cores) and memory (measured in GiB), hover the cursor over each progress bar.\nCompare the actual usage with the requested usage to assess the distributed workload configuration.\nIf necessary, reconfigure the distributed workload to reduce or increase the requested resources.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>On the <strong>Project metrics</strong> tab, the graphs and table provide resource-usage data for the distributed workloads in the selected project.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-the-status-of-distributed-workloads_distributed-workloads\">Viewing the status of distributed workloads</h3>\n<div class=\"paragraph _abstract\">\n<p>In Open Data Hub, you can view the status of all distributed workloads in the selected project.\nYou can track the progress of the distributed workloads, and identify corrective action when necessary.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed Open Data Hub.</p>\n</li>\n<li>\n<p>On the OpenShift cluster where Open Data Hub is installed, user workload monitoring is enabled.</p>\n</li>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>Your data science project contains distributed workloads.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the Open Data Hub left navigation pane, click <strong>Distributed Workloads Metrics</strong>.</p>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project that contains the distributed workloads that you want to monitor.</p>\n</li>\n<li>\n<p>Click the <strong>Distributed workload status</strong> tab.</p>\n</li>\n<li>\n<p>Optional: From the <strong>Refresh interval</strong> list, select a value to specify how frequently the graphs on the metrics page are refreshed to show the latest data.</p>\n<div class=\"paragraph\">\n<p>You can select one of these values: <strong>15 seconds</strong>, <strong>30 seconds</strong>, <strong>1 minute</strong>, <strong>5 minutes</strong>, <strong>15 minutes</strong>, <strong>30 minutes</strong>, <strong>1 hour</strong>, <strong>2 hours</strong>, or <strong>1 day</strong>.</p>\n</div>\n</li>\n<li>\n<p>In the <strong>Status overview</strong> section, review a summary of the status of all distributed workloads in the selected project.</p>\n<div class=\"paragraph\">\n<p>The status can be <strong>Pending</strong>, <strong>Inadmissible</strong>, <strong>Admitted</strong>, <strong>Running</strong>, <strong>Evicted</strong>, <strong>Succeeded</strong>, or <strong>Failed</strong>.</p>\n</div>\n</li>\n<li>\n<p>Scroll down to view the <strong>Distributed workloads</strong> table, which lists all of the distributed workloads in the selected project.\nThe table provides the priority, status, creation date, and latest message for each distributed workload.</p>\n<div class=\"paragraph\">\n<p>The latest message provides more information about the current status of the distributed workload.\nReview the latest message to identify any corrective action needed.\nFor example, a distributed workload might be <strong>Inadmissible</strong> because the requested resources exceed the available resources.\nIn such cases, you can either reconfigure the distributed workload to reduce the requested resources, or reconfigure the cluster queue for the project to increase the resource quota.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>On the <strong>Distributed workload status</strong> tab, the graph provides a summarized view of the status of all distributed workloads in the selected project, and the table provides more details about the status of each distributed workload.</p>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"tuning-a-model-by-using-the-training-operator_distributed-workloads\">Tuning a model by using the Training Operator</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>To tune a model by using the Kubeflow Training Operator, complete the following tasks:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Configure the Training Operator permissions (you can skip this task if you use Kueue to run PyTorch jobs)</p>\n</li>\n<li>\n<p>Configure the training job</p>\n</li>\n<li>\n<p>Run the training job</p>\n</li>\n<li>\n<p>Monitor the training job</p>\n</li>\n</ul>\n</div>\n<div class=\"sect2\">\n<h3 id=\"configuring-the-training-operator-permissions-when-not-using-kueue_distributed-workloads\">Configuring the Training Operator permissions when not using Kueue</h3>\n<div class=\"paragraph _abstract\">\n<p>If you want to run a PyTorch training job in an environment where Kueue is not installed, you must create a service account and apply access permissions to it.</p>\n</div>\n<div class=\"paragraph\">\n<p>If you use Kueue to run PyTorch jobs, you can skip this section.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to OpenShift Container Platform with the <code>cluster-admin</code> role.</p>\n</li>\n<li>\n<p>You have access to a data science cluster that is configured to run distributed workloads as described in <a href=\"https://opendatahub.io/docs/working-with-distributed-workloads/#configuring-distributed-workloads_distributed-workloads\">Configuring distributed workloads</a>.</p>\n</li>\n<li>\n<p>You have created a data science project.\nFor information about how to create a project, see <a href=\"https://opendatahub.io/docs/working-on-data-science-projects/#creating-a-data-science-project_projects\">Creating a data science project</a>.</p>\n</li>\n<li>\n<p>You have Admin access for the data science project.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>If you created the project, you automatically have Admin access.</p>\n</li>\n<li>\n<p>If you did not create the project, your cluster administrator must give you Admin access.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>You have access to a model.</p>\n</li>\n<li>\n<p>You have access to data that you can use to train the model.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In a terminal window, if you are not already logged in to your OpenShift cluster as a cluster administrator, log in to the OpenShift CLI as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc login <em>&lt;openshift_cluster_url&gt;</em> -u <em>&lt;admin_username&gt;</em> -p <em>&lt;password&gt;</em></code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Create a cluster role with the required access permissions, as follows:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Create a YAML file named <code>pytorchjob_role.yaml</code>.</p>\n</li>\n<li>\n<p>Add the following <code>ClusterRole</code> object definition:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: pytorchjob-role\nrules:\n  - apiGroups:\n      - \"kubeflow.org\"\n    resources:\n      - pytorchjobs\n    verbs:\n      - create\n      - delete\n      - get\n      - list\n      - patch\n      - update\n      - watch\n  - apiGroups:\n      - \"kubeflow.org\"\n    resources:\n      - pytorchjobs/status\n    verbs:\n      - get</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Save your changes in the <code>pytorchjob_role.yaml</code> file.</p>\n</li>\n<li>\n<p>Apply the configuration to create the <code>pytorchjob-role</code> object:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc apply -f pytorchjob_role.yaml</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Create a service account, as follows:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Create a YAML file named <code>pytorchjob_serviceaccount.yaml</code>.</p>\n</li>\n<li>\n<p>Add the following <code>ServiceAccount</code> object definition:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: pytorchjob-sa\n  namespace: kfto</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Replace the example namespace value <code>kfto</code> with the name of your project.</p>\n</li>\n<li>\n<p>Save your changes in the <code>pytorchjob_serviceaccount.yaml</code> file.</p>\n</li>\n<li>\n<p>Apply the configuration to create the <code>pytorchjob-sa</code> service account:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc apply -f pytorchjob_serviceaccount.yaml</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Assign the cluster role to the service account, as follows:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Create a YAML file named <code>pytorchjob_rolebinding.yaml</code>.</p>\n</li>\n<li>\n<p>Add the following <code>ClusterRoleBinding</code> object definition:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: pytorchjob-rolebinding\nsubjects:\n- kind: ServiceAccount\n  name: pytorchjob-sa\n  namespace: kfto\nroleRef:\n  kind: ClusterRole\n  name: pytorchjob-role\n  apiGroup: rbac.authorization.k8s.io</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Replace the example namespace value <code>kfto</code> with the name of your project.</p>\n</li>\n<li>\n<p>Save your changes in the <code>pytorchjob_rolebinding.yaml</code> file.</p>\n</li>\n<li>\n<p>Apply the configuration to assign the <code>pytorchjob-role</code> cluster role to the <code>pytorchjob-sa</code> service account:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc apply -f pytorchjob_rolebinding.yaml</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Verification</div>\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift Container Platform console, select your project from the <strong>Project</strong> list.</p>\n</li>\n<li>\n<p>Click <strong>User Management &#8594; ServiceAccounts</strong> and verify that <code>pytorchjob-sa</code> is listed.</p>\n</li>\n<li>\n<p>Click <strong>User Management &#8594; Roles</strong> and verify that <code>pytorchjob-role</code> is correctly created and assigned.</p>\n</li>\n<li>\n<p>Click <strong>User Management &#8594; RoleBindings</strong> and verify that <code>pytorchjob-rolebinding</code> is correctly created and assigned.</p>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"configuring-the-training-job_distributed-workloads\">Configuring the training job</h3>\n<div class=\"paragraph _abstract\">\n<p>Before you can use a training job to tune a model, you must configure the training job.\nThe example training job in this section is based on the IBM and Hugging Face tuning example provided <a href=\"https://github.com/foundation-model-stack/fms-hf-tuning/tree/main/examples/prompt_tuning_twitter_complaints\">here</a>.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to OpenShift Container Platform.</p>\n</li>\n<li>\n<p>You have access to a data science cluster that is configured to run distributed workloads as described in <a href=\"https://opendatahub.io/docs/working-with-distributed-workloads/#configuring-distributed-workloads_distributed-workloads\">Configuring distributed workloads</a>.</p>\n</li>\n<li>\n<p>You have created a data science project.\nFor information about how to create a project, see <a href=\"https://opendatahub.io/docs/working-on-data-science-projects/#creating-a-data-science-project_projects\">Creating a data science project</a>.</p>\n</li>\n<li>\n<p>You have Admin access for the data science project.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>If you created the project, you automatically have Admin access.</p>\n</li>\n<li>\n<p>If you did not create the project, your cluster administrator must give you Admin access.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>You have access to a model.</p>\n</li>\n<li>\n<p>You have access to data that you can use to train the model.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In a terminal window, if you are not already logged in to your OpenShift cluster, log in to the OpenShift CLI as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc login <em>&lt;openshift_cluster_url&gt;</em> -u <em>&lt;username&gt;</em> -p <em>&lt;password&gt;</em></code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Configure a training job, as follows:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Create a YAML file named <code>config_trainingjob.yaml</code>.</p>\n</li>\n<li>\n<p>Add the following <code>ConfigMap</code> object definition:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>kind: ConfigMap\napiVersion: v1\nmetadata:\n  name: my-config-trainingjob\n  namespace: kfto\ndata:\n  config.json: |\n    {\n      \"model_name_or_path\": \"bigscience/bloom-560m\",\n      \"training_data_path\": \"/data/input/twitter_complaints.json\",\n      \"output_dir\": \"/data/output/tuning/bloom-twitter\",\n      \"num_train_epochs\": 10.0,\n      \"per_device_train_batch_size\": 4,\n      \"gradient_accumulation_steps\": 4,\n      \"learning_rate\": 1e-05,\n      \"response_template\": \"\\n### Label:\",\n      \"dataset_text_field\": \"output\",\n      \"use_flash_attn\": false\n    }</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Replace the example namespace value <code>kfto</code> with the name of your project.</p>\n</li>\n<li>\n<p>Edit the following parameters of the training job:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <code>model_name_or_path</code> field, specify the name of the pre-trained model.</p>\n</li>\n<li>\n<p>In the <code>training_data_path</code> field, specify the path to the training data that you set in the <code>training_data.yaml</code> ConfigMap.</p>\n</li>\n<li>\n<p>In the <code>output_dir</code> field, specify the output directory for the model.</p>\n</li>\n<li>\n<p>In the <code>num_train_epochs</code> field, specify the number of epochs for training.\nIn this example, the training job is set to run 10 times.</p>\n</li>\n<li>\n<p>In the <code>per_device_train_batch_size</code> field, specify the batch size; that is, how many data set examples to process together.\nIn this example, the training job processes 4 examples at a time.</p>\n</li>\n<li>\n<p>In the <code>gradient_accumulation_steps</code> field, specify the number of gradient accumulation steps.</p>\n</li>\n<li>\n<p>In the <code>learning_rate</code> field, specify the learning rate for the training.</p>\n</li>\n<li>\n<p>In the <code>response_template</code> field, specify the template formatting for the response.</p>\n</li>\n<li>\n<p>In the <code>dataset_text_field</code> field, specify the dataset field for training output. This field is set in the <code>training_data.yaml</code> config map.</p>\n</li>\n<li>\n<p>In the <code>use_flash_attn</code> field, specify whether to use flash attention.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Save your changes in the <code>config_trainingjob.yaml</code> file.</p>\n</li>\n<li>\n<p>Apply the configuration to create the <code>my-config-trainingjob</code> object:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc apply -f config_trainingjob.yaml</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Create the training data, as follows:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Create a YAML file named <code>training_data.yaml</code>.</p>\n</li>\n<li>\n<p>Add the following <code>ConfigMap</code> object definition:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>kind: ConfigMap\napiVersion: v1\nmetadata:\n  name: twitter-complaints\n  namespace: kfto\ndata:\n  twitter_complaints.json: |\n    [\n        {\"Tweet text\":\"@HMRCcustomers No this is my first job\",\"ID\":0,\"Label\":2,\"text_label\":\"no complaint\",\"output\":\"### Text: @HMRCcustomers No this is my first job\\n\\n### Label: no complaint\"},\n        {\"Tweet text\":\"@KristaMariePark Thank you for your interest! If you decide to cancel, you can call Customer Care at 1-800-NYTIMES.\",\"ID\":1,\"Label\":2,\"text_label\":\"no complaint\",\"output\":\"### Text: @KristaMariePark Thank you for your interest! If you decide to cancel, you can call Customer Care at 1-800-NYTIMES.\\n\\n### Label: no complaint\"},\n        {\"Tweet text\":\"@EE On Rosneath Arial having good upload and download speeds but terrible latency 200ms. Why is this.\",\"ID\":3,\"Label\":1,\"text_label\":\"complaint\",\"output\":\"### Text: @EE On Rosneath Arial having good upload and download speeds but terrible latency 200ms. Why is this.\\n\\n### Label: complaint\"},\n        {\"Tweet text\":\"Couples wallpaper, so cute. :) #BrothersAtHome\",\"ID\":4,\"Label\":2,\"text_label\":\"no complaint\",\"output\":\"### Text: Couples wallpaper, so cute. :) #BrothersAtHome\\n\\n### Label: no complaint\"},\n        {\"Tweet text\":\"@mckelldogs This might just be me, but-- eyedrops? Artificial tears are so useful when you're sleep-deprived and sp https:\\/\\/t.co\\/WRtNsokblG\",\"ID\":5,\"Label\":2,\"text_label\":\"no complaint\",\"output\":\"### Text: @mckelldogs This might just be me, but-- eyedrops? Artificial tears are so useful when you're sleep-deprived and sp https:\\/\\/t.co\\/WRtNsokblG\\n\\n### Label: no complaint\"},\n        {\"Tweet text\":\"@Yelp can we get the exact calculations for a business rating (for example if its 4 stars but actually 4.2) or do we use a 3rd party site?\",\"ID\":6,\"Label\":2,\"text_label\":\"no complaint\",\"output\":\"### Text: @Yelp can we get the exact calculations for a business rating (for example if its 4 stars but actually 4.2) or do we use a 3rd party site?\\n\\n### Label: no complaint\"},\n        {\"Tweet text\":\"@nationalgridus I have no water and the bill is current and paid. Can you do something about this?\",\"ID\":7,\"Label\":1,\"text_label\":\"complaint\",\"output\":\"### Text: @nationalgridus I have no water and the bill is current and paid. Can you do something about this?\\n\\n### Label: complaint\"},\n        {\"Tweet text\":\"@JenniferTilly Merry Christmas to as well. You get more stunning every year \",\"ID\":9,\"Label\":2,\"text_label\":\"no complaint\",\"output\":\"### Text: @JenniferTilly Merry Christmas to as well. You get more stunning every year \\n\\n### Label: no complaint\"}\n    ]</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Replace the example namespace value <code>kfto</code> with the name of your project.</p>\n</li>\n<li>\n<p>Replace the example training data with your training data.</p>\n</li>\n<li>\n<p>Save your changes in the <code>training_data.yaml</code> file.</p>\n</li>\n<li>\n<p>Apply the configuration to create the training data:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc apply -f training_data.yaml</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Create a persistent volume claim (PVC), as follows:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Create a YAML file named <code>trainedmodelpvc.yaml</code>.</p>\n</li>\n<li>\n<p>Add the following <code>PersistentVolumeClaim</code> object definition:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: trained-model\n  namespace: kfto\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Replace the example namespace value <code>kfto</code> with the name of your project, and update the other parameters to suit your environment.</p>\n</li>\n<li>\n<p>Save your changes in the <code>trainedmodelpvc.yaml</code> file.</p>\n</li>\n<li>\n<p>Apply the configuration to create a Persistent Volume Claim (PVC) for the training job:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc apply -f trainedmodelpvc.yaml</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Verification</div>\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift Container Platform console, select your project from the <strong>Project</strong> list.</p>\n</li>\n<li>\n<p>Click <strong>ConfigMaps</strong> and verify that the <code>my-config-trainingjob</code> and <code>twitter-complaints</code> ConfigMaps are listed.</p>\n</li>\n<li>\n<p>Click <strong>Search</strong>. From the <strong>Resources</strong> list, select <strong>PersistentVolumeClaim</strong> and verify that the <code>trained-model</code> PVC is listed.</p>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"running-the-training-job_distributed-workloads\">Running the training job</h3>\n<div class=\"paragraph _abstract\">\n<p>You can run a training job to tune a model.\nThe example training job in this section is based on the IBM and Hugging Face tuning example provided <a href=\"https://github.com/foundation-model-stack/fms-hf-tuning/tree/main/examples/prompt_tuning_twitter_complaints\">here</a>.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to OpenShift Container Platform with the <code>cluster-admin</code> role.</p>\n</li>\n<li>\n<p>You have access to a data science cluster that is configured to run distributed workloads as described in <a href=\"https://opendatahub.io/docs/working-with-distributed-workloads/#configuring-distributed-workloads_distributed-workloads\">Configuring distributed workloads</a>.</p>\n</li>\n<li>\n<p>You have created a data science project.\nFor information about how to create a project, see <a href=\"https://opendatahub.io/docs/working-on-data-science-projects/#creating-a-data-science-project_projects\">Creating a data science project</a>.</p>\n</li>\n<li>\n<p>You have Admin access for the data science project.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>If you created the project, you automatically have Admin access.</p>\n</li>\n<li>\n<p>If you did not create the project, your cluster administrator must give you Admin access.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>You have access to a model.</p>\n</li>\n<li>\n<p>You have access to data that you can use to train the model.</p>\n</li>\n<li>\n<p>You have configured the training job as described in <a href=\"https://opendatahub.io/docs/working-with-distributed-workloads/#configuring-the-training-job_distributed-workloads\">Configuring the training job</a>.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In a terminal window, if you are not already logged in to your OpenShift cluster as a cluster administrator, log in to the OpenShift CLI as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc login <em>&lt;openshift_cluster_url&gt;</em> -u <em>&lt;admin_username&gt;</em> -p <em>&lt;password&gt;</em></code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Create a PyTorch training job, as follows:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Create a YAML file named <code>pytorchjob.yaml</code>.</p>\n</li>\n<li>\n<p>Add the following <code>PyTorchJob</code> object definition:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: kubeflow.org/v1\nkind: PyTorchJob\nmetadata:\n  name: kfto-demo\n  namespace: kfto\nspec:\n  pytorchReplicaSpecs:\n    Master:\n      replicas: 1\n      restartPolicy: Never\n      template:\n        spec:\n          containers:\n            - env:\n                - name: SFT_TRAINER_CONFIG_JSON_PATH\n                  value: /etc/config/config.json\n              image: 'quay.io/modh/fms-hf-tuning:release'\n              imagePullPolicy: IfNotPresent\n              name: pytorch\n              volumeMounts:\n                - mountPath: /etc/config\n                  name: config-volume\n                - mountPath: /data/input\n                  name: dataset-volume\n                - mountPath: /data/output\n                  name: model-volume\n          volumes:\n            - configMap:\n                items:\n                  - key: config.json\n                    path: config.json\n                name: my-config-trainingjob\n              name: config-volume\n            - configMap:\n                name: twitter-complaints\n              name: dataset-volume\n            - name: model-volume\n              persistentVolumeClaim:\n                claimName: trained-model\n  runPolicy:\n    suspend: false</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Replace the example namespace value <code>kfto</code> with the name of your project, and update the other parameters to suit your environment.</p>\n</li>\n<li>\n<p>Edit the parameters of the PyTorch training job, to provide the details for your training job and environment.</p>\n</li>\n<li>\n<p>Save your changes in the <code>pytorchjob.yaml</code> file.</p>\n</li>\n<li>\n<p>Apply the configuration to run the PyTorch training job:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc apply -f pytorchjob.yaml</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Verification</div>\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift Container Platform console, select your project from the <strong>Project</strong> list.</p>\n</li>\n<li>\n<p>Click <strong>Workloads</strong> &#8594; <strong>Pods</strong> and verify that the <strong><em>&lt;project-name&gt;</em>-demo-master-0</strong> pod is listed.</p>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"monitoring-the-training-job_distributed-workloads\">Monitoring the training job</h3>\n<div class=\"paragraph _abstract\">\n<p>When you run a training job to tune a model, you can monitor the progress of the job.\nThe example training job in this section is based on the IBM and Hugging Face tuning example provided <a href=\"https://github.com/foundation-model-stack/fms-hf-tuning/tree/main/examples/prompt_tuning_twitter_complaints\">here</a>.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to OpenShift Container Platform with the <code>cluster-admin</code> role.</p>\n</li>\n<li>\n<p>You have access to a data science cluster that is configured to run distributed workloads as described in <a href=\"https://opendatahub.io/docs/working-with-distributed-workloads/#configuring-distributed-workloads_distributed-workloads\">Configuring distributed workloads</a>.</p>\n</li>\n<li>\n<p>You have created a data science project.\nFor information about how to create a project, see <a href=\"https://opendatahub.io/docs/working-on-data-science-projects/#creating-a-data-science-project_projects\">Creating a data science project</a>.</p>\n</li>\n<li>\n<p>You have Admin access for the data science project.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>If you created the project, you automatically have Admin access.</p>\n</li>\n<li>\n<p>If you did not create the project, your cluster administrator must give you Admin access.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>You have access to a model.</p>\n</li>\n<li>\n<p>You have access to data that you can use to train the model.</p>\n</li>\n<li>\n<p>You are running the training job as described in <a href=\"https://opendatahub.io/docs/working-with-distributed-workloads/#running-the-training-job_distributed-workloads\">Running the training job</a>.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift Container Platform console, select your project from the <strong>Project</strong> list.</p>\n</li>\n<li>\n<p>Click <strong>Workloads</strong> &#8594; <strong>Pods</strong>.</p>\n</li>\n<li>\n<p>Search for the pod that corresponds to the PyTorch job, that is, <strong><em>&lt;project-name&gt;</em>-demo-master-0</strong>.</p>\n<div class=\"paragraph\">\n<p>For example, if the project name is <code>kfto</code>, the pod name is <strong>kfto-demo-master-0</strong>.</p>\n</div>\n</li>\n<li>\n<p>Click the pod name to open the pod details page.</p>\n</li>\n<li>\n<p>Click the <strong>Logs</strong> tab to monitor the progress of the job and view status updates, as shown in the following example output:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>0%| | 0/10 [00:00&lt;?, ?it/s] 10%| | 1/10 [01:10&lt;10:32, 70.32s/it] {'loss': 6.9531, 'grad_norm': 1104.0, 'learning_rate': 9e-06, 'epoch': 1.0}\n10%| | 1/10 [01:10&lt;10:32, 70.32s/it] 20%| | 2/10 [01:40&lt;06:13, 46.71s/it] 30%| | 3/10 [02:26&lt;05:25, 46.55s/it] {'loss': 2.4609, 'grad_norm': 736.0, 'learning_rate': 7e-06, 'epoch': 2.0}\n30%| | 3/10 [02:26&lt;05:25, 46.55s/it] 40%| | 4/10 [03:23&lt;05:02, 50.48s/it] 50%| | 5/10 [03:41&lt;03:13, 38.66s/it] {'loss': 1.7617, 'grad_norm': 328.0, 'learning_rate': 5e-06, 'epoch': 3.0}\n50%| | 5/10 [03:41&lt;03:13, 38.66s/it] 60%| | 6/10 [04:54&lt;03:22, 50.58s/it] {'loss': 3.1797, 'grad_norm': 1016.0, 'learning_rate': 4.000000000000001e-06, 'epoch': 4.0}\n60%| | 6/10 [04:54&lt;03:22, 50.58s/it] 70%| | 7/10 [06:03&lt;02:49, 56.59s/it] {'loss': 2.9297, 'grad_norm': 984.0, 'learning_rate': 3e-06, 'epoch': 5.0}\n70%| | 7/10 [06:03&lt;02:49, 56.59s/it] 80%| | 8/10 [06:38&lt;01:39, 49.57s/it] 90%| | 9/10 [07:22&lt;00:48, 48.03s/it] {'loss': 1.4219, 'grad_norm': 684.0, 'learning_rate': 1.0000000000000002e-06, 'epoch': 6.0}\n90%| | 9/10 [07:22&lt;00:48, 48.03s/it]100%|| 10/10 [08:25&lt;00:00, 52.53s/it] {'loss': 1.9609, 'grad_norm': 648.0, 'learning_rate': 0.0, 'epoch': 6.67}\n100%|| 10/10 [08:25&lt;00:00, 52.53s/it] {'train_runtime': 508.0444, 'train_samples_per_second': 0.197, 'train_steps_per_second': 0.02, 'train_loss': 2.63125, 'epoch': 6.67}\n100%|| 10/10 [08:28&lt;00:00, 52.53s/it]100%|| 10/10 [08:28&lt;00:00, 50.80s/it]</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>In the example output, the solid blocks indicate progress bars.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Verification</div>\n<ol class=\"arabic\">\n<li>\n<p>The <strong><em>&lt;project-name&gt;</em>-demo-master-0</strong> pod is running.</p>\n</li>\n<li>\n<p>The <strong>Logs</strong> tab provides information about the job progress and job status.</p>\n</li>\n</ol>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads\">Troubleshooting common problems with distributed workloads for users</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>If you are experiencing errors in Open Data Hub relating to distributed workloads, read this section to understand what could be causing the problem, and how to resolve the problem.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_my_ray_cluster_is_in_a_suspended_state\">My Ray cluster is in a suspended state</h3>\n<div class=\"paragraph\">\n<div class=\"title\">Problem</div>\n<p>The resource quota specified in the cluster queue configuration might be insufficient, or the resource flavor might not yet be created.</p>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Diagnosis</div>\n<p>The Ray cluster head pod or worker pods remain in a suspended state.</p>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Resolution</div>\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift Container Platform console, select your project from the <strong>Project</strong> list.</p>\n</li>\n<li>\n<p>Check the workload resource:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click <strong>Search</strong>, and from the <strong>Resources</strong> list, select <strong>Workload</strong>.</p>\n</li>\n<li>\n<p>Select the workload resource that is created with the Ray cluster resource, and click the <strong>YAML</strong> tab.</p>\n</li>\n<li>\n<p>Check the text in the <code>status.conditions.message</code> field, which provides the reason for the suspended state, as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">status:\n conditions:\n   - lastTransitionTime: '2024-05-29T13:05:09Z'\n     message: 'couldn''t assign flavors to pod set small-group-jobtest12: insufficient quota for nvidia.com/gpu in flavor default-flavor in ClusterQueue'</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Check the Ray cluster resource:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click <strong>Search</strong>, and from the <strong>Resources</strong> list, select <strong>RayCluster</strong>.</p>\n</li>\n<li>\n<p>Select the Ray cluster resource, and click the <strong>YAML</strong> tab.</p>\n</li>\n<li>\n<p>Check the text in the <code>status.conditions.message</code> field.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Check the cluster queue resource:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click <strong>Search</strong>, and from the <strong>Resources</strong> list, select <strong>ClusterQueue</strong>.</p>\n</li>\n<li>\n<p>Check your cluster queue configuration to ensure that the resources that you requested are within the limits defined for the project.</p>\n</li>\n<li>\n<p>Either reduce your requested resources, or contact your administrator to request more resources.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_my_ray_cluster_is_in_a_failed_state\">My Ray cluster is in a failed state</h3>\n<div class=\"paragraph\">\n<div class=\"title\">Problem</div>\n<p>You might have insufficient resources.</p>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Diagnosis</div>\n<p>The Ray cluster head pod or worker pods are not running.\nWhen a Ray cluster is created, it initially enters a <code>failed</code> state.\nThis failed state usually resolves after the reconciliation process completes and the Ray cluster pods are running.</p>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Resolution</div>\n<p>If the failed state persists, complete the following steps:</p>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift Container Platform console, select your project from the <strong>Project</strong> list.</p>\n</li>\n<li>\n<p>Click <strong>Search</strong>, and from the <strong>Resources</strong> list, select <strong>Pod</strong>.</p>\n</li>\n<li>\n<p>Click your pod name to open the pod details page.</p>\n</li>\n<li>\n<p>Click the <strong>Events</strong> tab, and review the pod events to identify the cause of the problem.</p>\n</li>\n<li>\n<p>If you cannot resolve the problem, contact your administrator to request assistance.</p>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_i_see_a_failed_to_call_webhook_error_message_for_the_codeflare_operator\">I see a <strong>failed to call webhook</strong> error message for the CodeFlare Operator</h3>\n<div class=\"paragraph\">\n<div class=\"title\">Problem</div>\n<p>After you run the <code>cluster.up()</code> command, the following error is shown:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">ApiException: (500)\nReason: Internal Server Error\nHTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"Internal error occurred: failed calling webhook \\\"mraycluster.ray.openshift.ai\\\": failed to call webhook: Post \\\"https://codeflare-operator-webhook-service.redhat-ods-applications.svc:443/mutate-ray-io-v1-raycluster?timeout=10s\\\": no endpoints available for service \\\"codeflare-operator-webhook-service\\\"\",\"reason\":\"InternalError\",\"details\":{\"causes\":[{\"message\":\"failed calling webhook \\\"mraycluster.ray.openshift.ai\\\": failed to call webhook: Post \\\"https://codeflare-operator-webhook-service.redhat-ods-applications.svc:443/mutate-ray-io-v1-raycluster?timeout=10s\\\": no endpoints available for service \\\"codeflare-operator-webhook-service\\\"\"}]},\"code\":500}</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Diagnosis</div>\n<p>The CodeFlare Operator pod might not be running.</p>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Resolution</div>\n<p>Contact your administrator to request assistance.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_i_see_a_failed_to_call_webhook_error_message_for_kueue\">I see a <strong>failed to call webhook</strong> error message for Kueue</h3>\n<div class=\"paragraph\">\n<div class=\"title\">Problem</div>\n<p>After you run the <code>cluster.up()</code> command, the following error is shown:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">ApiException: (500)\nReason: Internal Server Error\nHTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"Internal error occurred: failed calling webhook \\\"mraycluster.kb.io\\\": failed to call webhook: Post \\\"https://kueue-webhook-service.redhat-ods-applications.svc:443/mutate-ray-io-v1-raycluster?timeout=10s\\\": no endpoints available for service \\\"kueue-webhook-service\\\"\",\"reason\":\"InternalError\",\"details\":{\"causes\":[{\"message\":\"failed calling webhook \\\"mraycluster.kb.io\\\": failed to call webhook: Post \\\"https://kueue-webhook-service.redhat-ods-applications.svc:443/mutate-ray-io-v1-raycluster?timeout=10s\\\": no endpoints available for service \\\"kueue-webhook-service\\\"\"}]},\"code\":500}</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Diagnosis</div>\n<p>The Kueue pod might not be running.</p>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Resolution</div>\n<p>Contact your administrator to request assistance.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_my_ray_cluster_doesnt_start\">My Ray cluster doesn&#8217;t start</h3>\n<div class=\"paragraph\">\n<div class=\"title\">Problem</div>\n<p>After you run the <code>cluster.up()</code> command, when you run either the <code>cluster.details()</code> command or the <code>cluster.status()</code> command, the Ray Cluster remains in the <code>Starting</code> status instead of changing to the <code>Ready</code> status.\nNo pods are created.</p>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Diagnosis</div>\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift Container Platform console, select your project from the <strong>Project</strong> list.</p>\n</li>\n<li>\n<p>Check the workload resource:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click <strong>Search</strong>, and from the <strong>Resources</strong> list, select <strong>Workload</strong>.</p>\n</li>\n<li>\n<p>Select the workload resource that is created with the Ray cluster resource, and click the <strong>YAML</strong> tab.</p>\n</li>\n<li>\n<p>Check the text in the <code>status.conditions.message</code> field, which provides the reason for remaining in the <code>Starting</code> state.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Check the Ray cluster resource:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click <strong>Search</strong>, and from the <strong>Resources</strong> list, select <strong>RayCluster</strong>.</p>\n</li>\n<li>\n<p>Select the Ray cluster resource, and click the <strong>YAML</strong> tab.</p>\n</li>\n<li>\n<p>Check the text in the <code>status.conditions.message</code> field.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Resolution</div>\n<p>If you cannot resolve the problem, contact your administrator to request assistance.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_i_see_a_default_local_queue_not_found_error_message\">I see a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message</h3>\n<div class=\"paragraph\">\n<div class=\"title\">Problem</div>\n<p>After you run the <code>cluster.up()</code> command, the following error is shown:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">Default Local Queue with kueue.x-k8s.io/default-queue: true annotation not found please create a default Local Queue or provide the local_queue name in Cluster Configuration.</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Diagnosis</div>\n<p>No default local queue is defined, and a local queue is not specified in the cluster configuration.</p>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Resolution</div>\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift Container Platform console, select your project from the <strong>Project</strong> list.</p>\n</li>\n<li>\n<p>Click <strong>Search</strong>, and from the <strong>Resources</strong> list, select <strong>LocalQueue</strong>.</p>\n</li>\n<li>\n<p>Resolve the problem in one of the following ways:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>If a local queue exists, add it to your cluster configuration as follows:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">local_queue=\"<em>&lt;local_queue_name&gt;</em>\"</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>If no local queue exists, contact your administrator to request assistance.</p>\n</li>\n</ul>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_i_see_a_local_queue_provided_does_not_exist_error_message\">I see a <strong>local_queue provided does not exist</strong> error message</h3>\n<div class=\"paragraph\">\n<div class=\"title\">Problem</div>\n<p>After you run the <code>cluster.up()</code> command, the following error is shown:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">local_queue provided does not exist or is not in this namespace. Please provide the correct local_queue name in Cluster Configuration.</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Diagnosis</div>\n<p>An incorrect value is specified for the local queue in the cluster configuration, or an incorrect default local queue is defined.\nThe specified local queue either does not exist, or exists in a different namespace.</p>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Resolution</div>\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift Container Platform console, select your project from the <strong>Project</strong> list.</p>\n</li>\n<li>\n<p>Click <strong>Search</strong>, and from the <strong>Resources</strong> list, select <strong>LocalQueue</strong>.</p>\n</li>\n<li>\n<p>Resolve the problem in one of the following ways:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>If a local queue exists, ensure that you spelled the local queue name correctly in your cluster configuration, and that the <code>namespace</code> value in the cluster configuration matches your project name.\nIf you do not specify a <code>namespace</code> value in the cluster configuration, the Ray cluster is created in the current project.</p>\n</li>\n<li>\n<p>If no local queue exists, contact your administrator to request assistance.</p>\n</li>\n</ul>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_i_cannot_create_a_ray_cluster_or_submit_jobs\">I cannot create a Ray cluster or submit jobs</h3>\n<div class=\"paragraph\">\n<div class=\"title\">Problem</div>\n<p>After you run the <code>cluster.up()</code> command, an error similar to the following error is shown:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">RuntimeError: Failed to get RayCluster CustomResourceDefinition: (403)\nReason: Forbidden\nHTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"rayclusters.ray.io is forbidden: User \\\"system:serviceaccount:regularuser-project:regularuser-workbench\\\" cannot list resource \\\"rayclusters\\\" in API group \\\"ray.io\\\" in the namespace \\\"regularuser-project\\\"\",\"reason\":\"Forbidden\",\"details\":{\"group\":\"ray.io\",\"kind\":\"rayclusters\"},\"code\":403}</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Diagnosis</div>\n<p>The correct OpenShift login credentials are not specified in the <code>TokenAuthentication</code> section of your notebook code.</p>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Resolution</div>\n<ol class=\"arabic\">\n<li>\n<p>Identify the correct OpenShift login credentials as follows:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the OpenShift Container Platform console header, click your username and click <strong>Copy login command</strong>.</p>\n</li>\n<li>\n<p>In the new tab that opens, log in as the user whose credentials you want to use.</p>\n</li>\n<li>\n<p>Click <strong>Display Token</strong>.</p>\n</li>\n<li>\n<p>From the <strong>Log in with this token</strong> section, copy the <code>token</code> and <code>server</code> values.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>In your notebook code, specify the copied <code>token</code> and <code>server</code> values as follows:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">auth = TokenAuthentication(\n    token = \"<em>&lt;token&gt;</em>\",\n    server = \"<em>&lt;server&gt;</em>\",\n    skip_tls=False\n)\nauth.login()</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_my_pod_provisioned_by_kueue_is_terminated_before_my_image_is_pulled\">My pod provisioned by Kueue is terminated before my image is pulled</h3>\n<div class=\"paragraph\">\n<div class=\"title\">Problem</div>\n<p>Kueue waits for a period of time before marking a workload as ready, to enable all of the workload pods to become provisioned and running.\nBy default, Kueue waits for 5 minutes.\nIf the pod image is very large and is still being pulled after the 5-minute waiting period elapses, Kueue fails the workload and terminates the related pods.</p>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Diagnosis</div>\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift Container Platform console, select your project from the <strong>Project</strong> list.</p>\n</li>\n<li>\n<p>Click <strong>Search</strong>, and from the <strong>Resources</strong> list, select <strong>Pod</strong>.</p>\n</li>\n<li>\n<p>Click the Ray head pod name to open the pod details page.</p>\n</li>\n<li>\n<p>Click the <strong>Events</strong> tab, and review the pod events to check whether the image pull completed successfully.</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Resolution</div>\n<p>If the pod takes more than 5 minutes to pull the image, contact your administrator to request assistance.</p>\n</div>\n</div>\n</div>\n</div>","id":"3d003914-8f46-57be-8b41-2c52922ec83a","document":{"title":"Working with distributed workloads"}},"markdownRemark":null},"pageContext":{"id":"3d003914-8f46-57be-8b41-2c52922ec83a"}},"staticQueryHashes":["2604506565"],"slicesMap":{}}