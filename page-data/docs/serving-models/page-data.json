{"componentChunkName":"component---src-templates-docs-page-tsx","path":"/docs/serving-models/","result":{"data":{"allFile":{"edges":[{"node":{"childAsciidoc":{"fields":{"slug":"/docs/README/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/getting-started-with-open-data-hub/"},"sections":[{"parentId":null,"name":"Overview","level":1,"index":0,"id":"overview-for-getting-started_get-started"},{"parentId":"overview-for-getting-started_get-started","name":"Data science workflow","level":2,"index":0,"id":"_data_science_workflow"},{"parentId":"overview-for-getting-started_get-started","name":"About this guide","level":2,"index":1,"id":"_about_this_guide"},{"parentId":null,"name":"Logging in to Open Data Hub","level":1,"index":1,"id":"logging-in_get-started"},{"parentId":null,"name":"Creating a data science project","level":1,"index":2,"id":"creating-a-data-science-project_get-started"},{"parentId":null,"name":"Creating a workbench and selecting an IDE","level":1,"index":3,"id":"creating-a-workbench-select-ide_get-started"},{"parentId":"creating-a-workbench-select-ide_get-started","name":"About workbench images","level":2,"index":0,"id":"about-workbench-images_get-started"},{"parentId":"creating-a-workbench-select-ide_get-started","name":"Creating a workbench","level":2,"index":1,"id":"creating-a-project-workbench_get-started"},{"parentId":null,"name":"Next steps","level":1,"index":4,"id":"next-steps_get-started"},{"parentId":"next-steps_get-started","name":"Additional resources","level":2,"index":0,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/installing-open-data-hub/"},"sections":[{"parentId":null,"name":"Installing Open Data Hub version 2","level":1,"index":0,"id":"installing-odh-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing the Open Data Hub Operator version 2","level":2,"index":0,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing Open Data Hub components","level":2,"index":1,"id":"installing-odh-components_installv2"},{"parentId":null,"name":"Installing Open Data Hub version 1","level":1,"index":1,"id":"installing-odh-v1_installv1"},{"parentId":"installing-odh-v1_installv1","name":"Installing the Open Data Hub Operator version 1","level":2,"index":0,"id":"installing-the-odh-operator-v1_installv1"},{"parentId":"installing-odh-v1_installv1","name":"Creating a new project for your Open Data Hub instance","level":2,"index":1,"id":"creating-a-new-project-for-your-odh-instance_installv1"},{"parentId":"installing-odh-v1_installv1","name":"Adding an Open Data Hub instance","level":2,"index":2,"id":"adding-an-odh-instance_installv1"},{"parentId":"installing-odh-v1_installv1","name":"Accessing the Open Data Hub dashboard","level":2,"index":3,"id":"accessing-the-odh-dashboard_installv1"},{"parentId":null,"name":"Installing the distributed workloads components","level":1,"index":2,"id":"installing-the-distributed-workloads-components_install"},{"parentId":null,"name":"Accessing the Open Data Hub dashboard","level":1,"index":3,"id":"accessing-the-odh-dashboard_install"},{"parentId":null,"name":"Working with certificates","level":1,"index":4,"id":"working-with-certificates_certs"},{"parentId":"working-with-certificates_certs","name":"Understanding certificates in Open Data Hub","level":2,"index":0,"id":"understanding-certificates_certs"},{"parentId":"understanding-certificates_certs","name":"How CA bundles are injected","level":3,"index":0,"id":"_how_ca_bundles_are_injected"},{"parentId":"understanding-certificates_certs","name":"How the ConfigMap is managed","level":3,"index":1,"id":"_how_the_configmap_is_managed"},{"parentId":"working-with-certificates_certs","name":"Adding a CA bundle","level":2,"index":1,"id":"adding-a-ca-bundle_certs"},{"parentId":"working-with-certificates_certs","name":"Removing a CA bundle","level":2,"index":2,"id":"removing-a-ca-bundle_certs"},{"parentId":"working-with-certificates_certs","name":"Removing a CA bundle from a namespace","level":2,"index":3,"id":"removing-a-ca-bundle-from-a-namespace_certs"},{"parentId":"working-with-certificates_certs","name":"Managing certificates","level":2,"index":4,"id":"managing-certificates_certs"},{"parentId":"working-with-certificates_certs","name":"Using self-signed certificates with Open Data Hub components","level":2,"index":5,"id":"_using_self_signed_certificates_with_open_data_hub_components"},{"parentId":"_using_self_signed_certificates_with_open_data_hub_components","name":"Using certificates with data science pipelines","level":3,"index":0,"id":"using-certificates-with-data-science-pipelines_certs"},{"parentId":"using-certificates-with-data-science-pipelines_certs","name":"Providing a CA bundle only for data science pipelines","level":4,"index":0,"id":"_providing_a_ca_bundle_only_for_data_science_pipelines"},{"parentId":"_using_self_signed_certificates_with_open_data_hub_components","name":"Using certificates with workbenches","level":3,"index":1,"id":"using-certificates-with-workbenches_certs"},{"parentId":"using-certificates-with-workbenches_certs","name":"Creating data science pipelines with Elyra and self-signed certificates","level":4,"index":0,"id":"_creating_data_science_pipelines_with_elyra_and_self_signed_certificates"},{"parentId":null,"name":"Configuring the Open Data Hub Operator logger","level":0,"index":5,"id":"configuring-the-odh-operator-logger_operator-log"},{"parentId":"configuring-the-odh-operator-logger_operator-log","name":"Configuring the Open Data Hub Operator logger","level":1,"index":0,"id":"configuring-the-operator-logger_operator-log"},{"parentId":"configuring-the-operator-logger_operator-log","name":"Viewing the Open Data Hub Operator log","level":2,"index":0,"id":"_viewing_the_open_data_hub_operator_log"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/lmeval-evaluation-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/lmeval-scenarios/"},"sections":[{"parentId":null,"name":"Configuring the LM-eval environment","level":1,"index":0,"id":"_configuring_the_lm_eval_environment"},{"parentId":null,"name":"Custom Unitxt Card","level":1,"index":1,"id":"_custom_unitxt_card"},{"parentId":null,"name":"Using PVCs as storage","level":1,"index":2,"id":"_using_pvcs_as_storage"},{"parentId":"_using_pvcs_as_storage","name":"Managed PVCs","level":2,"index":0,"id":"_managed_pvcs"},{"parentId":"_using_pvcs_as_storage","name":"Existing PVCs","level":2,"index":1,"id":"_existing_pvcs"},{"parentId":null,"name":"Using an InferenceService","level":1,"index":3,"id":"_using_an_inferenceservice"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/managing-odh/"},"sections":[{"parentId":null,"name":"Managing users and groups","level":1,"index":0,"id":"managing-users-and-groups"},{"parentId":"managing-users-and-groups","name":"Overview of user types and permissions","level":2,"index":0,"id":"overview-of-user-types-and-permissions_managing-odh"},{"parentId":"managing-users-and-groups","name":"Viewing Open Data Hub users","level":2,"index":1,"id":"viewing-data-science-users_managing-odh"},{"parentId":"managing-users-and-groups","name":"Adding users to Open Data Hub user groups","level":2,"index":2,"id":"adding-users-to-user-groups_managing-odh"},{"parentId":"managing-users-and-groups","name":"Selecting Open Data Hub administrator and user groups","level":2,"index":3,"id":"selecting-admin-and-user-groups_managing-odh"},{"parentId":"managing-users-and-groups","name":"Deleting users","level":2,"index":4,"id":"_deleting_users"},{"parentId":"_deleting_users","name":"About deleting users and their resources","level":3,"index":0,"id":"about-deleting-users-and-resources_managing-odh"},{"parentId":"_deleting_users","name":"Stopping notebook servers owned by other users","level":3,"index":1,"id":"stopping-notebook-servers-owned-by-other-users_managing-odh"},{"parentId":"_deleting_users","name":"Revoking user access to Jupyter","level":3,"index":2,"id":"revoking-user-access-to-jupyter_managing-odh"},{"parentId":"_deleting_users","name":"Backing up storage data","level":3,"index":3,"id":"backing-up-storage-data_managing-odh"},{"parentId":"_deleting_users","name":"Cleaning up after deleting users","level":3,"index":4,"id":"cleaning-up-after-deleting-users_managing-odh"},{"parentId":null,"name":"Creating custom workbench images","level":1,"index":1,"id":"creating-custom-workbench-images"},{"parentId":"creating-custom-workbench-images","name":"Creating a custom image from a default Open Data Hub image","level":2,"index":0,"id":"creating-a-custom-image-from-default-image_custom-images"},{"parentId":"creating-custom-workbench-images","name":"Creating a custom image from your own image","level":2,"index":1,"id":"creating-a-custom-image-from-your-own-image_custom-images"},{"parentId":"creating-a-custom-image-from-your-own-image_custom-images","name":"Basic guidelines for creating your own workbench image","level":3,"index":0,"id":"_basic_guidelines_for_creating_your_own_workbench_image"},{"parentId":"creating-a-custom-image-from-your-own-image_custom-images","name":"Advanced guidelines for creating your own workbench image","level":3,"index":1,"id":"_advanced_guidelines_for_creating_your_own_workbench_image"},{"parentId":"creating-custom-workbench-images","name":"Enabling custom images in Open Data Hub","level":2,"index":2,"id":"enabling-custom-images_custom-images"},{"parentId":"creating-custom-workbench-images","name":"Importing a custom workbench image","level":2,"index":3,"id":"importing-a-custom-workbench-image_custom-images"},{"parentId":null,"name":"Customizing the dashboard","level":1,"index":2,"id":"customizing-the-dashboard"},{"parentId":"customizing-the-dashboard","name":"Editing the dashboard configuration file","level":2,"index":0,"id":"editing-the-dashboard-configuration-file_dashboard"},{"parentId":"customizing-the-dashboard","name":"Dashboard configuration options","level":2,"index":1,"id":"ref-dashboard-configuration-options_dashboard"},{"parentId":null,"name":"Managing applications that show in the dashboard","level":1,"index":3,"id":"managing-applications-that-show-in-the-dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Adding an application to the dashboard","level":2,"index":0,"id":"adding-an-application-to-the-dashboard_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Preventing users from adding applications to the dashboard","level":2,"index":1,"id":"preventing-users-from-adding-applications-to-the-dashboard_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Disabling applications connected to Open Data Hub","level":2,"index":2,"id":"disabling-applications-connected_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Showing or hiding information about enabled applications","level":2,"index":3,"id":"showing-hiding-information-about-enabled-applications_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Hiding the default Jupyter application","level":2,"index":4,"id":"hiding-the-default-jupyter-application_dashboard"},{"parentId":null,"name":"Allocating additional resources to Open Data Hub users","level":1,"index":4,"id":"allocating-additional-resources-to-data-science-users_managing-odh"},{"parentId":null,"name":"Customizing component deployment resources","level":1,"index":5,"id":"customizing-component-deployment-resources_managing-resources"},{"parentId":"customizing-component-deployment-resources_managing-resources","name":"Overview of component resource customization","level":2,"index":0,"id":"overview-of-component-resource-customization_managing-resources"},{"parentId":"customizing-component-deployment-resources_managing-resources","name":"Customizing component resources","level":2,"index":1,"id":"customizing-component-resources_managing-resources"},{"parentId":"customizing-component-deployment-resources_managing-resources","name":"Disabling component resource customization","level":2,"index":2,"id":"disabling-component-resource-customization_managing-resources"},{"parentId":"customizing-component-deployment-resources_managing-resources","name":"Re-enabling component resource customization","level":2,"index":3,"id":"reenabling-component-resource-customization_managing-resources"},{"parentId":null,"name":"Enabling accelerators","level":1,"index":6,"id":"_enabling_accelerators"},{"parentId":"_enabling_accelerators","name":"Enabling NVIDIA GPUs","level":2,"index":0,"id":"enabling-nvidia-gpus_managing-odh"},{"parentId":"_enabling_accelerators","name":"Intel Gaudi AI Accelerator integration","level":2,"index":1,"id":"intel-gaudi-ai-accelerator-integration_managing-odh"},{"parentId":null,"name":"Managing distributed workloads","level":1,"index":7,"id":"_managing_distributed_workloads"},{"parentId":"_managing_distributed_workloads","name":"Overview of Kueue resources","level":2,"index":0,"id":"overview-of-kueue-resources_managing-odh"},{"parentId":"overview-of-kueue-resources_managing-odh","name":"Resource flavor","level":3,"index":0,"id":"_resource_flavor"},{"parentId":"overview-of-kueue-resources_managing-odh","name":"Cluster queue","level":3,"index":1,"id":"_cluster_queue"},{"parentId":"overview-of-kueue-resources_managing-odh","name":"Local queue","level":3,"index":2,"id":"_local_queue"},{"parentId":"_managing_distributed_workloads","name":"Example Kueue resource configurations","level":2,"index":1,"id":"example-kueue-resource-configurations_managing-odh"},{"parentId":"example-kueue-resource-configurations_managing-odh","name":"NVIDIA GPUs","level":3,"index":0,"id":"_nvidia_gpus"},{"parentId":"_nvidia_gpus","name":"NVIDIA RTX A400 GPU resource flavor","level":4,"index":0,"id":"_nvidia_rtx_a400_gpu_resource_flavor"},{"parentId":"_nvidia_gpus","name":"NVIDIA RTX A1000 GPU resource flavor","level":4,"index":1,"id":"_nvidia_rtx_a1000_gpu_resource_flavor"},{"parentId":"_nvidia_gpus","name":"NVIDIA RTX A400 GPU cluster queue","level":4,"index":2,"id":"_nvidia_rtx_a400_gpu_cluster_queue"},{"parentId":"_nvidia_gpus","name":"NVIDIA RTX A1000 GPU cluster queue","level":4,"index":3,"id":"_nvidia_rtx_a1000_gpu_cluster_queue"},{"parentId":"example-kueue-resource-configurations_managing-odh","name":"Additional resources","level":3,"index":1,"id":"_additional_resources"},{"parentId":"_managing_distributed_workloads","name":"Configuring quota management for distributed workloads","level":2,"index":2,"id":"configuring-quota-management-for-distributed-workloads_managing-odh"},{"parentId":"_managing_distributed_workloads","name":"Configuring the CodeFlare Operator","level":2,"index":3,"id":"configuring-the-codeflare-operator_managing-odh"},{"parentId":"_managing_distributed_workloads","name":"Troubleshooting common problems with distributed workloads for administrators","level":2,"index":4,"id":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user&#8217;s Ray cluster is in a suspended state","level":3,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user&#8217;s Ray cluster is in a failed state","level":3,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user receives a <strong>failed to call webhook</strong> error message for the CodeFlare Operator","level":3,"index":2,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user receives a <strong>failed to call webhook</strong> error message for Kueue","level":3,"index":3,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user&#8217;s Ray cluster does not start","level":3,"index":4,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user receives a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":3,"index":5,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user receives a <strong>local_queue provided does not exist</strong> error message","level":3,"index":6,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user cannot create a Ray cluster or submit jobs","level":3,"index":7,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"The user&#8217;s pod provisioned by Kueue is terminated before the user&#8217;s image is pulled","level":3,"index":8,"id":"_the_users_pod_provisioned_by_kueue_is_terminated_before_the_users_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/managing-resources/"},"sections":[{"parentId":null,"name":"Selecting Open Data Hub administrator and user groups","level":1,"index":0,"id":"selecting-admin-and-user-groups_managing-resources"},{"parentId":null,"name":"Importing a custom workbench image","level":1,"index":1,"id":"importing-a-custom-workbench-image_managing-resources"},{"parentId":null,"name":"Managing cluster PVC size","level":1,"index":2,"id":"managing-cluster-pvc-size"},{"parentId":"managing-cluster-pvc-size","name":"Configuring the default PVC size for your cluster","level":2,"index":0,"id":"configuring-the-default-pvc-size-for-your-cluster_managing-resources"},{"parentId":"managing-cluster-pvc-size","name":"Restoring the default PVC size for your cluster","level":2,"index":1,"id":"restoring-the-default-pvc-size-for-your-cluster_managing-resources"},{"parentId":null,"name":"Managing storage classes","level":1,"index":3,"id":"managing-storage-classes"},{"parentId":"managing-storage-classes","name":"Configuring storage class settings","level":2,"index":0,"id":"configuring-storage-class-settings_managing-resources"},{"parentId":"managing-storage-classes","name":"Configuring the default storage class for your cluster","level":2,"index":1,"id":"configuring-the-default-storage-class-for-your-cluster_managing-resources"},{"parentId":"managing-storage-classes","name":"Overview of object storage endpoints","level":2,"index":2,"id":"overview-of-object-storage-endpoints_managing-resources"},{"parentId":"overview-of-object-storage-endpoints_managing-resources","name":"MinIO (On-Cluster)","level":3,"index":0,"id":"_minio_on_cluster"},{"parentId":"overview-of-object-storage-endpoints_managing-resources","name":"Amazon S3","level":3,"index":1,"id":"_amazon_s3"},{"parentId":"overview-of-object-storage-endpoints_managing-resources","name":"Other S3-Compatible Object Stores","level":3,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":"overview-of-object-storage-endpoints_managing-resources","name":"Verification and Troubleshooting","level":3,"index":3,"id":"_verification_and_troubleshooting"},{"parentId":null,"name":"Managing Jupyter notebook servers","level":1,"index":4,"id":"managing-notebook-servers"},{"parentId":"managing-notebook-servers","name":"Accessing the Jupyter administration interface","level":2,"index":0,"id":"accessing-the-jupyter-administration-interface_managing-resources"},{"parentId":"managing-notebook-servers","name":"Starting notebook servers owned by other users","level":2,"index":1,"id":"starting-notebook-servers-owned-by-other-users_managing-resources"},{"parentId":"managing-notebook-servers","name":"Accessing notebook servers owned by other users","level":2,"index":2,"id":"accessing-notebook-servers-owned-by-other-users_managing-resources"},{"parentId":"managing-notebook-servers","name":"Stopping notebook servers owned by other users","level":2,"index":3,"id":"stopping-notebook-servers-owned-by-other-users_managing-resources"},{"parentId":"managing-notebook-servers","name":"Stopping idle notebooks","level":2,"index":4,"id":"stopping-idle-notebooks_managing-resources"},{"parentId":"managing-notebook-servers","name":"Adding notebook pod tolerations","level":2,"index":5,"id":"adding-notebook-pod-tolerations_managing-resources"},{"parentId":"managing-notebook-servers","name":"Troubleshooting common problems in Jupyter for administrators","level":2,"index":6,"id":"troubleshooting-common-problems-in-jupyter-for-administrators_managing-resources"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_managing-resources","name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":3,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_managing-resources","name":"A user&#8217;s notebook server does not start","level":3,"index":1,"id":"_a_users_notebook_server_does_not_start"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_managing-resources","name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":3,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/monitoring-data-science-models/"},"sections":[{"parentId":null,"name":"Overview of model monitoring","level":1,"index":0,"id":"overview-of-model-monitoring_monitor"},{"parentId":null,"name":"Configuring TrustyAI","level":1,"index":1,"id":"configuring-trustyai_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Configuring monitoring for the multi-model serving platform","level":2,"index":0,"id":"configuring-monitoring-for-the-multi-model-serving-platform_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Enabling the TrustyAI component","level":2,"index":1,"id":"enabling-trustyai-component_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Configuring TrustyAI with a database","level":2,"index":2,"id":"configuring-trustyai-with-a-database_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Installing the TrustyAI service for a project","level":2,"index":3,"id":"installing-trustyai-service_monitor"},{"parentId":"installing-trustyai-service_monitor","name":"Installing the TrustyAI service by using the dashboard","level":3,"index":0,"id":"installing-trustyai-service-using-dashboard_monitor"},{"parentId":"installing-trustyai-service_monitor","name":"Installing the TrustyAI service by using the CLI","level":3,"index":1,"id":"installing-trustyai-service-using-cli_monitor"},{"parentId":null,"name":"Setting up TrustyAI for your project","level":1,"index":2,"id":"setting-up-trustyai-for-your-project_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Authenticating the TrustyAI service","level":2,"index":0,"id":"authenticating-trustyai-service_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Sending training data to TrustyAI","level":2,"index":1,"id":"sending-training-data-to-trustyai_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Labeling data fields","level":2,"index":2,"id":"labeling-data-fields_monitor"},{"parentId":null,"name":"Monitoring model bias","level":1,"index":3,"id":"monitoring-model-bias_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Creating a bias metric","level":2,"index":0,"id":"creating-a-bias-metric_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the dashboard","level":3,"index":0,"id":"creating-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the CLI","level":3,"index":1,"id":"creating-a-bias-metric-using-cli_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Duplicating a bias metric","level":3,"index":2,"id":"duplicating-a-bias-metric_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Deleting a bias metric","level":2,"index":1,"id":"deleting-a-bias-metric_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the dashboard","level":3,"index":0,"id":"deleting-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the CLI","level":3,"index":1,"id":"deleting-a-bias-metric-using-cli_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Viewing bias metrics for a model","level":2,"index":2,"id":"viewing-bias-metrics_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Supported bias metrics","level":2,"index":3,"id":"supported-bias-metrics_bias-monitoring"},{"parentId":null,"name":"Monitoring data drift","level":1,"index":4,"id":"monitoring-data-drift_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Creating a drift metric","level":2,"index":0,"id":"creating-a-drift-metric_drift-monitoring"},{"parentId":"creating-a-drift-metric_drift-monitoring","name":"Creating a drift metric by using the CLI","level":3,"index":0,"id":"creating-a-drift-metric-using-cli_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Deleting a drift metric by using the CLI","level":2,"index":1,"id":"deleting-a-drift-metric-using-cli_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Viewing data drift metrics for a model","level":2,"index":2,"id":"viewing-drift-metrics_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Supported drift metrics","level":2,"index":3,"id":"supported-drift-metrics_drift-monitoring"},{"parentId":null,"name":"Using explainability","level":1,"index":5,"id":"using-explainability_explainers"},{"parentId":"using-explainability_explainers","name":"Requesting a LIME explanation","level":2,"index":0,"id":"requesting-a-lime-explanation_explainers"},{"parentId":"requesting-a-lime-explanation_explainers","name":"Requesting a LIME explanation by using the CLI","level":3,"index":0,"id":"requesting-a-lime-explanation-using-CLI_explainers"},{"parentId":"using-explainability_explainers","name":"Requesting a SHAP explanation","level":2,"index":1,"id":"requesting-a-shap-explanation_explainers"},{"parentId":"requesting-a-shap-explanation_explainers","name":"Requesting a SHAP explanation by using the CLI","level":3,"index":0,"id":"requesting-a-shap-explanation-using-CLI_explainers"},{"parentId":"using-explainability_explainers","name":"Supported explainers","level":2,"index":2,"id":"supported-explainers_explainers"},{"parentId":null,"name":"Bias monitoring tutorial - Gender bias example","level":1,"index":6,"id":"bias-monitoring-tutorial_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Introduction","level":2,"index":0,"id":"t-bias-introduction_bias-tutorial"},{"parentId":"t-bias-introduction_bias-tutorial","name":"About the example models","level":3,"index":0,"id":"_about_the_example_models"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Setting up your environment","level":2,"index":1,"id":"t-bias-setting-up-your-environment_bias-tutorial"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Downloading the tutorial files","level":3,"index":0,"id":"_downloading_the_tutorial_files"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Logging in to the OpenShift cluster from the command line","level":3,"index":1,"id":"_logging_in_to_the_openshift_cluster_from_the_command_line"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Configuring monitoring for the model serving platform","level":3,"index":2,"id":"_configuring_monitoring_for_the_model_serving_platform"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Enabling the TrustyAI component","level":3,"index":3,"id":"enabling-trustyai-component_bias-tutorial"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Setting up a project","level":3,"index":4,"id":"_setting_up_a_project"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Authenticating the TrustyAI service","level":3,"index":5,"id":"_authenticating_the_trustyai_service"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Deploying models","level":2,"index":2,"id":"t-bias-deploying-models_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Sending training data to the models","level":2,"index":3,"id":"t-bias-sending-training-data-to-the-models_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Labeling data fields","level":2,"index":4,"id":"t-bias-labeling-data-fields_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Checking model fairness","level":2,"index":5,"id":"t-bias-checking-model-fairness_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Scheduling a fairness metric request","level":2,"index":6,"id":"t-bias-scheduling-a-fairness-metric-request_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Scheduling an identity metric request","level":2,"index":7,"id":"t-bias-scheduling-an-identity-metric-request_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Simulating real world data","level":2,"index":8,"id":"t-bias-simulating-real-world-data_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Reviewing the results","level":2,"index":9,"id":"t-bias-reviewing-the-results_bias-tutorial"},{"parentId":"t-bias-reviewing-the-results_bias-tutorial","name":"Are the models biased?","level":3,"index":0,"id":"_are_the_models_biased"},{"parentId":"t-bias-reviewing-the-results_bias-tutorial","name":"How does the production data compare to the training data?","level":3,"index":1,"id":"_how_does_the_production_data_compare_to_the_training_data"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/serving-models/"},"sections":[{"parentId":null,"name":"About model serving","level":1,"index":0,"id":"about-model-serving_about-model-serving"},{"parentId":null,"name":"Serving small and medium-sized models","level":1,"index":1,"id":"serving-small-and-medium-sized-models_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Configuring model servers","level":2,"index":0,"id":"_configuring_model_servers"},{"parentId":"_configuring_model_servers","name":"Enabling the multi-model serving platform","level":3,"index":0,"id":"enabling-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a custom model-serving runtime for the multi-model serving platform","level":3,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a tested and verified model-serving runtime for the multi-model serving platform","level":3,"index":2,"id":"adding-a-tested-and-verified-model-serving-runtime-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a model server for the multi-model serving platform","level":3,"index":3,"id":"adding-a-model-server-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Deleting a model server","level":3,"index":4,"id":"deleting-a-model-server_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Working with deployed models","level":2,"index":1,"id":"_working_with_deployed_models"},{"parentId":"_working_with_deployed_models","name":"Deploying a model by using the multi-model serving platform","level":3,"index":0,"id":"deploying-a-model-using-the-multi-model-serving-platform_model-serving"},{"parentId":"_working_with_deployed_models","name":"Viewing a deployed model","level":3,"index":1,"id":"viewing-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Updating the deployment properties of a deployed model","level":3,"index":2,"id":"updating-the-deployment-properties-of-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Deleting a deployed model","level":3,"index":3,"id":"deleting-a-deployed-model_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Configuring monitoring for the multi-model serving platform","level":2,"index":2,"id":"configuring-monitoring-for-the-multi-model-serving-platform_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Viewing model-serving runtime metrics for the multi-model serving platform","level":2,"index":3,"id":"viewing-metrics-for-the-multi-model-serving-platform_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Monitoring model performance","level":2,"index":4,"id":"_monitoring_model_performance"},{"parentId":"_monitoring_model_performance","name":"Viewing performance metrics for all models on a model server","level":3,"index":0,"id":"viewing-performance-metrics-for-model-server_model-serving"},{"parentId":"_monitoring_model_performance","name":"Viewing HTTP request metrics for a deployed model","level":3,"index":1,"id":"viewing-http-request-metrics-for-a-deployed-model_model-serving"},{"parentId":null,"name":"Serving large models","level":1,"index":2,"id":"serving-large-models_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"About the single-model serving platform","level":2,"index":0,"id":"about-the-single-model-serving-platform_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"About KServe deployment modes","level":2,"index":1,"id":"about-kserve-deployment-modes_serving-large-models"},{"parentId":"about-kserve-deployment-modes_serving-large-models","name":"Serverless mode","level":3,"index":0,"id":"_serverless_mode"},{"parentId":"about-kserve-deployment-modes_serving-large-models","name":"Raw deployment mode","level":3,"index":1,"id":"_raw_deployment_mode"},{"parentId":"serving-large-models_serving-large-models","name":"Installing KServe","level":2,"index":2,"id":"installing-kserve_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Deploying models by using the single-model serving platform","level":2,"index":3,"id":"deploying-models-using-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Enabling the single-model serving platform","level":3,"index":0,"id":"enabling-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Adding a custom model-serving runtime for the single-model serving platform","level":3,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Adding a tested and verified model-serving runtime for the single-model serving platform","level":3,"index":2,"id":"adding-a-tested-and-verified-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Deploying models on the single-model serving platform","level":3,"index":3,"id":"deploying-models-on-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Using OCI containers for model storage","level":3,"index":4,"id":"using-oci-containers-for-model-storage_serving-large-models"},{"parentId":"using-oci-containers-for-model-storage_serving-large-models","name":"Creating an OCI image and storing a model in the container image","level":4,"index":0,"id":"_creating_an_oci_image_and_storing_a_model_in_the_container_image"},{"parentId":"using-oci-containers-for-model-storage_serving-large-models","name":"Deploying a model stored in an OCI image from a public repository","level":4,"index":1,"id":"_deploying_a_model_stored_in_an_oci_image_from_a_public_repository"},{"parentId":"using-oci-containers-for-model-storage_serving-large-models","name":"Deploying a model stored in an OCI image from a private repository","level":4,"index":2,"id":"_deploying_a_model_stored_in_an_oci_image_from_a_private_repository"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Accessing the inference endpoint for a deployed model","level":3,"index":5,"id":"accessing-inference-endpoint-for-deployed-model_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Configuring monitoring for the single-model serving platform","level":2,"index":4,"id":"configuring-monitoring-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Viewing model-serving runtime metrics for the single-model serving platform","level":2,"index":5,"id":"viewing-metrics-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Monitoring model performance","level":2,"index":6,"id":"_monitoring_model_performance_2"},{"parentId":"_monitoring_model_performance_2","name":"Viewing performance metrics for a deployed model","level":3,"index":0,"id":"viewing-performance-metrics-for-deployed-model_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Optimizing model-serving runtimes","level":2,"index":7,"id":"_optimizing_model_serving_runtimes"},{"parentId":"_optimizing_model_serving_runtimes","name":"Optimizing the vLLM model-serving runtime","level":3,"index":0,"id":"optimizing-the-vllm-runtime_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Performance tuning on the single-model serving platform","level":2,"index":8,"id":"_performance_tuning_on_the_single_model_serving_platform"},{"parentId":"_performance_tuning_on_the_single_model_serving_platform","name":"Resolving CUDA out-of-memory errors","level":3,"index":0,"id":"resolving-cuda-oom-errors-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Supported model-serving runtimes","level":2,"index":9,"id":"supported-model-serving-runtimes_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Tested and verified model-serving runtimes","level":2,"index":10,"id":"tested-verified-runtimes_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Inference endpoints","level":2,"index":11,"id":"inference-endpoints_serving-large-models"},{"parentId":"inference-endpoints_serving-large-models","name":"Caikit TGIS ServingRuntime for KServe","level":3,"index":0,"id":"_caikit_tgis_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"Caikit Standalone ServingRuntime for KServe","level":3,"index":1,"id":"_caikit_standalone_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"TGIS Standalone ServingRuntime for KServe","level":3,"index":2,"id":"_tgis_standalone_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"OpenVINO Model Server","level":3,"index":3,"id":"_openvino_model_server"},{"parentId":"inference-endpoints_serving-large-models","name":"vLLM ServingRuntime for KServe","level":3,"index":4,"id":"_vllm_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"NVIDIA Triton Inference Server","level":3,"index":5,"id":"_nvidia_triton_inference_server"},{"parentId":"inference-endpoints_serving-large-models","name":"Additional resources","level":3,"index":6,"id":"_additional_resources"},{"parentId":"serving-large-models_serving-large-models","name":"About the NVIDIA NIM model serving platform","level":2,"index":12,"id":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models"},{"parentId":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models","name":"Enabling the NVIDIA NIM model serving platform","level":3,"index":0,"id":"enabling-the-nvidia-nim-model-serving-platform_serving-large-models"},{"parentId":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models","name":"Deploying models on the NVIDIA NIM model serving platform","level":3,"index":1,"id":"deploying-models-on-the-NVIDIA-NIM-model-serving-platform_serving-large-models"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/upgrading-open-data-hub/"},"sections":[{"parentId":null,"name":"Overview of upgrading Open Data Hub","level":1,"index":0,"id":"overview-of-upgrading-odh_upgrade"},{"parentId":null,"name":"Requirements for upgrading Open Data Hub","level":1,"index":1,"id":"requirements-for-upgrading-odh_upgrade"},{"parentId":null,"name":"Upgrading Open Data Hub version 1 to version 2","level":1,"index":2,"id":"upgrading-odh-v1-to-v2_upgradev1"},{"parentId":"upgrading-odh-v1-to-v2_upgradev1","name":"Upgrading the Open Data Hub Operator version 1","level":2,"index":0,"id":"upgrading-the-odh-operator-v1_upgradev1"},{"parentId":"upgrading-odh-v1-to-v2_upgradev1","name":"Installing Open Data Hub components","level":2,"index":1,"id":"installing-odh-components_upgradev1"},{"parentId":"upgrading-odh-v1-to-v2_upgradev1","name":"Accessing the Open Data Hub dashboard","level":2,"index":2,"id":"accessing-the-odh-dashboard_upgradev1"},{"parentId":null,"name":"Upgrading Open Data Hub version 2.0 to version 2.2","level":1,"index":3,"id":"upgrading-odh-v2_upgradev2"},{"parentId":"upgrading-odh-v2_upgradev2","name":"Installing Open Data Hub version 2","level":2,"index":0,"id":"installing-odh-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing the Open Data Hub Operator version 2","level":3,"index":0,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing Open Data Hub components","level":3,"index":1,"id":"installing-odh-components_installv2"},{"parentId":"upgrading-odh-v2_upgradev2","name":"Adding a CA bundle after upgrading","level":2,"index":1,"id":"adding-a-ca-bundle-after-upgrading_upgradev2"},{"parentId":null,"name":"Adding a CA bundle after upgrading","level":1,"index":4,"id":"adding-a-ca-bundle-after-upgrading_upgradev2"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-in-your-data-science-ide/"},"sections":[{"parentId":null,"name":"Accessing your workbench IDE","level":1,"index":0,"id":"accessing-your-workbench-ide_ide"},{"parentId":null,"name":"Working in JupyterLab","level":1,"index":1,"id":"_working_in_jupyterlab"},{"parentId":"_working_in_jupyterlab","name":"Creating and importing notebooks","level":2,"index":0,"id":"creating-and-importing-notebooks_ide"},{"parentId":"creating-and-importing-notebooks_ide","name":"Creating a new notebook","level":3,"index":0,"id":"creating-a-new-notebook_ide"},{"parentId":"creating-and-importing-notebooks_ide","name":"Uploading an existing notebook file from local storage","level":3,"index":1,"id":"uploading-an-existing-notebook-file-from-local-storage_ide"},{"parentId":"creating-and-importing-notebooks_ide","name":"Additional resources","level":3,"index":2,"id":"_additional_resources"},{"parentId":"_working_in_jupyterlab","name":"Collaborating on notebooks by using Git","level":2,"index":1,"id":"collaborating-on-notebooks-by-using-git_ide"},{"parentId":"collaborating-on-notebooks-by-using-git_ide","name":"Uploading an existing notebook file from a Git repository by using JupyterLab","level":3,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_ide"},{"parentId":"collaborating-on-notebooks-by-using-git_ide","name":"Uploading an existing notebook file from a Git repository by using the command line interface","level":3,"index":1,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_ide"},{"parentId":"collaborating-on-notebooks-by-using-git_ide","name":"Updating your project with changes from a remote Git repository","level":3,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_ide"},{"parentId":"collaborating-on-notebooks-by-using-git_ide","name":"Pushing project changes to a Git repository","level":3,"index":3,"id":"pushing-project-changes-to-a-git-repository_ide"},{"parentId":"_working_in_jupyterlab","name":"Managing Python packages","level":2,"index":2,"id":"managing-python-packages_ide"},{"parentId":"managing-python-packages_ide","name":"Viewing Python packages installed on your notebook server","level":3,"index":0,"id":"viewing-python-packages-installed-on-your-notebook-server_ide"},{"parentId":"managing-python-packages_ide","name":"Installing Python packages on your notebook server","level":3,"index":1,"id":"installing-python-packages-on-your-notebook-server_ide"},{"parentId":"_working_in_jupyterlab","name":"Troubleshooting common problems in Jupyter for users","level":2,"index":3,"id":"troubleshooting-common-problems-in-jupyter-for-users_ide"},{"parentId":null,"name":"Working in code-server","level":1,"index":2,"id":"working-in-code-server_ide"},{"parentId":"working-in-code-server_ide","name":"Installing extensions with code-server","level":2,"index":0,"id":"_installing_extensions_with_code_server"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-on-data-science-projects/"},"sections":[{"parentId":null,"name":"Using data science projects","level":1,"index":0,"id":"using-data-science-projects_projects"},{"parentId":"using-data-science-projects_projects","name":"Creating a data science project","level":2,"index":0,"id":"creating-a-data-science-project_projects"},{"parentId":"using-data-science-projects_projects","name":"Updating a data science project","level":2,"index":1,"id":"updating-a-data-science-project_projects"},{"parentId":"using-data-science-projects_projects","name":"Deleting a data science project","level":2,"index":2,"id":"deleting-a-data-science-project_projects"},{"parentId":null,"name":"Using project workbenches","level":1,"index":1,"id":"using-project-workbenches_projects"},{"parentId":"using-project-workbenches_projects","name":"Creating a workbench and selecting an IDE","level":2,"index":0,"id":"creating-a-workbench-select-ide_projects"},{"parentId":"creating-a-workbench-select-ide_projects","name":"About workbench images","level":3,"index":0,"id":"about-workbench-images_projects"},{"parentId":"creating-a-workbench-select-ide_projects","name":"Creating a workbench","level":3,"index":1,"id":"creating-a-project-workbench_projects"},{"parentId":"using-project-workbenches_projects","name":"Starting a workbench","level":2,"index":1,"id":"starting-a-workbench_projects"},{"parentId":"using-project-workbenches_projects","name":"Updating a project workbench","level":2,"index":2,"id":"updating-a-project-workbench_projects"},{"parentId":"using-project-workbenches_projects","name":"Deleting a workbench from a data science project","level":2,"index":3,"id":"deleting-a-workbench-from-a-data-science-project_projects"},{"parentId":null,"name":"Using data connections","level":1,"index":2,"id":"using-data-connections_projects"},{"parentId":"using-data-connections_projects","name":"Adding a data connection to your data science project","level":2,"index":0,"id":"adding-a-data-connection-to-your-data-science-project_projects"},{"parentId":"using-data-connections_projects","name":"Deleting a data connection","level":2,"index":1,"id":"deleting-a-data-connection_projects"},{"parentId":"using-data-connections_projects","name":"Updating a connected data source","level":2,"index":2,"id":"updating-a-connected-data-source_projects"},{"parentId":null,"name":"Configuring cluster storage","level":1,"index":3,"id":"configuring-cluster-storage_projects"},{"parentId":"configuring-cluster-storage_projects","name":"Adding cluster storage to your data science project","level":2,"index":0,"id":"adding-cluster-storage-to-your-data-science-project_projects"},{"parentId":"configuring-cluster-storage_projects","name":"Updating cluster storage","level":2,"index":1,"id":"updating-cluster-storage_projects"},{"parentId":"configuring-cluster-storage_projects","name":"Changing the storage class for an existing cluster storage instance","level":2,"index":2,"id":"changing-the-storage-class-for-an-existing-cluster-storage-instance_projects"},{"parentId":"configuring-cluster-storage_projects","name":"Deleting cluster storage from a data science project","level":2,"index":3,"id":"deleting-cluster-storage-from-a-data-science-project_projects"},{"parentId":null,"name":"Managing access to data science projects","level":1,"index":4,"id":"managing-access-to-data-science-projects_projects"},{"parentId":"managing-access-to-data-science-projects_projects","name":"Configuring access to a data science project","level":2,"index":0,"id":"configuring-access-to-a-data-science-project_projects"},{"parentId":"managing-access-to-data-science-projects_projects","name":"Sharing access to a data science project","level":2,"index":1,"id":"sharing-access-to-a-data-science-project_projects"},{"parentId":"managing-access-to-data-science-projects_projects","name":"Updating access to a data science project","level":2,"index":2,"id":"updating-access-to-a-data-science-project_projects"},{"parentId":"managing-access-to-data-science-projects_projects","name":"Removing access to a data science project","level":2,"index":3,"id":"removing-access-to-a-data-science-project_projects"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-accelerators/"},"sections":[{"parentId":null,"name":"Overview of accelerators","level":1,"index":0,"id":"overview-of-accelerators_accelerators"},{"parentId":null,"name":"Enabling NVIDIA GPUs","level":1,"index":1,"id":"enabling-nvidia-gpus_accelerators"},{"parentId":null,"name":"Intel Gaudi AI Accelerator integration","level":1,"index":2,"id":"intel-gaudi-ai-accelerator-integration_accelerators"},{"parentId":null,"name":"Working with accelerator profiles","level":1,"index":3,"id":"working-with-accelerator-profiles_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Creating an accelerator profile","level":2,"index":0,"id":"creating-an-accelerator-profile_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Updating an accelerator profile","level":2,"index":1,"id":"updating-an-accelerator-profile_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Deleting an accelerator profile","level":2,"index":2,"id":"deleting-an-accelerator-profile_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Viewing accelerator profiles","level":2,"index":3,"id":"viewing-accelerator-profiles_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Configuring a recommended accelerator for notebook images","level":2,"index":4,"id":"configuring-a-recommended-accelerator-for-notebook-images_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Configuring a recommended accelerator for serving runtimes","level":2,"index":5,"id":"configuring-a-recommended-accelerator-for-serving-runtimes_accelerators"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-connected-applications/"},"sections":[{"parentId":null,"name":"Viewing applications that are connected to Open Data Hub","level":1,"index":0,"id":"viewing-connected-applications_connected-apps"},{"parentId":null,"name":"Enabling applications that are connected to Open Data Hub","level":1,"index":1,"id":"enabling-applications-connected_connected-apps"},{"parentId":null,"name":"Removing disabled applications from the dashboard","level":1,"index":2,"id":"removing-disabled-applications_connected-apps"},{"parentId":null,"name":"Using the Jupyter application","level":1,"index":3,"id":"_using_the_jupyter_application"},{"parentId":"_using_the_jupyter_application","name":"Starting a Jupyter notebook server","level":2,"index":0,"id":"starting-a-jupyter-notebook-server_connected-apps"},{"parentId":"_using_the_jupyter_application","name":"Creating and importing notebooks","level":2,"index":1,"id":"creating-and-importing-notebooks_connected-apps"},{"parentId":"creating-and-importing-notebooks_connected-apps","name":"Creating a new notebook","level":3,"index":0,"id":"creating-a-new-notebook_connected-apps"},{"parentId":"creating-and-importing-notebooks_connected-apps","name":"Uploading an existing notebook file from local storage","level":3,"index":1,"id":"uploading-an-existing-notebook-file-from-local-storage_connected-apps"},{"parentId":"creating-and-importing-notebooks_connected-apps","name":"Additional resources","level":3,"index":2,"id":"_additional_resources"},{"parentId":"_using_the_jupyter_application","name":"Collaborating on notebooks by using Git","level":2,"index":2,"id":"collaborating-on-notebooks-by-using-git_connected-apps"},{"parentId":"collaborating-on-notebooks-by-using-git_connected-apps","name":"Uploading an existing notebook file from a Git repository by using JupyterLab","level":3,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_connected-apps"},{"parentId":"collaborating-on-notebooks-by-using-git_connected-apps","name":"Uploading an existing notebook file from a Git repository by using the command line interface","level":3,"index":1,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_connected-apps"},{"parentId":"collaborating-on-notebooks-by-using-git_connected-apps","name":"Updating your project with changes from a remote Git repository","level":3,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_connected-apps"},{"parentId":"collaborating-on-notebooks-by-using-git_connected-apps","name":"Pushing project changes to a Git repository","level":3,"index":3,"id":"pushing-project-changes-to-a-git-repository_connected-apps"},{"parentId":"_using_the_jupyter_application","name":"Managing Python packages","level":2,"index":3,"id":"managing-python-packages_connected-apps"},{"parentId":"managing-python-packages_connected-apps","name":"Viewing Python packages installed on your notebook server","level":3,"index":0,"id":"viewing-python-packages-installed-on-your-notebook-server_connected-apps"},{"parentId":"managing-python-packages_connected-apps","name":"Installing Python packages on your notebook server","level":3,"index":1,"id":"installing-python-packages-on-your-notebook-server_connected-apps"},{"parentId":"_using_the_jupyter_application","name":"Updating notebook server settings by restarting your server","level":2,"index":4,"id":"updating-notebook-server-settings-by-restarting-your-server_connected-apps"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-data-in-s3-compatible-object-store/"},"sections":[{"parentId":null,"name":"Prerequisites","level":1,"index":0,"id":"s3-prerequisites_s3"},{"parentId":null,"name":"Creating an S3 client","level":1,"index":1,"id":"creating-an-s3-client_s3"},{"parentId":null,"name":"Listing available buckets in your object store","level":1,"index":2,"id":"listing-available-amazon-buckets_s3"},{"parentId":null,"name":"Creating a bucket in your object store","level":1,"index":3,"id":"creating-an-s3-bucket_s3"},{"parentId":null,"name":"Listing files in your bucket","level":1,"index":4,"id":"listing-files-in-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Downloading files from your bucket","level":1,"index":5,"id":"downloading-files-from-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Uploading files to your bucket","level":1,"index":6,"id":"uploading-files-to-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Copying files between buckets","level":1,"index":7,"id":"copying-files-to-between-buckets_s3"},{"parentId":null,"name":"Deleting files from your bucket","level":1,"index":8,"id":"Deleting-files-on-your-object-store_s3"},{"parentId":null,"name":"Deleting a bucket from your object store","level":1,"index":9,"id":"deleting-a-s3-bucket_s3"},{"parentId":null,"name":"Overview of object storage endpoints","level":1,"index":10,"id":"overview-of-object-storage-endpoints_s3"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"MinIO (On-Cluster)","level":2,"index":0,"id":"_minio_on_cluster"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Amazon S3","level":2,"index":1,"id":"_amazon_s3"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Other S3-Compatible Object Stores","level":2,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Verification and Troubleshooting","level":2,"index":3,"id":"_verification_and_troubleshooting"},{"parentId":null,"name":"Additional resources","level":1,"index":11,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Enabling data science pipelines 2.0","level":1,"index":0,"id":"enabling-data-science-pipelines-2_ds-pipelines"},{"parentId":"enabling-data-science-pipelines-2_ds-pipelines","name":"Installing Open Data Hub with data science pipelines 2.0","level":2,"index":0,"id":"_installing_open_data_hub_with_data_science_pipelines_2_0"},{"parentId":"enabling-data-science-pipelines-2_ds-pipelines","name":"Upgrading to data science pipelines 2.0","level":2,"index":1,"id":"_upgrading_to_data_science_pipelines_2_0"},{"parentId":null,"name":"Managing data science pipelines","level":1,"index":1,"id":"managing-data-science-pipelines_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Configuring a pipeline server","level":2,"index":0,"id":"configuring-a-pipeline-server_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Defining a pipeline","level":2,"index":1,"id":"defining-a-pipeline_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Importing a data science pipeline","level":2,"index":2,"id":"importing-a-data-science-pipeline_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Deleting a data science pipeline","level":2,"index":3,"id":"deleting-a-data-science-pipeline_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Deleting a pipeline server","level":2,"index":4,"id":"deleting-a-pipeline-server_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Viewing the details of a pipeline server","level":2,"index":5,"id":"viewing-the-details-of-a-pipeline-server_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Viewing existing pipelines","level":2,"index":6,"id":"viewing-existing-pipelines_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Overview of pipeline versions","level":2,"index":7,"id":"overview-of-pipeline-versions_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Uploading a pipeline version","level":2,"index":8,"id":"uploading-a-pipeline-version_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Deleting a pipeline version","level":2,"index":9,"id":"deleting-a-pipeline-version_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Viewing the details of a pipeline version","level":2,"index":10,"id":"viewing-the-details-of-a-pipeline-version_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Downloading a data science pipeline version","level":2,"index":11,"id":"downloading-a-data-science-pipeline-version_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Overview of data science pipelines caching","level":2,"index":12,"id":"overview-of-data-science-pipelines-caching_ds-pipelines"},{"parentId":"overview-of-data-science-pipelines-caching_ds-pipelines","name":"Caching criteria","level":3,"index":0,"id":"_caching_criteria"},{"parentId":"overview-of-data-science-pipelines-caching_ds-pipelines","name":"Viewing cached steps in the Open Data Hub user interface","level":3,"index":1,"id":"_viewing_cached_steps_in_the_open_data_hub_user_interface"},{"parentId":"overview-of-data-science-pipelines-caching_ds-pipelines","name":"Disabling caching for specific tasks or pipelines","level":3,"index":2,"id":"_disabling_caching_for_specific_tasks_or_pipelines"},{"parentId":"_disabling_caching_for_specific_tasks_or_pipelines","name":"Disabling caching for individual tasks","level":4,"index":0,"id":"_disabling_caching_for_individual_tasks"},{"parentId":"_disabling_caching_for_specific_tasks_or_pipelines","name":"Disabling caching for pipelines","level":4,"index":1,"id":"_disabling_caching_for_pipelines"},{"parentId":"overview-of-data-science-pipelines-caching_ds-pipelines","name":"Verification and troubleshooting","level":3,"index":3,"id":"_verification_and_troubleshooting"},{"parentId":null,"name":"Managing pipeline experiments","level":1,"index":2,"id":"managing-pipeline-experiments_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Overview of pipeline experiments","level":2,"index":0,"id":"overview-of-pipeline-experiments_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Creating a pipeline experiment","level":2,"index":1,"id":"creating-a-pipeline-experiment_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Archiving a pipeline experiment","level":2,"index":2,"id":"archiving-a-pipeline-experiment_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Deleting an archived pipeline experiment","level":2,"index":3,"id":"deleting-an-archived-pipeline-experiment_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Restoring an archived pipeline experiment","level":2,"index":4,"id":"restoring-an-archived-pipeline-experiment_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Viewing pipeline task executions","level":2,"index":5,"id":"viewing-pipeline-task-executions_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Viewing pipeline artifacts","level":2,"index":6,"id":"viewing-pipeline-artifacts_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Comparing runs","level":2,"index":7,"id":"comparing-runs_ds-pipelines"},{"parentId":null,"name":"Managing pipeline runs","level":1,"index":3,"id":"managing-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Overview of pipeline runs","level":2,"index":0,"id":"overview-of-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Storing data with data science pipelines","level":2,"index":1,"id":"storing-data-with-data-science-pipelines_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Viewing active pipeline runs","level":2,"index":2,"id":"viewing-active-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Executing a pipeline run","level":2,"index":3,"id":"executing-a-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Stopping an active pipeline run","level":2,"index":4,"id":"stopping-an-active-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Duplicating an active pipeline run","level":2,"index":5,"id":"duplicating-an-active-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Viewing scheduled pipeline runs","level":2,"index":6,"id":"viewing-scheduled-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Scheduling a pipeline run using a cron job","level":2,"index":7,"id":"scheduling-a-pipeline-run-using-a-cron-job_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Scheduling a pipeline run","level":2,"index":8,"id":"scheduling-a-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Duplicating a scheduled pipeline run","level":2,"index":9,"id":"duplicating-a-scheduled-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Deleting a scheduled pipeline run","level":2,"index":10,"id":"deleting-a-scheduled-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Viewing the details of a pipeline run","level":2,"index":11,"id":"viewing-the-details-of-a-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Viewing archived pipeline runs","level":2,"index":12,"id":"viewing-archived-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Archiving a pipeline run","level":2,"index":13,"id":"archiving-a-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Restoring an archived pipeline run","level":2,"index":14,"id":"restoring-an-archived-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Deleting an archived pipeline run","level":2,"index":15,"id":"deleting-an-archived-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Duplicating an archived pipeline run","level":2,"index":16,"id":"duplicating-an-archived-pipeline-run_ds-pipelines"},{"parentId":null,"name":"Working with pipeline logs","level":1,"index":4,"id":"working-with-pipeline-logs_ds-pipelines"},{"parentId":"working-with-pipeline-logs_ds-pipelines","name":"About pipeline logs","level":2,"index":0,"id":"about-pipeline-logs_ds-pipelines"},{"parentId":"working-with-pipeline-logs_ds-pipelines","name":"Viewing pipeline step logs","level":2,"index":1,"id":"viewing-pipeline-step-logs_ds-pipelines"},{"parentId":"working-with-pipeline-logs_ds-pipelines","name":"Downloading pipeline step logs","level":2,"index":2,"id":"downloading-pipeline-step-logs_ds-pipelines"},{"parentId":null,"name":"Working with pipelines in JupyterLab","level":1,"index":5,"id":"working-with-pipelines-in-jupyterlab_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Overview of pipelines in JupyterLab","level":2,"index":0,"id":"overview-of-pipelines-in-jupyterlab_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Accessing the pipeline editor","level":2,"index":1,"id":"accessing-the-pipeline-editor_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Creating a runtime configuration","level":2,"index":2,"id":"creating-a-runtime-configuration_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Updating a runtime configuration","level":2,"index":3,"id":"updating-a-runtime-configuration_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Deleting a runtime configuration","level":2,"index":4,"id":"deleting-a-runtime-configuration_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Duplicating a runtime configuration","level":2,"index":5,"id":"duplicating-a-runtime-configuration_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Running a pipeline in JupyterLab","level":2,"index":6,"id":"running-a-pipeline-in-jupyterlab_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Exporting a pipeline in JupyterLab","level":2,"index":7,"id":"exporting-a-pipeline-in-jupyterlab_ds-pipelines"},{"parentId":null,"name":"Troubleshooting DSPA component errors","level":1,"index":6,"id":"troubleshooting-dspa-component-errors_ds-pipelines"},{"parentId":"troubleshooting-dspa-component-errors_ds-pipelines","name":"Common errors across DSP components","level":2,"index":0,"id":"_common_errors_across_dsp_components"},{"parentId":null,"name":"Additional resources","level":1,"index":7,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-distributed-workloads/"},"sections":[{"parentId":null,"name":"Overview of distributed workloads","level":1,"index":0,"id":"overview-of-distributed-workloads_distributed-workloads"},{"parentId":null,"name":"Managing custom training images","level":1,"index":1,"id":"managing-custom-training-images_distributed-workloads"},{"parentId":"managing-custom-training-images_distributed-workloads","name":"About base training images","level":2,"index":0,"id":"about-base-training-images_distributed-workloads"},{"parentId":"managing-custom-training-images_distributed-workloads","name":"Creating a custom training image","level":2,"index":1,"id":"creating-a-custom-training-image_distributed-workloads"},{"parentId":"managing-custom-training-images_distributed-workloads","name":"Pushing an image to the integrated OpenShift image registry","level":2,"index":2,"id":"pushing-an-image-to-the-integrated-openshift-image-registry_distributed-workloads"},{"parentId":null,"name":"Running distributed workloads","level":1,"index":2,"id":"running-distributed-workloads_distributed-workloads"},{"parentId":"running-distributed-workloads_distributed-workloads","name":"Running distributed data science workloads from notebooks","level":2,"index":0,"id":"running-distributed-data-science-workloads-from-notebooks_distributed-workloads"},{"parentId":"running-distributed-data-science-workloads-from-notebooks_distributed-workloads","name":"Downloading the demo notebooks from the CodeFlare SDK","level":3,"index":0,"id":"downloading-the-demo-notebooks-from-the-codeflare-sdk_distributed-workloads"},{"parentId":"running-distributed-data-science-workloads-from-notebooks_distributed-workloads","name":"Running the demo notebooks from the CodeFlare SDK","level":3,"index":1,"id":"running-the-demo-notebooks-from-the-codeflare-sdk_distributed-workloads"},{"parentId":"running-distributed-data-science-workloads-from-notebooks_distributed-workloads","name":"Managing Ray clusters from within a Jupyter notebook","level":3,"index":2,"id":"managing-ray-clusters-from-within-a-jupyter-notebook_distributed-workloads"},{"parentId":"running-distributed-workloads_distributed-workloads","name":"Running distributed data science workloads from data science pipelines","level":2,"index":1,"id":"running-distributed-data-science-workloads-from-ds-pipelines_distributed-workloads"},{"parentId":null,"name":"Monitoring distributed workloads","level":1,"index":3,"id":"monitoring-distributed-workloads_distributed-workloads"},{"parentId":"monitoring-distributed-workloads_distributed-workloads","name":"Viewing project metrics for distributed workloads","level":2,"index":0,"id":"viewing-project-metrics-for-distributed-workloads_distributed-workloads"},{"parentId":"monitoring-distributed-workloads_distributed-workloads","name":"Viewing the status of distributed workloads","level":2,"index":1,"id":"viewing-the-status-of-distributed-workloads_distributed-workloads"},{"parentId":"monitoring-distributed-workloads_distributed-workloads","name":"Viewing Kueue alerts for distributed workloads","level":2,"index":2,"id":"viewing-kueue-alerts-for-distributed-workloads_distributed-workloads"},{"parentId":null,"name":"Tuning a model by using the Training Operator","level":1,"index":4,"id":"tuning-a-model-by-using-the-training-operator_distributed-workloads"},{"parentId":"tuning-a-model-by-using-the-training-operator_distributed-workloads","name":"Configuring the training job","level":2,"index":0,"id":"configuring-the-training-job_distributed-workloads"},{"parentId":"tuning-a-model-by-using-the-training-operator_distributed-workloads","name":"Running the training job","level":2,"index":1,"id":"running-the-training-job_distributed-workloads"},{"parentId":"tuning-a-model-by-using-the-training-operator_distributed-workloads","name":"Monitoring the training job","level":2,"index":2,"id":"monitoring-the-training-job_distributed-workloads"},{"parentId":null,"name":"Troubleshooting common problems with distributed workloads for users","level":1,"index":5,"id":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My Ray cluster is in a suspended state","level":2,"index":0,"id":"_my_ray_cluster_is_in_a_suspended_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My Ray cluster is in a failed state","level":2,"index":1,"id":"_my_ray_cluster_is_in_a_failed_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a <strong>failed to call webhook</strong> error message for the CodeFlare Operator","level":2,"index":2,"id":"_i_see_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a <strong>failed to call webhook</strong> error message for Kueue","level":2,"index":3,"id":"_i_see_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My Ray cluster doesn&#8217;t start","level":2,"index":4,"id":"_my_ray_cluster_doesnt_start"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":2,"index":5,"id":"_i_see_a_default_local_queue_not_found_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a <strong>local_queue provided does not exist</strong> error message","level":2,"index":6,"id":"_i_see_a_local_queue_provided_does_not_exist_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I cannot create a Ray cluster or submit jobs","level":2,"index":7,"id":"_i_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My pod provisioned by Kueue is terminated before my image is pulled","level":2,"index":8,"id":"_my_pod_provisioned_by_kueue_is_terminated_before_my_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-model-registries/"},"sections":[{"parentId":null,"name":"Overview of model registries","level":1,"index":0,"id":"overview-of-model-registries_model-registry"},{"parentId":null,"name":"Configuring the model registry component","level":0,"index":1,"id":"_configuring_the_model_registry_component"},{"parentId":"_configuring_the_model_registry_component","name":"Configuring the model registry component","level":1,"index":0,"id":"configuring-the-model-registry-component_model-registry"},{"parentId":null,"name":"Managing model registries","level":0,"index":2,"id":"_managing_model_registries"},{"parentId":"_managing_model_registries","name":"Creating a model registry","level":1,"index":0,"id":"creating-a-model-registry_model-registry"},{"parentId":"_managing_model_registries","name":"Managing model registry permissions","level":1,"index":1,"id":"managing-model-registry-permissionsmodel-registry"},{"parentId":"_managing_model_registries","name":"Deleting a model registry","level":1,"index":2,"id":"deleting-a-model-registry_model-registry"},{"parentId":null,"name":"Working with model registries","level":0,"index":3,"id":"_working_with_model_registries"},{"parentId":"_working_with_model_registries","name":"Working with model registries","level":1,"index":0,"id":"working-with-model-registries_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Registering a model","level":2,"index":0,"id":"registering-a-model_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Registering a model version","level":2,"index":1,"id":"registering-a-model-version_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Viewing registered models","level":2,"index":2,"id":"viewing-registered-models_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Viewing registered model versions","level":2,"index":3,"id":"viewing-registered-model-versions_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Editing model metadata in a model registry","level":2,"index":4,"id":"editing-model-metadata-in-a-model-registry_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Editing model version metadata in a model registry","level":2,"index":5,"id":"editing-model-version-metadata-in-a-model-registry_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Deploying a model version from a model registry","level":2,"index":6,"id":"deploying-a-model-version-from-a-model-registry_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Archiving a model","level":2,"index":7,"id":"archiving-a-model_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Archiving a model version","level":2,"index":8,"id":"archiving-a-model-version_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Restoring a model","level":2,"index":9,"id":"restoring-a-model_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Restoring a model version","level":2,"index":10,"id":"restoring-a-model-version_model-registry"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/_artifacts/document-attributes-global/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/bias-monitoring-tutorial/"},"sections":[{"parentId":null,"name":"Introduction","level":1,"index":0,"id":"t-bias-introduction_bias-tutorial"},{"parentId":"t-bias-introduction_bias-tutorial","name":"About the example models","level":2,"index":0,"id":"_about_the_example_models"},{"parentId":null,"name":"Setting up your environment","level":1,"index":1,"id":"t-bias-setting-up-your-environment_bias-tutorial"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Downloading the tutorial files","level":2,"index":0,"id":"_downloading_the_tutorial_files"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Logging in to the OpenShift cluster from the command line","level":2,"index":1,"id":"_logging_in_to_the_openshift_cluster_from_the_command_line"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Configuring monitoring for the model serving platform","level":2,"index":2,"id":"_configuring_monitoring_for_the_model_serving_platform"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Enabling the TrustyAI component","level":2,"index":3,"id":"enabling-trustyai-component_bias-tutorial"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Setting up a project","level":2,"index":4,"id":"_setting_up_a_project"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Authenticating the TrustyAI service","level":2,"index":5,"id":"_authenticating_the_trustyai_service"},{"parentId":null,"name":"Deploying models","level":1,"index":2,"id":"t-bias-deploying-models_bias-tutorial"},{"parentId":null,"name":"Sending training data to the models","level":1,"index":3,"id":"t-bias-sending-training-data-to-the-models_bias-tutorial"},{"parentId":null,"name":"Labeling data fields","level":1,"index":4,"id":"t-bias-labeling-data-fields_bias-tutorial"},{"parentId":null,"name":"Checking model fairness","level":1,"index":5,"id":"t-bias-checking-model-fairness_bias-tutorial"},{"parentId":null,"name":"Scheduling a fairness metric request","level":1,"index":6,"id":"t-bias-scheduling-a-fairness-metric-request_bias-tutorial"},{"parentId":null,"name":"Scheduling an identity metric request","level":1,"index":7,"id":"t-bias-scheduling-an-identity-metric-request_bias-tutorial"},{"parentId":null,"name":"Simulating real world data","level":1,"index":8,"id":"t-bias-simulating-real-world-data_bias-tutorial"},{"parentId":null,"name":"Reviewing the results","level":1,"index":9,"id":"t-bias-reviewing-the-results_bias-tutorial"},{"parentId":"t-bias-reviewing-the-results_bias-tutorial","name":"Are the models biased?","level":2,"index":0,"id":"_are_the_models_biased"},{"parentId":"t-bias-reviewing-the-results_bias-tutorial","name":"How does the production data compare to the training data?","level":2,"index":1,"id":"_how_does_the_production_data_compare_to_the_training_data"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/collaborating-on-notebooks-by-using-git/"},"sections":[{"parentId":null,"name":"Uploading an existing notebook file from a Git repository by using JupyterLab","level":1,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_{context}"},{"parentId":null,"name":"Uploading an existing notebook file from a Git repository by using the command line interface","level":1,"index":1,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_{context}"},{"parentId":null,"name":"Updating your project with changes from a remote Git repository","level":1,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_{context}"},{"parentId":null,"name":"Pushing project changes to a Git repository","level":1,"index":3,"id":"pushing-project-changes-to-a-git-repository_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-cluster-storage/"},"sections":[{"parentId":null,"name":"Adding cluster storage to your data science project","level":1,"index":0,"id":"adding-cluster-storage-to-your-data-science-project_{context}"},{"parentId":null,"name":"Updating cluster storage","level":1,"index":1,"id":"updating-cluster-storage_{context}"},{"parentId":null,"name":"Changing the storage class for an existing cluster storage instance","level":1,"index":2,"id":"changing-the-storage-class-for-an-existing-cluster-storage-instance_{context}"},{"parentId":null,"name":"Deleting cluster storage from a data science project","level":1,"index":3,"id":"deleting-cluster-storage-from-a-data-science-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-the-odh-operator-logger/"},"sections":[{"parentId":null,"name":"Configuring the {productname-short} Operator logger","level":1,"index":0,"id":"configuring-the-operator-logger_operator-log"},{"parentId":"configuring-the-operator-logger_operator-log","name":"Viewing the {productname-short} Operator log","level":2,"index":0,"id":"_viewing_the_productname_short_operator_log"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-trustyai/"},"sections":[{"parentId":null,"name":"Configuring monitoring for the multi-model serving platform","level":1,"index":0,"id":"configuring-monitoring-for-the-multi-model-serving-platform_{context}"},{"parentId":null,"name":"Enabling the TrustyAI component","level":1,"index":1,"id":"enabling-trustyai-component_{context}"},{"parentId":null,"name":"Configuring TrustyAI with a database","level":1,"index":2,"id":"configuring-trustyai-with-a-database_{context}"},{"parentId":null,"name":"Installing the TrustyAI service for a project","level":1,"index":3,"id":"installing-trustyai-service_{context}"},{"parentId":"installing-trustyai-service_{context}","name":"Installing the TrustyAI service by using the dashboard","level":2,"index":0,"id":"installing-trustyai-service-using-dashboard_{context}"},{"parentId":"installing-trustyai-service_{context}","name":"Installing the TrustyAI service by using the CLI","level":2,"index":1,"id":"installing-trustyai-service-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/creating-and-importing-notebooks/"},"sections":[{"parentId":null,"name":"Creating a new notebook","level":1,"index":0,"id":"creating-a-new-notebook_{context}"},{"parentId":null,"name":"Uploading an existing notebook file from local storage","level":1,"index":1,"id":"uploading-an-existing-notebook-file-from-local-storage_{context}"},{"parentId":null,"name":"Additional resources","level":1,"index":2,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/creating-custom-workbench-images/"},"sections":[{"parentId":null,"name":"Creating a custom image from a default {productname-short} image","level":1,"index":0,"id":"creating-a-custom-image-from-default-image_custom-images"},{"parentId":null,"name":"Creating a custom image from your own image","level":1,"index":1,"id":"creating-a-custom-image-from-your-own-image_custom-images"},{"parentId":"creating-a-custom-image-from-your-own-image_custom-images","name":"Basic guidelines for creating your own workbench image","level":2,"index":0,"id":"_basic_guidelines_for_creating_your_own_workbench_image"},{"parentId":"creating-a-custom-image-from-your-own-image_custom-images","name":"Advanced guidelines for creating your own workbench image","level":2,"index":1,"id":"_advanced_guidelines_for_creating_your_own_workbench_image"},{"parentId":null,"name":"Enabling custom images in {productname-short}","level":1,"index":2,"id":"enabling-custom-images_custom-images"},{"parentId":null,"name":"Importing a custom workbench image","level":1,"index":3,"id":"importing-a-custom-workbench-image_custom-images"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/customizing-component-deployment-resources/"},"sections":[{"parentId":null,"name":"Overview of component resource customization","level":1,"index":0,"id":"overview-of-component-resource-customization_managing-resources"},{"parentId":null,"name":"Customizing component resources","level":1,"index":1,"id":"customizing-component-resources_managing-resources"},{"parentId":null,"name":"Disabling component resource customization","level":1,"index":2,"id":"disabling-component-resource-customization_managing-resources"},{"parentId":null,"name":"Re-enabling component resource customization","level":1,"index":3,"id":"reenabling-component-resource-customization_managing-resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/customizing-the-dashboard/"},"sections":[{"parentId":null,"name":"Editing the dashboard configuration file","level":1,"index":0,"id":"editing-the-dashboard-configuration-file_dashboard"},{"parentId":null,"name":"Dashboard configuration options","level":1,"index":1,"id":"ref-dashboard-configuration-options_dashboard"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/installing-odh-v1/"},"sections":[{"parentId":null,"name":"Installing the Open Data Hub Operator version 1","level":1,"index":0,"id":"installing-the-odh-operator-v1_installv1"},{"parentId":null,"name":"Creating a new project for your Open Data Hub instance","level":1,"index":1,"id":"creating-a-new-project-for-your-odh-instance_installv1"},{"parentId":null,"name":"Adding an Open Data Hub instance","level":1,"index":2,"id":"adding-an-odh-instance_installv1"},{"parentId":null,"name":"Accessing the Open Data Hub dashboard","level":1,"index":3,"id":"accessing-the-odh-dashboard_installv1"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/installing-odh-v2/"},"sections":[{"parentId":null,"name":"Installing the Open Data Hub Operator version 2","level":1,"index":0,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":null,"name":"Installing Open Data Hub components","level":1,"index":1,"id":"installing-odh-components_installv2"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-access-to-data-science-projects/"},"sections":[{"parentId":null,"name":"Configuring access to a data science project","level":1,"index":0,"id":"configuring-access-to-a-data-science-project_{context}"},{"parentId":null,"name":"Sharing access to a data science project","level":1,"index":1,"id":"sharing-access-to-a-data-science-project_{context}"},{"parentId":null,"name":"Updating access to a data science project","level":1,"index":2,"id":"updating-access-to-a-data-science-project_{context}"},{"parentId":null,"name":"Removing access to a data science project","level":1,"index":3,"id":"removing-access-to-a-data-science-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-applications-that-show-in-the-dashboard/"},"sections":[{"parentId":null,"name":"Adding an application to the dashboard","level":1,"index":0,"id":"adding-an-application-to-the-dashboard_dashboard"},{"parentId":null,"name":"Preventing users from adding applications to the dashboard","level":1,"index":1,"id":"preventing-users-from-adding-applications-to-the-dashboard_dashboard"},{"parentId":null,"name":"Disabling applications connected to {productname-short}","level":1,"index":2,"id":"disabling-applications-connected_dashboard"},{"parentId":null,"name":"Showing or hiding information about enabled applications","level":1,"index":3,"id":"showing-hiding-information-about-enabled-applications_dashboard"},{"parentId":null,"name":"Hiding the default Jupyter application","level":1,"index":4,"id":"hiding-the-default-jupyter-application_dashboard"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-cluster-pvc-size/"},"sections":[{"parentId":null,"name":"Configuring the default PVC size for your cluster","level":1,"index":0,"id":"configuring-the-default-pvc-size-for-your-cluster_{context}"},{"parentId":null,"name":"Restoring the default PVC size for your cluster","level":1,"index":1,"id":"restoring-the-default-pvc-size-for-your-cluster_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-custom-training-images/"},"sections":[{"parentId":null,"name":"About base training images","level":1,"index":0,"id":"about-base-training-images_{context}"},{"parentId":null,"name":"Creating a custom training image","level":1,"index":1,"id":"creating-a-custom-training-image_{context}"},{"parentId":null,"name":"Pushing an image to the integrated OpenShift image registry","level":1,"index":2,"id":"pushing-an-image-to-the-integrated-openshift-image-registry_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Configuring a pipeline server","level":1,"index":0,"id":"configuring-a-pipeline-server_{context}"},{"parentId":null,"name":"Defining a pipeline","level":1,"index":1,"id":"defining-a-pipeline_{context}"},{"parentId":null,"name":"Importing a data science pipeline","level":1,"index":2,"id":"importing-a-data-science-pipeline_{context}"},{"parentId":null,"name":"Deleting a data science pipeline","level":1,"index":3,"id":"deleting-a-data-science-pipeline_{context}"},{"parentId":null,"name":"Deleting a pipeline server","level":1,"index":4,"id":"deleting-a-pipeline-server_{context}"},{"parentId":null,"name":"Viewing the details of a pipeline server","level":1,"index":5,"id":"viewing-the-details-of-a-pipeline-server_{context}"},{"parentId":null,"name":"Viewing existing pipelines","level":1,"index":6,"id":"viewing-existing-pipelines_{context}"},{"parentId":null,"name":"Overview of pipeline versions","level":1,"index":7,"id":"overview-of-pipeline-versions_{context}"},{"parentId":null,"name":"Uploading a pipeline version","level":1,"index":8,"id":"uploading-a-pipeline-version_{context}"},{"parentId":null,"name":"Deleting a pipeline version","level":1,"index":9,"id":"deleting-a-pipeline-version_{context}"},{"parentId":null,"name":"Viewing the details of a pipeline version","level":1,"index":10,"id":"viewing-the-details-of-a-pipeline-version_{context}"},{"parentId":null,"name":"Downloading a data science pipeline version","level":1,"index":11,"id":"downloading-a-data-science-pipeline-version_{context}"},{"parentId":null,"name":"Overview of data science pipelines caching","level":1,"index":12,"id":"overview-of-data-science-pipelines-caching_{context}"},{"parentId":"overview-of-data-science-pipelines-caching_{context}","name":"Caching criteria","level":2,"index":0,"id":"_caching_criteria"},{"parentId":"overview-of-data-science-pipelines-caching_{context}","name":"Viewing cached steps in the {productname-short} user interface","level":2,"index":1,"id":"_viewing_cached_steps_in_the_productname_short_user_interface"},{"parentId":"overview-of-data-science-pipelines-caching_{context}","name":"Disabling caching for specific tasks or pipelines","level":2,"index":2,"id":"_disabling_caching_for_specific_tasks_or_pipelines"},{"parentId":"_disabling_caching_for_specific_tasks_or_pipelines","name":"Disabling caching for individual tasks","level":3,"index":0,"id":"_disabling_caching_for_individual_tasks"},{"parentId":"_disabling_caching_for_specific_tasks_or_pipelines","name":"Disabling caching for pipelines","level":3,"index":1,"id":"_disabling_caching_for_pipelines"},{"parentId":"overview-of-data-science-pipelines-caching_{context}","name":"Verification and troubleshooting","level":2,"index":3,"id":"_verification_and_troubleshooting"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-notebook-servers/"},"sections":[{"parentId":null,"name":"Accessing the Jupyter administration interface","level":1,"index":0,"id":"accessing-the-jupyter-administration-interface_{context}"},{"parentId":null,"name":"Starting notebook servers owned by other users","level":1,"index":1,"id":"starting-notebook-servers-owned-by-other-users_{context}"},{"parentId":null,"name":"Accessing notebook servers owned by other users","level":1,"index":2,"id":"accessing-notebook-servers-owned-by-other-users_{context}"},{"parentId":null,"name":"Stopping notebook servers owned by other users","level":1,"index":3,"id":"stopping-notebook-servers-owned-by-other-users_{context}"},{"parentId":null,"name":"Stopping idle notebooks","level":1,"index":4,"id":"stopping-idle-notebooks_{context}"},{"parentId":null,"name":"Adding notebook pod tolerations","level":1,"index":5,"id":"adding-notebook-pod-tolerations_{context}"},{"parentId":null,"name":"Troubleshooting common problems in Jupyter for administrators","level":1,"index":6,"id":"troubleshooting-common-problems-in-jupyter-for-administrators_{context}"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_{context}","name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":2,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_{context}","name":"A user&#8217;s notebook server does not start","level":2,"index":1,"id":"_a_users_notebook_server_does_not_start"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_{context}","name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":2,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-pipeline-experiments/"},"sections":[{"parentId":null,"name":"Overview of pipeline experiments","level":1,"index":0,"id":"overview-of-pipeline-experiments_{context}"},{"parentId":null,"name":"Creating a pipeline experiment","level":1,"index":1,"id":"creating-a-pipeline-experiment_{context}"},{"parentId":null,"name":"Archiving a pipeline experiment","level":1,"index":2,"id":"archiving-a-pipeline-experiment_{context}"},{"parentId":null,"name":"Deleting an archived pipeline experiment","level":1,"index":3,"id":"deleting-an-archived-pipeline-experiment_{context}"},{"parentId":null,"name":"Restoring an archived pipeline experiment","level":1,"index":4,"id":"restoring-an-archived-pipeline-experiment_{context}"},{"parentId":null,"name":"Viewing pipeline task executions","level":1,"index":5,"id":"viewing-pipeline-task-executions_{context}"},{"parentId":null,"name":"Viewing pipeline artifacts","level":1,"index":6,"id":"viewing-pipeline-artifacts_{context}"},{"parentId":null,"name":"Comparing runs","level":1,"index":7,"id":"comparing-runs_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-pipeline-runs/"},"sections":[{"parentId":null,"name":"Overview of pipeline runs","level":1,"index":0,"id":"overview-of-pipeline-runs_{context}"},{"parentId":null,"name":"Storing data with data science pipelines","level":1,"index":1,"id":"storing-data-with-data-science-pipelines_{context}"},{"parentId":null,"name":"Viewing active pipeline runs","level":1,"index":2,"id":"viewing-active-pipeline-runs_{context}"},{"parentId":null,"name":"Executing a pipeline run","level":1,"index":3,"id":"executing-a-pipeline-run_{context}"},{"parentId":null,"name":"Stopping an active pipeline run","level":1,"index":4,"id":"stopping-an-active-pipeline-run_{context}"},{"parentId":null,"name":"Duplicating an active pipeline run","level":1,"index":5,"id":"duplicating-an-active-pipeline-run_{context}"},{"parentId":null,"name":"Viewing scheduled pipeline runs","level":1,"index":6,"id":"viewing-scheduled-pipeline-runs_{context}"},{"parentId":null,"name":"Scheduling a pipeline run using a cron job","level":1,"index":7,"id":"scheduling-a-pipeline-run-using-a-cron-job_{context}"},{"parentId":null,"name":"Scheduling a pipeline run","level":1,"index":8,"id":"scheduling-a-pipeline-run_{context}"},{"parentId":null,"name":"Duplicating a scheduled pipeline run","level":1,"index":9,"id":"duplicating-a-scheduled-pipeline-run_{context}"},{"parentId":null,"name":"Deleting a scheduled pipeline run","level":1,"index":10,"id":"deleting-a-scheduled-pipeline-run_{context}"},{"parentId":null,"name":"Viewing the details of a pipeline run","level":1,"index":11,"id":"viewing-the-details-of-a-pipeline-run_{context}"},{"parentId":null,"name":"Viewing archived pipeline runs","level":1,"index":12,"id":"viewing-archived-pipeline-runs_{context}"},{"parentId":null,"name":"Archiving a pipeline run","level":1,"index":13,"id":"archiving-a-pipeline-run_{context}"},{"parentId":null,"name":"Restoring an archived pipeline run","level":1,"index":14,"id":"restoring-an-archived-pipeline-run_{context}"},{"parentId":null,"name":"Deleting an archived pipeline run","level":1,"index":15,"id":"deleting-an-archived-pipeline-run_{context}"},{"parentId":null,"name":"Duplicating an archived pipeline run","level":1,"index":16,"id":"duplicating-an-archived-pipeline-run_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-python-packages/"},"sections":[{"parentId":null,"name":"Viewing Python packages installed on your notebook server","level":1,"index":0,"id":"viewing-python-packages-installed-on-your-notebook-server_{context}"},{"parentId":null,"name":"Installing Python packages on your notebook server","level":1,"index":1,"id":"installing-python-packages-on-your-notebook-server_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-storage-classes/"},"sections":[{"parentId":null,"name":"Configuring storage class settings","level":1,"index":0,"id":"configuring-storage-class-settings_{context}"},{"parentId":null,"name":"Configuring the default storage class for your cluster","level":1,"index":1,"id":"configuring-the-default-storage-class-for-your-cluster_{context}"},{"parentId":null,"name":"Overview of object storage endpoints","level":1,"index":2,"id":"overview-of-object-storage-endpoints_{context}"},{"parentId":"overview-of-object-storage-endpoints_{context}","name":"MinIO (On-Cluster)","level":2,"index":0,"id":"_minio_on_cluster"},{"parentId":"overview-of-object-storage-endpoints_{context}","name":"Amazon S3","level":2,"index":1,"id":"_amazon_s3"},{"parentId":"overview-of-object-storage-endpoints_{context}","name":"Other S3-Compatible Object Stores","level":2,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":"overview-of-object-storage-endpoints_{context}","name":"Verification and Troubleshooting","level":2,"index":3,"id":"_verification_and_troubleshooting"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-users-and-groups/"},"sections":[{"parentId":null,"name":"Overview of user types and permissions","level":1,"index":0,"id":"overview-of-user-types-and-permissions_{context}"},{"parentId":null,"name":"Viewing {productname-short} users","level":1,"index":1,"id":"viewing-data-science-users_{context}"},{"parentId":null,"name":"Adding users to {productname-short} user groups","level":1,"index":2,"id":"adding-users-to-user-groups_{context}"},{"parentId":null,"name":"Selecting {productname-short} administrator and user groups","level":1,"index":3,"id":"selecting-admin-and-user-groups_{context}"},{"parentId":null,"name":"Deleting users","level":1,"index":4,"id":"_deleting_users"},{"parentId":"_deleting_users","name":"About deleting users and their resources","level":2,"index":0,"id":"about-deleting-users-and-resources_{context}"},{"parentId":"_deleting_users","name":"Stopping notebook servers owned by other users","level":2,"index":1,"id":"stopping-notebook-servers-owned-by-other-users_{context}"},{"parentId":"_deleting_users","name":"Revoking user access to Jupyter","level":2,"index":2,"id":"revoking-user-access-to-jupyter_{context}"},{"parentId":"_deleting_users","name":"Backing up storage data","level":2,"index":3,"id":"backing-up-storage-data_{context}"},{"parentId":"_deleting_users","name":"Cleaning up after deleting users","level":2,"index":4,"id":"cleaning-up-after-deleting-users_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-data-drift/"},"sections":[{"parentId":null,"name":"Creating a drift metric","level":1,"index":0,"id":"creating-a-drift-metric_drift-monitoring"},{"parentId":"creating-a-drift-metric_drift-monitoring","name":"Creating a drift metric by using the CLI","level":2,"index":0,"id":"creating-a-drift-metric-using-cli_drift-monitoring"},{"parentId":null,"name":"Deleting a drift metric by using the CLI","level":1,"index":1,"id":"deleting-a-drift-metric-using-cli_drift-monitoring"},{"parentId":null,"name":"Viewing data drift metrics for a model","level":1,"index":2,"id":"viewing-drift-metrics_drift-monitoring"},{"parentId":null,"name":"Supported drift metrics","level":1,"index":3,"id":"supported-drift-metrics_drift-monitoring"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-distributed-workloads/"},"sections":[{"parentId":null,"name":"Viewing project metrics for distributed workloads","level":1,"index":0,"id":"viewing-project-metrics-for-distributed-workloads_{context}"},{"parentId":null,"name":"Viewing the status of distributed workloads","level":1,"index":1,"id":"viewing-the-status-of-distributed-workloads_{context}"},{"parentId":null,"name":"Viewing Kueue alerts for distributed workloads","level":1,"index":2,"id":"viewing-kueue-alerts-for-distributed-workloads_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-model-bias/"},"sections":[{"parentId":null,"name":"Creating a bias metric","level":1,"index":0,"id":"creating-a-bias-metric_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the dashboard","level":2,"index":0,"id":"creating-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the CLI","level":2,"index":1,"id":"creating-a-bias-metric-using-cli_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Duplicating a bias metric","level":2,"index":2,"id":"duplicating-a-bias-metric_bias-monitoring"},{"parentId":null,"name":"Deleting a bias metric","level":1,"index":1,"id":"deleting-a-bias-metric_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the dashboard","level":2,"index":0,"id":"deleting-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the CLI","level":2,"index":1,"id":"deleting-a-bias-metric-using-cli_bias-monitoring"},{"parentId":null,"name":"Viewing bias metrics for a model","level":1,"index":2,"id":"viewing-bias-metrics_bias-monitoring"},{"parentId":null,"name":"Supported bias metrics","level":1,"index":3,"id":"supported-bias-metrics_bias-monitoring"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-model-performance/"},"sections":[{"parentId":null,"name":"Viewing performance metrics for all models on a model server","level":1,"index":0,"id":"viewing-performance-metrics-for-model-server_monitoring-model-performance"},{"parentId":null,"name":"Viewing HTTP request metrics for a deployed model","level":1,"index":1,"id":"viewing-http-request-metrics-for-a-deployed-model_monitoring-model-performance"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/running-distributed-workloads/"},"sections":[{"parentId":null,"name":"Running distributed data science workloads from notebooks","level":1,"index":0,"id":"running-distributed-data-science-workloads-from-notebooks_{context}"},{"parentId":"running-distributed-data-science-workloads-from-notebooks_{context}","name":"Downloading the demo notebooks from the CodeFlare SDK","level":2,"index":0,"id":"downloading-the-demo-notebooks-from-the-codeflare-sdk_{context}"},{"parentId":"running-distributed-data-science-workloads-from-notebooks_{context}","name":"Running the demo notebooks from the CodeFlare SDK","level":2,"index":1,"id":"running-the-demo-notebooks-from-the-codeflare-sdk_{context}"},{"parentId":"running-distributed-data-science-workloads-from-notebooks_{context}","name":"Managing Ray clusters from within a Jupyter notebook","level":2,"index":2,"id":"managing-ray-clusters-from-within-a-jupyter-notebook_{context}"},{"parentId":null,"name":"Running distributed data science workloads from data science pipelines","level":1,"index":1,"id":"running-distributed-data-science-workloads-from-ds-pipelines_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/serving-large-models/"},"sections":[{"parentId":null,"name":"About the single-model serving platform","level":1,"index":0,"id":"about-the-single-model-serving-platform_serving-large-models"},{"parentId":null,"name":"About KServe deployment modes","level":1,"index":1,"id":"about-kserve-deployment-modes_serving-large-models"},{"parentId":"about-kserve-deployment-modes_serving-large-models","name":"Serverless mode","level":2,"index":0,"id":"_serverless_mode"},{"parentId":"about-kserve-deployment-modes_serving-large-models","name":"Raw deployment mode","level":2,"index":1,"id":"_raw_deployment_mode"},{"parentId":null,"name":"Installing KServe","level":1,"index":2,"id":"installing-kserve_serving-large-models"},{"parentId":null,"name":"Deploying models by using the single-model serving platform","level":1,"index":3,"id":"deploying-models-using-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Enabling the single-model serving platform","level":2,"index":0,"id":"enabling-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Adding a custom model-serving runtime for the single-model serving platform","level":2,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Adding a tested and verified model-serving runtime for the single-model serving platform","level":2,"index":2,"id":"adding-a-tested-and-verified-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Deploying models on the single-model serving platform","level":2,"index":3,"id":"deploying-models-on-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Using OCI containers for model storage","level":2,"index":4,"id":"using-oci-containers-for-model-storage_serving-large-models"},{"parentId":"using-oci-containers-for-model-storage_serving-large-models","name":"Creating an OCI image and storing a model in the container image","level":3,"index":0,"id":"_creating_an_oci_image_and_storing_a_model_in_the_container_image"},{"parentId":"using-oci-containers-for-model-storage_serving-large-models","name":"Deploying a model stored in an OCI image from a public repository","level":3,"index":1,"id":"_deploying_a_model_stored_in_an_oci_image_from_a_public_repository"},{"parentId":"using-oci-containers-for-model-storage_serving-large-models","name":"Deploying a model stored in an OCI image from a private repository","level":3,"index":2,"id":"_deploying_a_model_stored_in_an_oci_image_from_a_private_repository"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Accessing the inference endpoint for a deployed model","level":2,"index":5,"id":"accessing-inference-endpoint-for-deployed-model_serving-large-models"},{"parentId":null,"name":"Configuring monitoring for the single-model serving platform","level":1,"index":4,"id":"configuring-monitoring-for-the-single-model-serving-platform_serving-large-models"},{"parentId":null,"name":"Viewing model-serving runtime metrics for the single-model serving platform","level":1,"index":5,"id":"viewing-metrics-for-the-single-model-serving-platform_serving-large-models"},{"parentId":null,"name":"Monitoring model performance","level":1,"index":6,"id":"_monitoring_model_performance"},{"parentId":"_monitoring_model_performance","name":"Viewing performance metrics for a deployed model","level":2,"index":0,"id":"viewing-performance-metrics-for-deployed-model_serving-large-models"},{"parentId":null,"name":"Optimizing model-serving runtimes","level":1,"index":7,"id":"_optimizing_model_serving_runtimes"},{"parentId":"_optimizing_model_serving_runtimes","name":"Optimizing the vLLM model-serving runtime","level":2,"index":0,"id":"optimizing-the-vllm-runtime_serving-large-models"},{"parentId":null,"name":"Performance tuning on the single-model serving platform","level":1,"index":8,"id":"_performance_tuning_on_the_single_model_serving_platform"},{"parentId":"_performance_tuning_on_the_single_model_serving_platform","name":"Resolving CUDA out-of-memory errors","level":2,"index":0,"id":"resolving-cuda-oom-errors-for-the-single-model-serving-platform_serving-large-models"},{"parentId":null,"name":"Supported model-serving runtimes","level":1,"index":9,"id":"supported-model-serving-runtimes_serving-large-models"},{"parentId":null,"name":"Tested and verified model-serving runtimes","level":1,"index":10,"id":"tested-verified-runtimes_serving-large-models"},{"parentId":null,"name":"Inference endpoints","level":1,"index":11,"id":"inference-endpoints_serving-large-models"},{"parentId":"inference-endpoints_serving-large-models","name":"Caikit TGIS ServingRuntime for KServe","level":2,"index":0,"id":"_caikit_tgis_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"Caikit Standalone ServingRuntime for KServe","level":2,"index":1,"id":"_caikit_standalone_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"TGIS Standalone ServingRuntime for KServe","level":2,"index":2,"id":"_tgis_standalone_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"OpenVINO Model Server","level":2,"index":3,"id":"_openvino_model_server"},{"parentId":"inference-endpoints_serving-large-models","name":"vLLM ServingRuntime for KServe","level":2,"index":4,"id":"_vllm_servingruntime_for_kserve"},{"parentId":"inference-endpoints_serving-large-models","name":"NVIDIA Triton Inference Server","level":2,"index":5,"id":"_nvidia_triton_inference_server"},{"parentId":"inference-endpoints_serving-large-models","name":"Additional resources","level":2,"index":6,"id":"_additional_resources"},{"parentId":null,"name":"About the NVIDIA NIM model serving platform","level":1,"index":12,"id":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models"},{"parentId":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models","name":"Enabling the NVIDIA NIM model serving platform","level":2,"index":0,"id":"enabling-the-nvidia-nim-model-serving-platform_serving-large-models"},{"parentId":"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models","name":"Deploying models on the NVIDIA NIM model serving platform","level":2,"index":1,"id":"deploying-models-on-the-NVIDIA-NIM-model-serving-platform_serving-large-models"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/serving-small-and-medium-sized-models/"},"sections":[{"parentId":null,"name":"Configuring model servers","level":1,"index":0,"id":"_configuring_model_servers"},{"parentId":"_configuring_model_servers","name":"Enabling the multi-model serving platform","level":2,"index":0,"id":"enabling-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a custom model-serving runtime for the multi-model serving platform","level":2,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a tested and verified model-serving runtime for the multi-model serving platform","level":2,"index":2,"id":"adding-a-tested-and-verified-model-serving-runtime-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a model server for the multi-model serving platform","level":2,"index":3,"id":"adding-a-model-server-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Deleting a model server","level":2,"index":4,"id":"deleting-a-model-server_model-serving"},{"parentId":null,"name":"Working with deployed models","level":1,"index":1,"id":"_working_with_deployed_models"},{"parentId":"_working_with_deployed_models","name":"Deploying a model by using the multi-model serving platform","level":2,"index":0,"id":"deploying-a-model-using-the-multi-model-serving-platform_model-serving"},{"parentId":"_working_with_deployed_models","name":"Viewing a deployed model","level":2,"index":1,"id":"viewing-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Updating the deployment properties of a deployed model","level":2,"index":2,"id":"updating-the-deployment-properties-of-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Deleting a deployed model","level":2,"index":3,"id":"deleting-a-deployed-model_model-serving"},{"parentId":null,"name":"Configuring monitoring for the multi-model serving platform","level":1,"index":2,"id":"configuring-monitoring-for-the-multi-model-serving-platform_model-serving"},{"parentId":null,"name":"Viewing model-serving runtime metrics for the multi-model serving platform","level":1,"index":3,"id":"viewing-metrics-for-the-multi-model-serving-platform_model-serving"},{"parentId":null,"name":"Monitoring model performance","level":1,"index":4,"id":"_monitoring_model_performance"},{"parentId":"_monitoring_model_performance","name":"Viewing performance metrics for all models on a model server","level":2,"index":0,"id":"viewing-performance-metrics-for-model-server_model-serving"},{"parentId":"_monitoring_model_performance","name":"Viewing HTTP request metrics for a deployed model","level":2,"index":1,"id":"viewing-http-request-metrics-for-a-deployed-model_model-serving"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/setting-up-trustyai-for-your-project/"},"sections":[{"parentId":null,"name":"Authenticating the TrustyAI service","level":1,"index":0,"id":"authenticating-trustyai-service_{context}"},{"parentId":null,"name":"Sending training data to TrustyAI","level":1,"index":1,"id":"sending-training-data-to-trustyai_{context}"},{"parentId":null,"name":"Labeling data fields","level":1,"index":2,"id":"labeling-data-fields_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/tuning-a-model-by-using-the-training-operator/"},"sections":[{"parentId":null,"name":"Configuring the training job","level":1,"index":0,"id":"configuring-the-training-job_{context}"},{"parentId":null,"name":"Running the training job","level":1,"index":1,"id":"running-the-training-job_{context}"},{"parentId":null,"name":"Monitoring the training job","level":1,"index":2,"id":"monitoring-the-training-job_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/upgrading-odh-v1-to-v2/"},"sections":[{"parentId":null,"name":"Upgrading the Open Data Hub Operator version 1","level":1,"index":0,"id":"upgrading-the-odh-operator-v1_upgradev1"},{"parentId":null,"name":"Installing Open Data Hub components","level":1,"index":1,"id":"installing-odh-components_upgradev1"},{"parentId":null,"name":"Accessing the Open Data Hub dashboard","level":1,"index":2,"id":"accessing-the-odh-dashboard_upgradev1"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/upgrading-odh-v2/"},"sections":[{"parentId":null,"name":"Installing Open Data Hub version 2","level":1,"index":0,"id":"installing-odh-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing the Open Data Hub Operator version 2","level":2,"index":0,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing Open Data Hub components","level":2,"index":1,"id":"installing-odh-components_installv2"},{"parentId":null,"name":"Adding a CA bundle after upgrading","level":1,"index":1,"id":"adding-a-ca-bundle-after-upgrading_upgradev2"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-data-connections/"},"sections":[{"parentId":null,"name":"Adding a data connection to your data science project","level":1,"index":0,"id":"adding-a-data-connection-to-your-data-science-project_{context}"},{"parentId":null,"name":"Deleting a data connection","level":1,"index":1,"id":"deleting-a-data-connection_{context}"},{"parentId":null,"name":"Updating a connected data source","level":1,"index":2,"id":"updating-a-connected-data-source_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-data-science-projects/"},"sections":[{"parentId":null,"name":"Creating a data science project","level":1,"index":0,"id":"creating-a-data-science-project_{context}"},{"parentId":null,"name":"Updating a data science project","level":1,"index":1,"id":"updating-a-data-science-project_{context}"},{"parentId":null,"name":"Deleting a data science project","level":1,"index":2,"id":"deleting-a-data-science-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-explainability/"},"sections":[{"parentId":null,"name":"Requesting a LIME explanation","level":1,"index":0,"id":"requesting-a-lime-explanation_explainers"},{"parentId":"requesting-a-lime-explanation_explainers","name":"Requesting a LIME explanation by using the CLI","level":2,"index":0,"id":"requesting-a-lime-explanation-using-CLI_explainers"},{"parentId":null,"name":"Requesting a SHAP explanation","level":1,"index":1,"id":"requesting-a-shap-explanation_explainers"},{"parentId":"requesting-a-shap-explanation_explainers","name":"Requesting a SHAP explanation by using the CLI","level":2,"index":0,"id":"requesting-a-shap-explanation-using-CLI_explainers"},{"parentId":null,"name":"Supported explainers","level":1,"index":2,"id":"supported-explainers_explainers"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-project-workbenches/"},"sections":[{"parentId":null,"name":"Creating a workbench and selecting an IDE","level":1,"index":0,"id":"creating-a-workbench-select-ide_{context}"},{"parentId":"creating-a-workbench-select-ide_{context}","name":"About workbench images","level":2,"index":0,"id":"about-workbench-images_{context}"},{"parentId":"creating-a-workbench-select-ide_{context}","name":"Creating a workbench","level":2,"index":1,"id":"creating-a-project-workbench_{context}"},{"parentId":null,"name":"Starting a workbench","level":1,"index":1,"id":"starting-a-workbench_{context}"},{"parentId":null,"name":"Updating a project workbench","level":1,"index":2,"id":"updating-a-project-workbench_{context}"},{"parentId":null,"name":"Deleting a workbench from a data science project","level":1,"index":3,"id":"deleting-a-workbench-from-a-data-science-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-certificates/"},"sections":[{"parentId":null,"name":"Understanding certificates in {productname-short}","level":1,"index":0,"id":"understanding-certificates_certs"},{"parentId":"understanding-certificates_certs","name":"How CA bundles are injected","level":2,"index":0,"id":"_how_ca_bundles_are_injected"},{"parentId":"understanding-certificates_certs","name":"How the ConfigMap is managed","level":2,"index":1,"id":"_how_the_configmap_is_managed"},{"parentId":null,"name":"Adding a CA bundle","level":1,"index":1,"id":"adding-a-ca-bundle_certs"},{"parentId":null,"name":"Removing a CA bundle","level":1,"index":2,"id":"removing-a-ca-bundle_certs"},{"parentId":null,"name":"Removing a CA bundle from a namespace","level":1,"index":3,"id":"removing-a-ca-bundle-from-a-namespace_certs"},{"parentId":null,"name":"Managing certificates","level":1,"index":4,"id":"managing-certificates_certs"},{"parentId":null,"name":"Using self-signed certificates with {productname-short} components","level":1,"index":5,"id":"_using_self_signed_certificates_with_productname_short_components"},{"parentId":"_using_self_signed_certificates_with_productname_short_components","name":"Using certificates with data science pipelines","level":2,"index":0,"id":"using-certificates-with-data-science-pipelines_certs"},{"parentId":"using-certificates-with-data-science-pipelines_certs","name":"Providing a CA bundle only for data science pipelines","level":3,"index":0,"id":"_providing_a_ca_bundle_only_for_data_science_pipelines"},{"parentId":"_using_self_signed_certificates_with_productname_short_components","name":"Using certificates with workbenches","level":2,"index":1,"id":"using-certificates-with-workbenches_certs"},{"parentId":"using-certificates-with-workbenches_certs","name":"Creating data science pipelines with Elyra and self-signed certificates","level":3,"index":0,"id":"_creating_data_science_pipelines_with_elyra_and_self_signed_certificates"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-data-in-s3-compatible-object-store/"},"sections":[{"parentId":null,"name":"Prerequisites","level":1,"index":0,"id":"s3-prerequisites_s3"},{"parentId":null,"name":"Creating an S3 client","level":1,"index":1,"id":"creating-an-s3-client_s3"},{"parentId":null,"name":"Listing available buckets in your object store","level":1,"index":2,"id":"listing-available-amazon-buckets_s3"},{"parentId":null,"name":"Creating a bucket in your object store","level":1,"index":3,"id":"creating-an-s3-bucket_s3"},{"parentId":null,"name":"Listing files in your bucket","level":1,"index":4,"id":"listing-files-in-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Downloading files from your bucket","level":1,"index":5,"id":"downloading-files-from-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Uploading files to your bucket","level":1,"index":6,"id":"uploading-files-to-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Copying files between buckets","level":1,"index":7,"id":"copying-files-to-between-buckets_s3"},{"parentId":null,"name":"Deleting files from your bucket","level":1,"index":8,"id":"Deleting-files-on-your-object-store_s3"},{"parentId":null,"name":"Deleting a bucket from your object store","level":1,"index":9,"id":"deleting-a-s3-bucket_s3"},{"parentId":null,"name":"Overview of object storage endpoints","level":1,"index":10,"id":"overview-of-object-storage-endpoints_s3"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"MinIO (On-Cluster)","level":2,"index":0,"id":"_minio_on_cluster"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Amazon S3","level":2,"index":1,"id":"_amazon_s3"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Other S3-Compatible Object Stores","level":2,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Verification and Troubleshooting","level":2,"index":3,"id":"_verification_and_troubleshooting"},{"parentId":null,"name":"Additional resources","level":0,"index":11,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-model-registries/"},"sections":[{"parentId":null,"name":"Registering a model","level":1,"index":0,"id":"registering-a-model_model-registry"},{"parentId":null,"name":"Registering a model version","level":1,"index":1,"id":"registering-a-model-version_model-registry"},{"parentId":null,"name":"Viewing registered models","level":1,"index":2,"id":"viewing-registered-models_model-registry"},{"parentId":null,"name":"Viewing registered model versions","level":1,"index":3,"id":"viewing-registered-model-versions_model-registry"},{"parentId":null,"name":"Editing model metadata in a model registry","level":1,"index":4,"id":"editing-model-metadata-in-a-model-registry_model-registry"},{"parentId":null,"name":"Editing model version metadata in a model registry","level":1,"index":5,"id":"editing-model-version-metadata-in-a-model-registry_model-registry"},{"parentId":null,"name":"Deploying a model version from a model registry","level":1,"index":6,"id":"deploying-a-model-version-from-a-model-registry_model-registry"},{"parentId":null,"name":"Archiving a model","level":1,"index":7,"id":"archiving-a-model_model-registry"},{"parentId":null,"name":"Archiving a model version","level":1,"index":8,"id":"archiving-a-model-version_model-registry"},{"parentId":null,"name":"Restoring a model","level":1,"index":9,"id":"restoring-a-model_model-registry"},{"parentId":null,"name":"Restoring a model version","level":1,"index":10,"id":"restoring-a-model-version_model-registry"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-pipeline-logs/"},"sections":[{"parentId":null,"name":"About pipeline logs","level":1,"index":0,"id":"about-pipeline-logs_{context}"},{"parentId":null,"name":"Viewing pipeline step logs","level":1,"index":1,"id":"viewing-pipeline-step-logs_{context}"},{"parentId":null,"name":"Downloading pipeline step logs","level":1,"index":2,"id":"downloading-pipeline-step-logs_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-pipelines-in-jupyterlab/"},"sections":[{"parentId":null,"name":"Overview of pipelines in JupyterLab","level":1,"index":0,"id":"overview-of-pipelines-in-jupyterlab_{context}"},{"parentId":null,"name":"Accessing the pipeline editor","level":1,"index":1,"id":"accessing-the-pipeline-editor_{context}"},{"parentId":null,"name":"Creating a runtime configuration","level":1,"index":2,"id":"creating-a-runtime-configuration_{context}"},{"parentId":null,"name":"Updating a runtime configuration","level":1,"index":3,"id":"updating-a-runtime-configuration_{context}"},{"parentId":null,"name":"Deleting a runtime configuration","level":1,"index":4,"id":"deleting-a-runtime-configuration_{context}"},{"parentId":null,"name":"Duplicating a runtime configuration","level":1,"index":5,"id":"duplicating-a-runtime-configuration_{context}"},{"parentId":null,"name":"Running a pipeline in JupyterLab","level":1,"index":6,"id":"running-a-pipeline-in-jupyterlab_{context}"},{"parentId":null,"name":"Exporting a pipeline in JupyterLab","level":1,"index":7,"id":"exporting-a-pipeline-in-jupyterlab_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-base-training-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-deleting-users-and-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-kserve-deployment-modes/"},"sections":[{"parentId":null,"name":"Serverless mode","level":1,"index":0,"id":"_serverless_mode"},{"parentId":null,"name":"Raw deployment mode","level":1,"index":1,"id":"_raw_deployment_mode"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-model-serving/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-pipeline-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-the-NVIDIA-NIM-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-workbench-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-authorization-token-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-inference-endpoint-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-jupyter-administration-interface/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-odh-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-pipeline-editor/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-your-workbench-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-ca-bundle-after-upgrading/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-data-connection-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-model-server-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-tested-and-verified-runtime-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-tested-and-verified-runtime-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-an-application-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-an-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-cluster-storage-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-notebook-pod-tolerations/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-users-to-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/allocating-additional-resources-to-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/authenticating-trustyai-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/backing-up-storage-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/changing-the-storage-class-for-an-existing-cluster-storage-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/cleaning-up-after-deleting-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/cleaning-up-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/comparing-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-recommended-accelerator-for-notebook-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-recommended-accelerator-for-serving-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-monitoring-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-monitoring-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-quota-management-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-storage-class-settings/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-codeflare-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-default-storage-class-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-model-registry-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-operator-logger/"},"sections":[{"parentId":null,"name":"Viewing the {productname-short} Operator log","level":1,"index":0,"id":"_viewing_the_productname_short_operator_log"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-training-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-trustyai-with-a-database/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/copying-files-between-buckets/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric/"},"sections":[{"parentId":null,"name":"Creating a bias metric by using the dashboard","level":1,"index":0,"id":"creating-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Creating a bias metric by using the CLI","level":1,"index":1,"id":"creating-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-custom-image-from-default-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-custom-image-from-your-own-image/"},"sections":[{"parentId":null,"name":"Basic guidelines for creating your own workbench image","level":1,"index":0,"id":"_basic_guidelines_for_creating_your_own_workbench_image"},{"parentId":null,"name":"Advanced guidelines for creating your own workbench image","level":1,"index":1,"id":"_advanced_guidelines_for_creating_your_own_workbench_image"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-custom-training-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-drift-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-drift-metric/"},"sections":[{"parentId":null,"name":"Creating a drift metric by using the CLI","level":1,"index":0,"id":"creating-a-drift-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-new-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-new-project-for-your-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-workbench-select-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-an-s3-client/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/customizing-component-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/defining-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric/"},"sections":[{"parentId":null,"name":"Deleting a bias metric by using the dashboard","level":1,"index":0,"id":"deleting-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Deleting a bias metric by using the CLI","level":1,"index":1,"id":"deleting-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-data-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-drift-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-workbench-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-cluster-storage-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-files-in-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-model-using-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-model-version-from-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-on-single-node-openshift-using-kserve-raw-deployment-mode/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-on-the-NVIDIA-NIM-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-on-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-using-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/disabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/disabling-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-a-data-science-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-files-from-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-the-demo-notebooks-from-the-codeflare-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-bias-metric/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-model-metadata-in-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-model-version-metadata-in-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-the-dashboard-configuration-file/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-custom-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-data-science-pipelines-2/"},"sections":[{"parentId":null,"name":"Installing {productname-short} with data science pipelines 2.0","level":1,"index":0,"id":"_installing_productname_short_with_data_science_pipelines_2_0"},{"parentId":null,"name":"Upgrading to data science pipelines 2.0","level":1,"index":1,"id":"_upgrading_to_data_science_pipelines_2_0"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-intel-gaudi-ai-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-nvidia-gpus/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-nvidia-nim-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-trustyai-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/evaluating-large-language-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/executing-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/exporting-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/importing-a-custom-workbench-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/importing-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-odh-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-python-packages-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-the-distributed-workloads-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-the-odh-operator-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-trustyai-service-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-trustyai-service-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/intel-gaudi-ai-accelerator-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-trustyai-service/"},"sections":[{"parentId":null,"name":"Installing the TrustyAI service by using the dashboard","level":1,"index":0,"id":"installing-trustyai-service-using-dashboard_{context}"},{"parentId":null,"name":"Installing the TrustyAI service by using the CLI","level":1,"index":1,"id":"installing-trustyai-service-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/labeling-data-fields/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/listing-available-buckets/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/listing-files-in-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/logging-in/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/managing-certificates/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/making-inference-requests-to-models-deployed-on-single-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/managing-model-registry-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/managing-ray-clusters-from-within-a-jupyter-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/monitoring-the-training-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/next-steps-getting-started/"},"sections":[{"parentId":null,"name":"Additional resources","level":1,"index":0,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/nvidia-gpu-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/optimizing-the-vllm-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-for-getting-started/"},"sections":[{"parentId":null,"name":"Data science workflow","level":1,"index":0,"id":"_data_science_workflow"},{"parentId":null,"name":"About this guide","level":1,"index":1,"id":"_about_this_guide"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-data-science-pipelines-caching/"},"sections":[{"parentId":null,"name":"Caching criteria","level":1,"index":0,"id":"_caching_criteria"},{"parentId":null,"name":"Viewing cached steps in the {productname-short} user interface","level":1,"index":1,"id":"_viewing_cached_steps_in_the_productname_short_user_interface"},{"parentId":null,"name":"Disabling caching for specific tasks or pipelines","level":1,"index":2,"id":"_disabling_caching_for_specific_tasks_or_pipelines"},{"parentId":"_disabling_caching_for_specific_tasks_or_pipelines","name":"Disabling caching for individual tasks","level":2,"index":0,"id":"_disabling_caching_for_individual_tasks"},{"parentId":"_disabling_caching_for_specific_tasks_or_pipelines","name":"Disabling caching for pipelines","level":2,"index":1,"id":"_disabling_caching_for_pipelines"},{"parentId":null,"name":"Verification and troubleshooting","level":1,"index":3,"id":"_verification_and_troubleshooting"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-kueue-resources/"},"sections":[{"parentId":null,"name":"Resource flavor","level":1,"index":0,"id":"_resource_flavor"},{"parentId":null,"name":"Cluster queue","level":1,"index":1,"id":"_cluster_queue"},{"parentId":null,"name":"Local queue","level":1,"index":2,"id":"_local_queue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-model-registries/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-model-monitoring/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-object-storage-endpoints/"},"sections":[{"parentId":null,"name":"MinIO (On-Cluster)","level":1,"index":0,"id":"_minio_on_cluster"},{"parentId":null,"name":"Amazon S3","level":1,"index":1,"id":"_amazon_s3"},{"parentId":null,"name":"Other S3-Compatible Object Stores","level":1,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":null,"name":"Verification and Troubleshooting","level":1,"index":3,"id":"_verification_and_troubleshooting"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-experiments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipelines-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-user-types-and-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/preventing-users-from-adding-applications-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/pushing-an-image-to-the-integrated-openshift-image-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/pushing-project-changes-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/reenabling-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-dashboard-configuration-options/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-inference-endpoints/"},"sections":[{"parentId":null,"name":"Caikit TGIS ServingRuntime for KServe","level":1,"index":0,"id":"_caikit_tgis_servingruntime_for_kserve"},{"parentId":null,"name":"Caikit Standalone ServingRuntime for KServe","level":1,"index":1,"id":"_caikit_standalone_servingruntime_for_kserve"},{"parentId":null,"name":"TGIS Standalone ServingRuntime for KServe","level":1,"index":2,"id":"_tgis_standalone_servingruntime_for_kserve"},{"parentId":null,"name":"OpenVINO Model Server","level":1,"index":3,"id":"_openvino_model_server"},{"parentId":null,"name":"vLLM ServingRuntime for KServe","level":1,"index":4,"id":"_vllm_servingruntime_for_kserve"},{"parentId":null,"name":"NVIDIA Triton Inference Server","level":1,"index":5,"id":"_nvidia_triton_inference_server"},{"parentId":null,"name":"Additional resources","level":1,"index":6,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kueue-resource-configurations/"},"sections":[{"parentId":null,"name":"NVIDIA GPUs","level":1,"index":0,"id":"_nvidia_gpus"},{"parentId":"_nvidia_gpus","name":"NVIDIA RTX A400 GPU resource flavor","level":2,"index":0,"id":"_nvidia_rtx_a400_gpu_resource_flavor"},{"parentId":"_nvidia_gpus","name":"NVIDIA RTX A1000 GPU resource flavor","level":2,"index":1,"id":"_nvidia_rtx_a1000_gpu_resource_flavor"},{"parentId":"_nvidia_gpus","name":"NVIDIA RTX A400 GPU cluster queue","level":2,"index":2,"id":"_nvidia_rtx_a400_gpu_cluster_queue"},{"parentId":"_nvidia_gpus","name":"NVIDIA RTX A1000 GPU cluster queue","level":2,"index":3,"id":"_nvidia_rtx_a1000_gpu_cluster_queue"},{"parentId":null,"name":"Additional resources","level":1,"index":1,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-supported-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-tested-verified-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/registering-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/registering-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-a-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-a-ca-bundle-from-a-namespace/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-disabled-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requesting-a-lime-explanation-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requesting-a-lime-explanation/"},"sections":[{"parentId":null,"name":"Requesting a LIME explanation by using the CLI","level":1,"index":0,"id":"requesting-a-lime-explanation-using-CLI_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requesting-a-shap-explanation/"},"sections":[{"parentId":null,"name":"Requesting a SHAP explanation by using the CLI","level":1,"index":0,"id":"requesting-a-shap-explanation-using-CLI_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requesting-a-shap-explanation-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requirements-for-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/resolving-cuda-oom-errors/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/revoking-user-access-to-jupyter/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-disconnected-env/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-from-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-from-ds-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-the-training-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-the-demo-notebooks-from-the-codeflare-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/scheduling-a-pipeline-run-using-a-cron-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/s3-prerequisites/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/selecting-admin-and-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/scheduling-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/sending-training-data-to-trustyai/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/setting-up-lmeval/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/sharing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/showing-hiding-information-about-enabled-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-a-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-a-jupyter-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-idle-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/storing-data-with-data-science-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/hiding-information-about-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/hiding-the-default-jupyter-application/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/supported-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/supported-drift-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/supported-explainers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-checking-model-fairness/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-introduction/"},"sections":[{"parentId":null,"name":"About the example models","level":1,"index":0,"id":"_about_the_example_models"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-deploying-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-labeling-data-fields/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-reviewing-the-results/"},"sections":[{"parentId":null,"name":"Are the models biased?","level":1,"index":0,"id":"_are_the_models_biased"},{"parentId":null,"name":"How does the production data compare to the training data?","level":1,"index":1,"id":"_how_does_the_production_data_compare_to_the_training_data"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-scheduling-a-fairness-metric-request/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-scheduling-an-identity-metric-request/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-sending-training-data-to-the-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-setting-up-your-environment/"},"sections":[{"parentId":null,"name":"Downloading the tutorial files","level":1,"index":0,"id":"_downloading_the_tutorial_files"},{"parentId":null,"name":"Logging in to the OpenShift cluster from the command line","level":1,"index":1,"id":"_logging_in_to_the_openshift_cluster_from_the_command_line"},{"parentId":null,"name":"Configuring monitoring for the model serving platform","level":1,"index":2,"id":"_configuring_monitoring_for_the_model_serving_platform"},{"parentId":null,"name":"Enabling the TrustyAI component","level":1,"index":3,"id":"enabling-trustyai-component_{context}"},{"parentId":null,"name":"Setting up a project","level":1,"index":4,"id":"_setting_up_a_project"},{"parentId":null,"name":"Authenticating the TrustyAI service","level":1,"index":5,"id":"_authenticating_the_trustyai_service"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-simulating-real-world-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-in-jupyter-for-administrators/"},"sections":[{"parentId":null,"name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":1,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":null,"name":"A user&#8217;s notebook server does not start","level":1,"index":1,"id":"_a_users_notebook_server_does_not_start"},{"parentId":null,"name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":1,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-in-jupyter-for-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-with-distributed-workloads-for-administrators/"},"sections":[{"parentId":null,"name":"A user&#8217;s Ray cluster is in a suspended state","level":1,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"A user&#8217;s Ray cluster is in a failed state","level":1,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"A user receives a <strong>failed to call webhook</strong> error message for the CodeFlare Operator","level":1,"index":2,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":null,"name":"A user receives a <strong>failed to call webhook</strong> error message for Kueue","level":1,"index":3,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"A user&#8217;s Ray cluster does not start","level":1,"index":4,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":null,"name":"A user receives a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":1,"index":5,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"A user receives a <strong>local_queue provided does not exist</strong> error message","level":1,"index":6,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"A user cannot create a Ray cluster or submit jobs","level":1,"index":7,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"The user&#8217;s pod provisioned by Kueue is terminated before the user&#8217;s image is pulled","level":1,"index":8,"id":"_the_users_pod_provisioned_by_kueue_is_terminated_before_the_users_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-with-distributed-workloads-for-users/"},"sections":[{"parentId":null,"name":"My Ray cluster is in a suspended state","level":1,"index":0,"id":"_my_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"My Ray cluster is in a failed state","level":1,"index":1,"id":"_my_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"I see a <strong>failed to call webhook</strong> error message for the CodeFlare Operator","level":1,"index":2,"id":"_i_see_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":null,"name":"I see a <strong>failed to call webhook</strong> error message for Kueue","level":1,"index":3,"id":"_i_see_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"My Ray cluster doesn&#8217;t start","level":1,"index":4,"id":"_my_ray_cluster_doesnt_start"},{"parentId":null,"name":"I see a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":1,"index":5,"id":"_i_see_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"I see a <strong>local_queue provided does not exist</strong> error message","level":1,"index":6,"id":"_i_see_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"I cannot create a Ray cluster or submit jobs","level":1,"index":7,"id":"_i_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"My pod provisioned by Kueue is terminated before my image is pulled","level":1,"index":8,"id":"_my_pod_provisioned_by_kueue_is_terminated_before_my_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-dspa-component-errors/"},"sections":[{"parentId":null,"name":"Common errors across DSP components","level":1,"index":0,"id":"_common_errors_across_dsp_components"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/understanding-certificates/"},"sections":[{"parentId":null,"name":"How CA bundles are injected","level":1,"index":0,"id":"_how_ca_bundles_are_injected"},{"parentId":null,"name":"How the ConfigMap is managed","level":1,"index":1,"id":"_how_the_configmap_is_managed"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-connected-data-source/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-cluster-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-the-deployment-properties-of-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-notebook-server-settings-by-restarting-your-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-your-project-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/upgrading-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-files-to-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-certificates-with-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Providing a CA bundle only for data science pipelines","level":1,"index":0,"id":"_providing_a_ca_bundle_only_for_data_science_pipelines"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-certificates-with-workbenches/"},"sections":[{"parentId":null,"name":"Creating data science pipelines with Elyra and self-signed certificates","level":1,"index":0,"id":"_creating_data_science_pipelines_with_elyra_and_self_signed_certificates"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-oci-containers-for-model-storage/"},"sections":[{"parentId":null,"name":"Creating an OCI image and storing a model in the container image","level":1,"index":0,"id":"_creating_an_oci_image_and_storing_a_model_in_the_container_image"},{"parentId":null,"name":"Deploying a model stored in an OCI image from a public repository","level":1,"index":1,"id":"_deploying_a_model_stored_in_an_oci_image_from_a_public_repository"},{"parentId":null,"name":"Deploying a model stored in an OCI image from a private repository","level":1,"index":2,"id":"_deploying_a_model_stored_in_an_oci_image_from_a_private_repository"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-active-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-archived-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-connected-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-drift-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-existing-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-http-request-metrics-for-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-kueue-alerts-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-metrics-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-metrics-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-performance-metrics-for-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-performance-metrics-for-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-pipeline-artifacts/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-pipeline-task-executions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-project-metrics-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-python-packages-installed-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-registered-model-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-registered-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-scheduled-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-status-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/working-in-code-server/"},"sections":[{"parentId":null,"name":"Installing extensions with code-server","level":1,"index":0,"id":"_installing_extensions_with_code_server"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/working-with-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-base-training-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-deleting-users-and-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-model-serving/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-kserve-deployment-modes/"},"sections":[{"parentId":null,"name":"Serverless mode","level":1,"index":0,"id":"_serverless_mode"},{"parentId":null,"name":"Raw deployment mode","level":1,"index":1,"id":"_raw_deployment_mode"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-pipeline-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-the-NVIDIA-NIM-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-workbench-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-authorization-token-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-inference-endpoint-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-jupyter-administration-interface/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-odh-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-pipeline-editor/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-your-workbench-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-ca-bundle-after-upgrading/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-data-connection-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-model-server-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-tested-and-verified-runtime-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-tested-and-verified-runtime-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-an-application-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-an-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-cluster-storage-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-notebook-pod-tolerations/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-users-to-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/allocating-additional-resources-to-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/changing-the-storage-class-for-an-existing-cluster-storage-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/cleaning-up-after-deleting-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/cleaning-up-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/comparing-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-recommended-accelerator-for-notebook-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-recommended-accelerator-for-serving-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-monitoring-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-monitoring-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-quota-management-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-storage-class-settings/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-codeflare-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-default-storage-class-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-model-registry-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-operator-logger/"},"sections":[{"parentId":null,"name":"Viewing the {productname-short} Operator log","level":1,"index":0,"id":"_viewing_the_productname_short_operator_log"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-training-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-trustyai-with-a-database/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/copying-files-between-buckets/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric/"},"sections":[{"parentId":null,"name":"Creating a bias metric by using the dashboard","level":1,"index":0,"id":"creating-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Creating a bias metric by using the CLI","level":1,"index":1,"id":"creating-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-custom-image-from-default-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-custom-image-from-your-own-image/"},"sections":[{"parentId":null,"name":"Basic guidelines for creating your own workbench image","level":1,"index":0,"id":"_basic_guidelines_for_creating_your_own_workbench_image"},{"parentId":null,"name":"Advanced guidelines for creating your own workbench image","level":1,"index":1,"id":"_advanced_guidelines_for_creating_your_own_workbench_image"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-custom-training-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-drift-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-drift-metric/"},"sections":[{"parentId":null,"name":"Creating a drift metric by using the CLI","level":1,"index":0,"id":"creating-a-drift-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-new-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-new-project-for-your-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-workbench-select-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-an-s3-client/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/customizing-component-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/defining-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric/"},"sections":[{"parentId":null,"name":"Deleting a bias metric by using the dashboard","level":1,"index":0,"id":"deleting-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Deleting a bias metric by using the CLI","level":1,"index":1,"id":"deleting-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-data-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-drift-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-workbench-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-cluster-storage-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-files-in-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-model-using-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-model-version-from-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-on-single-node-openshift-using-kserve-raw-deployment-mode/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-on-the-NVIDIA-NIM-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-on-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-using-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/disabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/disabling-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-a-data-science-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-files-from-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-the-demo-notebooks-from-the-codeflare-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-bias-metric/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-model-metadata-in-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-model-version-metadata-in-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-the-dashboard-configuration-file/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-custom-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-data-science-pipelines-2/"},"sections":[{"parentId":null,"name":"Installing {productname-short} with data science pipelines 2.0","level":1,"index":0,"id":"_installing_productname_short_with_data_science_pipelines_2_0"},{"parentId":null,"name":"Upgrading to data science pipelines 2.0","level":1,"index":1,"id":"_upgrading_to_data_science_pipelines_2_0"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-intel-gaudi-ai-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-nvidia-gpus/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-nvidia-nim-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-trustyai-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/evaluating-large-language-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/executing-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/exporting-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/hiding-information-about-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/hiding-the-default-jupyter-application/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/importing-a-custom-workbench-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/importing-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-odh-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-python-packages-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-the-distributed-workloads-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-the-odh-operator-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-trustyai-service-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-trustyai-service-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-trustyai-service/"},"sections":[{"parentId":null,"name":"Installing the TrustyAI service by using the dashboard","level":1,"index":0,"id":"installing-trustyai-service-using-dashboard_{context}"},{"parentId":null,"name":"Installing the TrustyAI service by using the CLI","level":1,"index":1,"id":"installing-trustyai-service-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/intel-gaudi-ai-accelerator-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/labeling-data-fields/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/listing-available-buckets/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/listing-files-in-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/logging-in/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/making-inference-requests-to-models-deployed-on-single-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/managing-certificates/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/managing-model-registry-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/managing-ray-clusters-from-within-a-jupyter-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/monitoring-the-training-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/next-steps-getting-started/"},"sections":[{"parentId":null,"name":"Additional resources","level":1,"index":0,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/nvidia-gpu-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/optimizing-the-vllm-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-for-getting-started/"},"sections":[{"parentId":null,"name":"Data science workflow","level":1,"index":0,"id":"_data_science_workflow"},{"parentId":null,"name":"About this guide","level":1,"index":1,"id":"_about_this_guide"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-data-science-pipelines-caching/"},"sections":[{"parentId":null,"name":"Caching criteria","level":1,"index":0,"id":"_caching_criteria"},{"parentId":null,"name":"Viewing cached steps in the {productname-short} user interface","level":1,"index":1,"id":"_viewing_cached_steps_in_the_productname_short_user_interface"},{"parentId":null,"name":"Disabling caching for specific tasks or pipelines","level":1,"index":2,"id":"_disabling_caching_for_specific_tasks_or_pipelines"},{"parentId":"_disabling_caching_for_specific_tasks_or_pipelines","name":"Disabling caching for individual tasks","level":2,"index":0,"id":"_disabling_caching_for_individual_tasks"},{"parentId":"_disabling_caching_for_specific_tasks_or_pipelines","name":"Disabling caching for pipelines","level":2,"index":1,"id":"_disabling_caching_for_pipelines"},{"parentId":null,"name":"Verification and troubleshooting","level":1,"index":3,"id":"_verification_and_troubleshooting"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-kueue-resources/"},"sections":[{"parentId":null,"name":"Resource flavor","level":1,"index":0,"id":"_resource_flavor"},{"parentId":null,"name":"Cluster queue","level":1,"index":1,"id":"_cluster_queue"},{"parentId":null,"name":"Local queue","level":1,"index":2,"id":"_local_queue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-model-monitoring/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-model-registries/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-object-storage-endpoints/"},"sections":[{"parentId":null,"name":"MinIO (On-Cluster)","level":1,"index":0,"id":"_minio_on_cluster"},{"parentId":null,"name":"Amazon S3","level":1,"index":1,"id":"_amazon_s3"},{"parentId":null,"name":"Other S3-Compatible Object Stores","level":1,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":null,"name":"Verification and Troubleshooting","level":1,"index":3,"id":"_verification_and_troubleshooting"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-experiments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipelines-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-user-types-and-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/preventing-users-from-adding-applications-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/pushing-project-changes-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/pushing-an-image-to-the-integrated-openshift-image-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/reenabling-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kueue-resource-configurations/"},"sections":[{"parentId":null,"name":"NVIDIA GPUs","level":1,"index":0,"id":"_nvidia_gpus"},{"parentId":"_nvidia_gpus","name":"NVIDIA RTX A400 GPU resource flavor","level":2,"index":0,"id":"_nvidia_rtx_a400_gpu_resource_flavor"},{"parentId":"_nvidia_gpus","name":"NVIDIA RTX A1000 GPU resource flavor","level":2,"index":1,"id":"_nvidia_rtx_a1000_gpu_resource_flavor"},{"parentId":"_nvidia_gpus","name":"NVIDIA RTX A400 GPU cluster queue","level":2,"index":2,"id":"_nvidia_rtx_a400_gpu_cluster_queue"},{"parentId":"_nvidia_gpus","name":"NVIDIA RTX A1000 GPU cluster queue","level":2,"index":3,"id":"_nvidia_rtx_a1000_gpu_cluster_queue"},{"parentId":null,"name":"Additional resources","level":1,"index":1,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-dashboard-configuration-options/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-inference-endpoints/"},"sections":[{"parentId":null,"name":"Caikit TGIS ServingRuntime for KServe","level":1,"index":0,"id":"_caikit_tgis_servingruntime_for_kserve"},{"parentId":null,"name":"Caikit Standalone ServingRuntime for KServe","level":1,"index":1,"id":"_caikit_standalone_servingruntime_for_kserve"},{"parentId":null,"name":"TGIS Standalone ServingRuntime for KServe","level":1,"index":2,"id":"_tgis_standalone_servingruntime_for_kserve"},{"parentId":null,"name":"OpenVINO Model Server","level":1,"index":3,"id":"_openvino_model_server"},{"parentId":null,"name":"vLLM ServingRuntime for KServe","level":1,"index":4,"id":"_vllm_servingruntime_for_kserve"},{"parentId":null,"name":"NVIDIA Triton Inference Server","level":1,"index":5,"id":"_nvidia_triton_inference_server"},{"parentId":null,"name":"Additional resources","level":1,"index":6,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-supported-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-tested-verified-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/registering-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/registering-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-a-ca-bundle-from-a-namespace/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-a-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-disabled-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requesting-a-lime-explanation-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requesting-a-lime-explanation/"},"sections":[{"parentId":null,"name":"Requesting a LIME explanation by using the CLI","level":1,"index":0,"id":"requesting-a-lime-explanation-using-CLI_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requesting-a-shap-explanation-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requesting-a-shap-explanation/"},"sections":[{"parentId":null,"name":"Requesting a SHAP explanation by using the CLI","level":1,"index":0,"id":"requesting-a-shap-explanation-using-CLI_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requirements-for-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/resolving-cuda-oom-errors/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/revoking-user-access-to-jupyter/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-disconnected-env/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-from-ds-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-from-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-the-demo-notebooks-from-the-codeflare-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-the-training-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/s3-prerequisites/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/scheduling-a-pipeline-run-using-a-cron-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/scheduling-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/selecting-admin-and-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/sending-training-data-to-trustyai/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/setting-up-lmeval/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/sharing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/showing-hiding-information-about-enabled-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-a-jupyter-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-a-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-idle-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/storing-data-with-data-science-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/supported-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/supported-drift-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/supported-explainers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-checking-model-fairness/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-deploying-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-introduction/"},"sections":[{"parentId":null,"name":"About the example models","level":1,"index":0,"id":"_about_the_example_models"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-labeling-data-fields/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-reviewing-the-results/"},"sections":[{"parentId":null,"name":"Are the models biased?","level":1,"index":0,"id":"_are_the_models_biased"},{"parentId":null,"name":"How does the production data compare to the training data?","level":1,"index":1,"id":"_how_does_the_production_data_compare_to_the_training_data"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-scheduling-a-fairness-metric-request/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-scheduling-an-identity-metric-request/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-sending-training-data-to-the-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-setting-up-your-environment/"},"sections":[{"parentId":null,"name":"Downloading the tutorial files","level":1,"index":0,"id":"_downloading_the_tutorial_files"},{"parentId":null,"name":"Logging in to the OpenShift cluster from the command line","level":1,"index":1,"id":"_logging_in_to_the_openshift_cluster_from_the_command_line"},{"parentId":null,"name":"Configuring monitoring for the model serving platform","level":1,"index":2,"id":"_configuring_monitoring_for_the_model_serving_platform"},{"parentId":null,"name":"Enabling the TrustyAI component","level":1,"index":3,"id":"enabling-trustyai-component_{context}"},{"parentId":null,"name":"Setting up a project","level":1,"index":4,"id":"_setting_up_a_project"},{"parentId":null,"name":"Authenticating the TrustyAI service","level":1,"index":5,"id":"_authenticating_the_trustyai_service"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-simulating-real-world-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/backing-up-storage-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-in-jupyter-for-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-with-distributed-workloads-for-administrators/"},"sections":[{"parentId":null,"name":"A user&#8217;s Ray cluster is in a suspended state","level":1,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"A user&#8217;s Ray cluster is in a failed state","level":1,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"A user receives a <strong>failed to call webhook</strong> error message for the CodeFlare Operator","level":1,"index":2,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":null,"name":"A user receives a <strong>failed to call webhook</strong> error message for Kueue","level":1,"index":3,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"A user&#8217;s Ray cluster does not start","level":1,"index":4,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":null,"name":"A user receives a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":1,"index":5,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"A user receives a <strong>local_queue provided does not exist</strong> error message","level":1,"index":6,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"A user cannot create a Ray cluster or submit jobs","level":1,"index":7,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"The user&#8217;s pod provisioned by Kueue is terminated before the user&#8217;s image is pulled","level":1,"index":8,"id":"_the_users_pod_provisioned_by_kueue_is_terminated_before_the_users_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-with-distributed-workloads-for-users/"},"sections":[{"parentId":null,"name":"My Ray cluster is in a suspended state","level":1,"index":0,"id":"_my_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"My Ray cluster is in a failed state","level":1,"index":1,"id":"_my_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"I see a <strong>failed to call webhook</strong> error message for the CodeFlare Operator","level":1,"index":2,"id":"_i_see_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":null,"name":"I see a <strong>failed to call webhook</strong> error message for Kueue","level":1,"index":3,"id":"_i_see_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"My Ray cluster doesn&#8217;t start","level":1,"index":4,"id":"_my_ray_cluster_doesnt_start"},{"parentId":null,"name":"I see a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":1,"index":5,"id":"_i_see_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"I see a <strong>local_queue provided does not exist</strong> error message","level":1,"index":6,"id":"_i_see_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"I cannot create a Ray cluster or submit jobs","level":1,"index":7,"id":"_i_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"My pod provisioned by Kueue is terminated before my image is pulled","level":1,"index":8,"id":"_my_pod_provisioned_by_kueue_is_terminated_before_my_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-dspa-component-errors/"},"sections":[{"parentId":null,"name":"Common errors across DSP components","level":1,"index":0,"id":"_common_errors_across_dsp_components"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/understanding-certificates/"},"sections":[{"parentId":null,"name":"How CA bundles are injected","level":1,"index":0,"id":"_how_ca_bundles_are_injected"},{"parentId":null,"name":"How the ConfigMap is managed","level":1,"index":1,"id":"_how_the_configmap_is_managed"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-connected-data-source/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-cluster-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-notebook-server-settings-by-restarting-your-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-the-deployment-properties-of-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-your-project-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/upgrading-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-files-to-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-certificates-with-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Providing a CA bundle only for data science pipelines","level":1,"index":0,"id":"_providing_a_ca_bundle_only_for_data_science_pipelines"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-certificates-with-workbenches/"},"sections":[{"parentId":null,"name":"Creating data science pipelines with Elyra and self-signed certificates","level":1,"index":0,"id":"_creating_data_science_pipelines_with_elyra_and_self_signed_certificates"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-oci-containers-for-model-storage/"},"sections":[{"parentId":null,"name":"Creating an OCI image and storing a model in the container image","level":1,"index":0,"id":"_creating_an_oci_image_and_storing_a_model_in_the_container_image"},{"parentId":null,"name":"Deploying a model stored in an OCI image from a public repository","level":1,"index":1,"id":"_deploying_a_model_stored_in_an_oci_image_from_a_public_repository"},{"parentId":null,"name":"Deploying a model stored in an OCI image from a private repository","level":1,"index":2,"id":"_deploying_a_model_stored_in_an_oci_image_from_a_private_repository"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-active-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-archived-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-connected-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-drift-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-existing-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-http-request-metrics-for-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-kueue-alerts-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-metrics-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-metrics-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-performance-metrics-for-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-performance-metrics-for-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-pipeline-artifacts/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-pipeline-task-executions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-project-metrics-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-python-packages-installed-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-registered-model-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-registered-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-scheduled-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-status-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/working-in-code-server/"},"sections":[{"parentId":null,"name":"Installing extensions with code-server","level":1,"index":0,"id":"_installing_extensions_with_code_server"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/working-with-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/authenticating-trustyai-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-in-jupyter-for-administrators/"},"sections":[{"parentId":null,"name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":1,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":null,"name":"A user&#8217;s notebook server does not start","level":1,"index":1,"id":"_a_users_notebook_server_does_not_start"},{"parentId":null,"name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":1,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}}]},"asciidoc":{"html":"<div id=\"toc\" class=\"toc\">\n<div id=\"toctitle\">Table of Contents</div>\n<ul class=\"sectlevel1\">\n<li><a href=\"#about-model-serving_about-model-serving\">About model serving</a></li>\n<li><a href=\"#serving-small-and-medium-sized-models_model-serving\">Serving small and medium-sized models</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#_configuring_model_servers\">Configuring model servers</a></li>\n<li><a href=\"#_working_with_deployed_models\">Working with deployed models</a></li>\n<li><a href=\"#configuring-monitoring-for-the-multi-model-serving-platform_model-serving\">Configuring monitoring for the multi-model serving platform</a></li>\n<li><a href=\"#viewing-metrics-for-the-multi-model-serving-platform_model-serving\">Viewing model-serving runtime metrics for the multi-model serving platform</a></li>\n<li><a href=\"#_monitoring_model_performance\">Monitoring model performance</a></li>\n</ul>\n</li>\n<li><a href=\"#serving-large-models_serving-large-models\">Serving large models</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#about-the-single-model-serving-platform_serving-large-models\">About the single-model serving platform</a></li>\n<li><a href=\"#about-kserve-deployment-modes_serving-large-models\">About KServe deployment modes</a></li>\n<li><a href=\"#installing-kserve_serving-large-models\">Installing KServe</a></li>\n<li><a href=\"#deploying-models-using-the-single-model-serving-platform_serving-large-models\">Deploying models by using the single-model serving platform</a></li>\n<li><a href=\"#configuring-monitoring-for-the-single-model-serving-platform_serving-large-models\">Configuring monitoring for the single-model serving platform</a></li>\n<li><a href=\"#viewing-metrics-for-the-single-model-serving-platform_serving-large-models\">Viewing model-serving runtime metrics for the single-model serving platform</a></li>\n<li><a href=\"#_monitoring_model_performance_2\">Monitoring model performance</a></li>\n<li><a href=\"#_optimizing_model_serving_runtimes\">Optimizing model-serving runtimes</a></li>\n<li><a href=\"#_performance_tuning_on_the_single_model_serving_platform\">Performance tuning on the single-model serving platform</a></li>\n<li><a href=\"#supported-model-serving-runtimes_serving-large-models\">Supported model-serving runtimes</a></li>\n<li><a href=\"#tested-verified-runtimes_serving-large-models\">Tested and verified model-serving runtimes</a></li>\n<li><a href=\"#inference-endpoints_serving-large-models\">Inference endpoints</a></li>\n<li><a href=\"#about-the-NVIDIA-NIM-model-serving-platform_serving-large-models\">About the NVIDIA NIM model serving platform</a></li>\n</ul>\n</li>\n</ul>\n</div>\n<div class=\"sect1\">\n<h2 id=\"about-model-serving_about-model-serving\">About model serving</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>Serving trained models on Open Data Hub means deploying the models on your OpenShift cluster to test and then integrate them into intelligent applications. Deploying a model makes it available as a service that you can access by using an API. This enables you to return predictions based on data inputs that you provide through API calls. This process is known as model <em>inferencing</em>. When you serve a model on Open Data Hub, the inference endpoints that you can access for the deployed model are shown in the dashboard.</p>\n</div>\n<div class=\"paragraph\">\n<p>Open Data Hub provides the following model serving platforms:</p>\n</div>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">Single-model serving platform</dt>\n<dd>\n<p>For deploying large models such as large language models (LLMs), Open Data Hub includes a <em>single-model serving platform</em> that is based on the <a href=\"https://github.com/kserve/kserve\" target=\"_blank\" rel=\"noopener\">KServe</a> component. Because each model is deployed from its own model server, the single-model serving platform helps you to deploy, monitor, scale, and maintain large models that require increased resources.</p>\n</dd>\n<dt class=\"hdlist1\">Multi-model serving platform</dt>\n<dd>\n<p>For deploying small and medium-sized models, Open Data Hub includes a <em>multi-model serving platform</em> that is based on the <a href=\"https://github.com/kserve/modelmesh\" target=\"_blank\" rel=\"noopener\">ModelMesh</a> component. On the multi-model serving platform, you can deploy multiple models on the same model server. Each of the deployed models shares the server resources. This approach can be advantageous on OpenShift clusters that have finite compute resources or pods.</p>\n</dd>\n</dl>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"serving-small-and-medium-sized-models_model-serving\">Serving small and medium-sized models</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>On the multi-model serving platform, multiple models can be deployed from the same model server and share the server resources.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_configuring_model_servers\">Configuring model servers</h3>\n<div class=\"sect3\">\n<h4 id=\"enabling-the-multi-model-serving-platform_model-serving\">Enabling the multi-model serving platform</h4>\n<div class=\"paragraph _abstract\">\n<p>To use the multi-model serving platform, you must first enable the platform.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub as a user with Open Data Hub administrator privileges.</p>\n</li>\n<li>\n<p>Your cluster administrator has <em>not</em> edited the Open Data Hub dashboard configuration to disable the ability to select the multi-model serving platform, which uses the ModelMesh component. For more information, see <a href=\"https://docs.redhat.com/en/documentation/open_data_hub/2//html/managing_openshift_ai/customizing-the-dashboard#ref-dashboard-configuration-options_dashboard\">Dashboard configuration options</a>.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left menu of the Open Data Hub dashboard, click <strong>Settings</strong>  <strong>Cluster settings</strong>.</p>\n</li>\n<li>\n<p>Locate the <strong>Model serving platforms</strong> section.</p>\n</li>\n<li>\n<p>Select the <strong>Multi-model serving platform</strong> checkbox.</p>\n</li>\n<li>\n<p>Click <strong>Save changes</strong>.</p>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform_model-serving\">Adding a custom model-serving runtime for the multi-model serving platform</h4>\n<div class=\"paragraph\">\n<p>A model-serving runtime adds support for a specified set of model frameworks and the model formats supported by those frameworks. By default, the multi-model serving platform includes the OpenVINO Model Server runtime. You can also add your own custom runtime if the default runtime does not meet your needs, such as supporting a specific model format.</p>\n</div>\n<div class=\"paragraph\">\n<p>As an administrator, you can use the Open Data Hub dashboard to add and enable a custom model-serving runtime. You can then choose the custom runtime when you create a new model server for the multi-model serving platform.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nRed&#160;Hat does not provide support for custom runtimes. You are responsible for ensuring that you are licensed to use any custom runtimes that you add, and for correctly configuring and maintaining them.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist _abstract\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub as a user with Open Data Hub administrator privileges.</p>\n</li>\n<li>\n<p>You are familiar with how to <a href=\"https://opendatahub.io/docs/serving-models/#adding-a-model-server-for-the-multi-model-serving-platform_model-serving\">add a model server to your project</a>. When you have added a custom model-serving runtime, you must configure a new model server to use the runtime.</p>\n</li>\n<li>\n<p>You have reviewed the example runtimes in the <a href=\"https://github.com/kserve/modelmesh-serving/tree/main/config/runtimes\" target=\"_blank\" rel=\"noopener\">kserve/modelmesh-serving</a> repository. You can use these examples as starting points. However, each runtime requires some further modification before you can deploy it in Open Data Hub. The required modifications are described in the following procedure.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nOpen Data Hub includes the OpenVINO Model Server runtime by default. You do not need to add this runtime to Open Data Hub.\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Settings</strong> &gt; <strong>Serving runtimes</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Serving runtimes</strong> page opens and shows the model-serving runtimes that are already installed and enabled.</p>\n</div>\n</li>\n<li>\n<p>To add a custom runtime, choose one of the following options:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>To start with an existing runtime (for example the OpenVINO Model Server runtime), click the action menu (&#8942;) next to the existing runtime and then click <strong>Duplicate</strong>.</p>\n</li>\n<li>\n<p>To add a new custom runtime, click <strong>Add serving runtime</strong>.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>In the <strong>Select the model serving platforms this runtime supports</strong> list, select <strong>Multi-model serving platform</strong>.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nThe multi-model serving platform supports only the REST protocol. Therefore, you cannot change the default value in the <strong>Select the API protocol this runtime supports</strong> list.\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Optional: If you started a new runtime (rather than duplicating an existing one), add your code by choosing one of the following options:</p>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>Upload a YAML file</strong></p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click <strong>Upload files</strong>.</p>\n</li>\n<li>\n<p>In the file browser, select a YAML file on your computer. This file might be the one of the example runtimes that you downloaded from the <a href=\"https://github.com/kserve/modelmesh-serving/tree/main/config/runtimes\" target=\"_blank\" rel=\"noopener\">kserve/modelmesh-serving</a> repository.</p>\n<div class=\"paragraph\">\n<p>The embedded YAML editor opens and shows the contents of the file that you uploaded.</p>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p><strong>Enter YAML code directly in the editor</strong></p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click <strong>Start from scratch</strong>.</p>\n</li>\n<li>\n<p>Enter or paste YAML code directly in the embedded editor. The YAML that you paste might be copied from one of the example runtimes in the <a href=\"https://github.com/kserve/modelmesh-serving/tree/main/config/runtimes\" target=\"_blank\" rel=\"noopener\">kserve/modelmesh-serving</a> repository.</p>\n</li>\n</ol>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</li>\n<li>\n<p>Optional: If you are adding one of the example runtimes in the <a href=\"https://github.com/kserve/modelmesh-serving/tree/main/config/runtimes\" target=\"_blank\" rel=\"noopener\">kserve/modelmesh-serving</a> repository, perform the following modifications:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the YAML editor, locate the <code>kind</code> field for your runtime. Update the value of this field to <code>ServingRuntime</code>.</p>\n</li>\n<li>\n<p>In the <a href=\"https://github.com/kserve/modelmesh-serving/blob/main/config/runtimes/kustomization.yaml\" target=\"_blank\" rel=\"noopener\">kustomization.yaml</a> file in the <a href=\"https://github.com/kserve/modelmesh-serving/tree/main/config/runtimes\" target=\"_blank\" rel=\"noopener\">kserve/modelmesh-serving</a> repository, take note of the <code>newName</code> and <code>newTag</code> values for the runtime that you want to add. You will specify these values in a later step.</p>\n</li>\n<li>\n<p>In the YAML editor for your custom runtime, locate the <code>containers.image</code> field.</p>\n</li>\n<li>\n<p>Update the value of the <code>containers.image</code> field in the format <code>newName:newTag</code>, based on the values that you previously noted in the <a href=\"https://github.com/kserve/modelmesh-serving/blob/main/config/runtimes/kustomization.yaml\" target=\"_blank\" rel=\"noopener\">kustomization.yaml</a> file. Some examples are shown.</p>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">Nvidia Triton Inference Server</dt>\n<dd>\n<div class=\"paragraph\">\n<p><code>image: nvcr.io/nvidia/tritonserver:23.04-py3</code></p>\n</div>\n</dd>\n<dt class=\"hdlist1\">Seldon Python MLServer</dt>\n<dd>\n<div class=\"paragraph\">\n<p><code>image: seldonio/mlserver:1.3.2</code></p>\n</div>\n</dd>\n<dt class=\"hdlist1\">TorchServe</dt>\n<dd>\n<div class=\"paragraph\">\n<p><code>image: pytorch/torchserve:0.7.1-cpu</code></p>\n</div>\n</dd>\n</dl>\n</div>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>In the <code>metadata.name</code> field, ensure that the value of the runtime you are adding is unique (that is, the value doesn&#8217;t match a runtime that you have already added).</p>\n</li>\n<li>\n<p>Optional: To configure a custom display name for the runtime that you are adding, add a <code>metadata.annotations.openshift.io/display-name</code> field and specify a value, as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: serving.kserve.io/v1alpha1\nkind: ServingRuntime\nmetadata:\n  name: mlserver-0.x\n  annotations:\n    openshift.io/display-name: MLServer</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nIf you do not configure a custom display name for your runtime, Open Data Hub shows the value of the <code>metadata.name</code> field.\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Click <strong>Add</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Serving runtimes</strong> page opens and shows the updated list of runtimes that are installed. Observe that the runtime you added is automatically enabled.</p>\n</div>\n</li>\n<li>\n<p>Optional: To edit your custom runtime, click the action menu (&#8942;) and select <strong>Edit</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The custom model-serving runtime that you added is shown in an enabled state on the <strong>Serving runtimes</strong> page.</p>\n</li>\n</ul>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p>To learn how to configure a model server that uses a custom model-serving runtime that you have added, see <a href=\"https://opendatahub.io/docs/serving-models/#adding-a-model-server-for-the-multi-model-serving-platform_model-serving\">Adding a model server to your data science project</a>.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"adding-a-tested-and-verified-model-serving-runtime-for-the-multi-model-serving-platform_model-serving\">Adding a tested and verified model-serving runtime for the multi-model serving platform</h4>\n<div class=\"paragraph\">\n<p>In addition to preinstalled and custom model-serving runtimes, you can also use Red&#160;Hat tested and verified model-serving runtimes such as the <a href=\"https://developer.nvidia.com/triton-inference-server\">NVIDIA Triton Inference Server</a> to support your needs. For more information about Red&#160;Hat tested and verified runtimes, see <a href=\"https://access.redhat.com/articles/7089743\" target=\"_blank\" rel=\"noopener\">Tested and verified runtimes for Open Data Hub</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can use the Open Data Hub dashboard to add and enable the <strong>NVIDIA Triton Inference Server</strong> runtime and then choose the runtime when you create a new model server for the multi-model serving platform.</p>\n</div>\n<div class=\"ulist _abstract\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub as a user with Open Data Hub administrator privileges.</p>\n</li>\n<li>\n<p>You are familiar with how to <a href=\"https://opendatahub.io/docs/serving-models/#adding-a-model-server-for-the-multi-model-serving-platform_model-serving\">add a model server to your project</a>. After you have added a tested and verified model-serving runtime, you must configure a new model server to use the runtime.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Settings</strong> &gt; <strong>Serving runtimes</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Serving runtimes</strong> page opens and shows the model-serving runtimes that are already installed and enabled.</p>\n</div>\n</li>\n<li>\n<p>To add a tested and verified runtime, click <strong>Add serving runtime</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Select the model serving platforms this runtime supports</strong> list, select <strong>Multi-model serving platform</strong>.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nThe multi-model serving platform supports only the REST protocol. Therefore, you cannot change the default value in the <strong>Select the API protocol this runtime supports</strong> list.\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Click <strong>Start from scratch</strong>.</p>\n</li>\n<li>\n<p>Enter or paste the following YAML code directly in the embedded editor.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: serving.kserve.io/v1alpha1\nkind: ServingRuntime\nmetadata:\n  annotations:\n    enable-route: \"true\"\n  name: modelmesh-triton\n  labels:\n    opendatahub.io/dashboard: \"true\"\nspec:\n  annotations:\n    opendatahub.io/modelServingSupport: '[\"multi\"x`x`]'\n    prometheus.kserve.io/path: /metrics\n    prometheus.kserve.io/port: \"8002\"\n  builtInAdapter:\n    env:\n      - name: CONTAINER_MEM_REQ_BYTES\n        value: \"268435456\"\n      - name: USE_EMBEDDED_PULLER\n        value: \"true\"\n    memBufferBytes: 134217728\n    modelLoadingTimeoutMillis: 90000\n    runtimeManagementPort: 8001\n    serverType: triton\n  containers:\n    - args:\n        - -c\n        - 'mkdir -p /models/_triton_models;  chmod 777\n          /models/_triton_models;  exec\n          tritonserver \"--model-repository=/models/_triton_models\" \"--model-control-mode=explicit\" \"--strict-model-config=false\" \"--strict-readiness=false\" \"--allow-http=true\" \"--allow-grpc=true\"  '\n      command:\n        - /bin/sh\n      image: nvcr.io/nvidia/tritonserver:23.05-py3\n      name: triton\n      resources:\n        limits:\n          cpu: \"1\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 2Gi\n  grpcDataEndpoint: port:8001\n  grpcEndpoint: port:8085\n  multiModel: true\n  protocolVersions:\n    - grpc-v2\n    - v2\n  supportedModelFormats:\n    - autoSelect: true\n      name: keras\n      version: \"2\"\n    - autoSelect: true\n      name: onnx\n      version: \"1\"\n    - autoSelect: true\n      name: pytorch\n      version: \"1\"\n    - autoSelect: true\n      name: tensorflow\n      version: \"1\"\n    - autoSelect: true\n      name: tensorflow\n      version: \"2\"</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>In the <code>metadata.name</code> field, make sure that the value of the runtime you are adding does not match a runtime that you have already added).</p>\n</li>\n<li>\n<p>Optional: To use a custom display name for the runtime that you are adding, add a <code>metadata.annotations.openshift.io/display-name</code> field and specify a value, as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: serving.kserve.io/v1alpha1\nkind: ServingRuntime\nmetadata:\n  name: modelmesh-triton\n  annotations:\n    openshift.io/display-name: Triton ServingRuntime</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nIf you do not configure a custom display name for your runtime, Open Data Hub shows the value of the <code>metadata.name</code> field.\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Click <strong>Create</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Serving runtimes</strong> page opens and shows the updated list of runtimes that are installed. Observe that the runtime you added is automatically enabled.</p>\n</div>\n</li>\n<li>\n<p>Optional: To edit the runtime, click the action menu (&#8942;) and select <strong>Edit</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The model-serving runtime that you added is shown in an enabled state on the <strong>Serving runtimes</strong> page.</p>\n</li>\n</ul>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p>To learn how to configure a model server that uses a model-serving runtime that you have added, see <a href=\"https://opendatahub.io/docs/serving-models/#adding-a-model-server-for-the-multi-model-serving-platform_model-serving\">Adding a model server to your data science project</a>.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"adding-a-model-server-for-the-multi-model-serving-platform_model-serving\">Adding a model server for the multi-model serving platform</h4>\n<div class=\"paragraph _abstract\">\n<p>When you have enabled the multi-model serving platform, you must configure a model server to deploy models. If you require extra computing power for use with large datasets, you can assign accelerators to your model server.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you use Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that you can add a model server to.</p>\n</li>\n<li>\n<p>You have enabled the multi-model serving platform.</p>\n</li>\n<li>\n<p>If you want to use a custom model-serving runtime for your model server, you have added and enabled the runtime. See <a href=\"https://opendatahub.io/docs/serving-models/#adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform_model-serving\">Adding a custom model-serving runtime</a>.</p>\n</li>\n<li>\n<p>If you want to use graphics processing units (GPUs) with your model server, you have enabled GPU support. This includes installing the Node Feature Discovery and NVIDIA GPU Operators. For more information, see <a href=\"https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html\" target=\"_blank\" rel=\"noopener\">NVIDIA GPU Operator on Red&#160;Hat OpenShift Container Platform</a> in the NVIDIA documentation.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left menu of the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to configure a model server for.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Models</strong> tab.</p>\n</li>\n<li>\n<p>Perform one of the following actions:</p>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p>If you see a <strong>Multi-model serving platform</strong> tile, click <strong>Add model server</strong> on the tile.</p>\n</li>\n<li>\n<p>If you do not see any tiles, click the <strong>Add model server</strong> button.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The <strong>Add model server</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>In the <strong>Model server name</strong> field, enter a unique name for the model server.</p>\n</li>\n<li>\n<p>From the <strong>Serving runtime</strong> list, select a model-serving runtime that is installed and enabled in your Open Data Hub deployment.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you are using a <em>custom</em> model-serving runtime with your model server and want to use GPUs, you must ensure that your custom runtime supports GPUs and is appropriately configured to use them.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>In the <strong>Number of model replicas to deploy</strong> field, specify a value.</p>\n</li>\n<li>\n<p>From the <strong>Model server size</strong> list, select a value.</p>\n</li>\n<li>\n<p>Optional: If you selected <strong>Custom</strong> in the preceding step, configure the following settings in the <strong>Model server size</strong> section to customize your model server:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>CPUs requested</strong> field, specify the number of CPUs to use with your model server. Use the list beside this field to specify the value in cores or millicores.</p>\n</li>\n<li>\n<p>In the <strong>CPU limit</strong> field, specify the maximum number of CPUs to use with your model server. Use the list beside this field to specify the value in cores or millicores.</p>\n</li>\n<li>\n<p>In the <strong>Memory requested</strong> field, specify the requested memory for the model server in gibibytes (Gi).</p>\n</li>\n<li>\n<p>In the <strong>Memory limit</strong> field, specify the maximum memory limit for the model server in gibibytes (Gi).</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Optional: From the <strong>Accelerator</strong> list, select an accelerator.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>If you selected an accelerator in the preceding step, specify the number of accelerators to use.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Optional: In the <strong>Model route</strong> section, select the <strong>Make deployed models available through an external route</strong> checkbox to make your deployed models available to external clients.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Token authorization</strong> section, select the <strong>Require token authentication</strong> checkbox to require token authentication for your model server. To finish configuring token authentication, perform the following actions:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Service account name</strong> field, enter a service account name for which the token will be generated. The generated token is created and displayed in the <strong>Token secret</strong> field when the model server is configured.</p>\n</li>\n<li>\n<p>To add an additional service account, click <strong>Add a service account</strong> and enter another service account name.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Add</strong>.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>The model server that you configured appears on the <strong>Models</strong> tab for the project, in the <strong>Models and model servers</strong> list.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Optional: To update the model server, click the action menu (<strong>&#8942;</strong>) beside the model server and select <strong>Edit model server</strong>.</p>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deleting-a-model-server_model-serving\">Deleting a model server</h4>\n<div class=\"paragraph _abstract\">\n<p>When you no longer need a model server to host models, you can remove it from your data science project.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nWhen you remove a model server, you also remove the models that are hosted on that model server. As a result, the models are no longer available to applications.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have created a data science project and an associated model server.</p>\n</li>\n<li>\n<p>You have notified the users of the applications that access the models that the models will no longer be available.</p>\n</li>\n<li>\n<p>If you are using Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project from which you want to delete the model server.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Models</strong> tab.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the project whose model server you want to delete and then click <strong>Delete model server</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Delete model server</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter the name of the model server in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete model server</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The model server that you deleted is no longer displayed on the <strong>Models</strong> tab for the project.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_working_with_deployed_models\">Working with deployed models</h3>\n<div class=\"sect3\">\n<h4 id=\"deploying-a-model-using-the-multi-model-serving-platform_model-serving\">Deploying a model by using the multi-model serving platform</h4>\n<div class=\"paragraph _abstract\">\n<p>You can deploy trained models on Open Data Hub to enable you to test and implement them into intelligent applications. Deploying a model makes it available as a service that you can access by using an API. This enables you to return predictions based on data inputs.</p>\n</div>\n<div class=\"paragraph\">\n<p>When you have enabled the multi-model serving platform, you can deploy models on the platform.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have enabled the multi-model serving platform.</p>\n</li>\n<li>\n<p>You have created a data science project and added a model server.</p>\n</li>\n<li>\n<p>You have access to S3-compatible object storage.</p>\n</li>\n<li>\n<p>For the model that you want to deploy, you know the associated folder path in your S3-compatible object storage bucket.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left menu of the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to deploy a model in.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Models</strong> tab.</p>\n</li>\n<li>\n<p>Click <strong>Deploy model</strong>.</p>\n</li>\n<li>\n<p>Configure properties for deploying your model as follows:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Model name</strong> field, enter a unique name for the model that you are deploying.</p>\n</li>\n<li>\n<p>From the <strong>Model framework</strong> list, select a framework for your model.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nThe <strong>Model framework</strong> list shows only the frameworks that are supported by the model-serving runtime that you specified when you configured your model server.\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>To specify the location of the model you want to deploy from S3-compatible object storage, perform one of the following sets of actions:</p>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>To use an existing data connection</strong></p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>Select <strong>Existing data connection</strong>.</p>\n</li>\n<li>\n<p>From the <strong>Name</strong> list, select a data connection that you previously defined.</p>\n</li>\n<li>\n<p>In the <strong>Path</strong> field, enter the folder path that contains the model in your specified data source.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p><strong>To use a new data connection</strong></p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>To define a new data connection that your model can access, select <strong>New data connection</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Name</strong> field, enter a unique name for the data connection.</p>\n</li>\n<li>\n<p>In the <strong>Access key</strong> field, enter the access key ID for the S3-compatible object storage provider.</p>\n</li>\n<li>\n<p>In the <strong>Secret key</strong> field, enter the secret access key for the S3-compatible object storage account that you specified.</p>\n</li>\n<li>\n<p>In the <strong>Endpoint</strong> field, enter the endpoint of your S3-compatible object storage bucket.</p>\n</li>\n<li>\n<p>In the <strong>Region</strong> field, enter the default region of your S3-compatible object storage account.</p>\n</li>\n<li>\n<p>In the <strong>Bucket</strong> field, enter the name of your S3-compatible object storage bucket.</p>\n</li>\n<li>\n<p>In the <strong>Path</strong> field, enter the folder path in your S3-compatible object storage that contains your data file.</p>\n</li>\n</ol>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</li>\n<li>\n<p>Click <strong>Deploy</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Confirm that the deployed model is shown on the <strong>Models</strong> tab for the project, and on the <strong>Model Serving</strong> page of the dashboard with a checkmark in the <strong>Status</strong> column.</p>\n</li>\n</ul>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p>To learn how to monitor your model for bias, see <a href=\"https://opendatahub.io/docs/monitoring-data-science-models\">Monitoring data science models</a>.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"viewing-a-deployed-model_model-serving\">Viewing a deployed model</h4>\n<div class=\"paragraph _abstract\">\n<p>To analyze the results of your work, you can view a list of deployed models on Open Data Hub. You can also view the current statuses of deployed models and their endpoints.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Model Serving</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Deployed models</strong> page opens.</p>\n</div>\n<div class=\"paragraph\">\n<p>For each model, the page shows details such as the model name, the project in which the model is deployed, the model-serving runtime that the model uses, and the deployment status.</p>\n</div>\n</li>\n<li>\n<p>Optional: For a given model, click the link in the <strong>Inference endpoint</strong> column to see the inference endpoints for the deployed model.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>A list of previously deployed data science models is displayed on the <strong>Deployed models</strong> page.</p>\n</li>\n</ul>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p>To learn how to monitor your model for bias, see <a href=\"https://opendatahub.io/docs/monitoring-data-science-models\">Monitoring data science models</a>.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"updating-the-deployment-properties-of-a-deployed-model_model-serving\">Updating the deployment properties of a deployed model</h4>\n<div class=\"paragraph _abstract\">\n<p>You can update the deployment properties of a model that has been deployed previously. This allows you to change the model&#8217;s data connection and name.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have deployed a model on Open Data Hub.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Model Serving</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Deployed models</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the model whose deployment properties you want to update and click <strong>Edit</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Edit model</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Update the deployment properties of the model as follows:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Model name</strong> field, enter a new, unique name for your model.</p>\n</li>\n<li>\n<p>From the <strong>Model servers</strong> list, select a model server for your model.</p>\n</li>\n<li>\n<p>From the <strong>Model framework</strong> list, select a framework for your model.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nThe <strong>Model framework</strong> list shows only the frameworks that are supported by the model-serving runtime that you specified when you configured your model server.\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>To update how you have specified the location of your model, perform one of the following sets of actions:</p>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>If you previously specified an existing data connection</strong></p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Path</strong> field, update the folder path that contains the model in your specified data source.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p><strong>If you previously specified a new data connection</strong></p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Name</strong> field, update a unique name for the data connection.</p>\n</li>\n<li>\n<p>In the <strong>Access key</strong> field, update the access key ID for the S3-compatible object storage provider.</p>\n</li>\n<li>\n<p>In the <strong>Secret key</strong> field, update the secret access key for the S3-compatible object storage account that you specified.</p>\n</li>\n<li>\n<p>In the <strong>Endpoint</strong> field, update the endpoint of your S3-compatible object storage bucket.</p>\n</li>\n<li>\n<p>In the <strong>Region</strong> field, update the default region of your S3-compatible object storage account.</p>\n</li>\n<li>\n<p>In the <strong>Bucket</strong> field, update the name of your S3-compatible object storage bucket.</p>\n</li>\n<li>\n<p>In the <strong>Path</strong> field, update the folder path in your S3-compatible object storage that contains your data file.</p>\n</li>\n</ol>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</li>\n<li>\n<p>Click <strong>Redeploy</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The model whose deployment properties you updated is displayed on the <strong>Model Serving</strong> page of the dashboard.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deleting-a-deployed-model_model-serving\">Deleting a deployed model</h4>\n<div class=\"paragraph _abstract\">\n<p>You can delete models you have previously deployed. This enables you to remove deployed models that are no longer required.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have deployed a model.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Model serving</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Deployed models</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the deployed model that you want to delete and click <strong>Delete</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Delete deployed model</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter the name of the deployed model in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete deployed model</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The model that you deleted is no longer displayed on the <strong>Deployed models</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"configuring-monitoring-for-the-multi-model-serving-platform_model-serving\">Configuring monitoring for the multi-model serving platform</h3>\n<div class=\"paragraph _abstract\">\n<p>The multi-model serving platform includes model and model server metrics for the ModelMesh component. ModelMesh generates its own set of metrics and does not rely on the underlying model-serving runtimes to provide them. The set of metrics that ModelMesh generates includes metrics for model request rates and timings, model loading and unloading rates, times and sizes, internal queuing delays, capacity and usage, cache state, and least recently-used models. For more information, see <a href=\"https://github.com/kserve/modelmesh-serving/blob/main/docs/monitoring.md\" target=\"_blank\" rel=\"noopener\">ModelMesh metrics</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>After you have configured monitoring, you can view metrics for the ModelMesh component.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have cluster administrator privileges for your OpenShift Container Platform cluster.</p>\n</li>\n<li>\n<p>You have downloaded and installed the OpenShift command-line interface (CLI). See <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a>.</p>\n</li>\n<li>\n<p>You are familiar with <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/monitoring/configuring-the-monitoring-stack#creating-user-defined-workload-monitoring-configmap_configuring-the-monitoring-stack\">creating a config map</a> for monitoring a user-defined workflow. You will perform similar steps in this procedure.</p>\n</li>\n<li>\n<p>You are familiar with <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/monitoring/enabling-monitoring-for-user-defined-projects\">enabling monitoring</a> for user-defined projects in OpenShift. You will perform similar steps in this procedure.</p>\n</li>\n<li>\n<p>You have <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/monitoring/enabling-monitoring-for-user-defined-projects#granting-users-permission-to-monitor-user-defined-projects_enabling-monitoring-for-user-defined-projects\">assigned</a> the <code>monitoring-rules-view</code> role to users that will monitor metrics.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In a terminal window, if you are not already logged in to your OpenShift cluster as a cluster administrator, log in to the OpenShift CLI as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc login <em>&lt;openshift_cluster_url&gt;</em> -u <em>&lt;admin_username&gt;</em> -p <em>&lt;password&gt;</em></code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Define a <code>ConfigMap</code> object in a YAML file called <code>uwm-cm-conf.yaml</code> with the following contents:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: user-workload-monitoring-config\n  namespace: openshift-user-workload-monitoring\ndata:\n  config.yaml: |\n    prometheus:\n      logLevel: debug\n      retention: 15d</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The <code>user-workload-monitoring-config</code> object configures the components that monitor user-defined projects.  Observe that the retention time is set to the recommended value of 15 days.</p>\n</div>\n</li>\n<li>\n<p>Apply the configuration to create the <code>user-workload-monitoring-config</code> object.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc apply -f uwm-cm-conf.yaml</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Define another <code>ConfigMap</code> object in a YAML file called <code>uwm-cm-enable.yaml</code> with the following contents:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cluster-monitoring-config\n  namespace: openshift-monitoring\ndata:\n  config.yaml: |\n    enableUserWorkload: true</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The <code>cluster-monitoring-config</code> object enables monitoring for user-defined projects.</p>\n</div>\n</li>\n<li>\n<p>Apply the configuration to create the <code>cluster-monitoring-config</code> object.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc apply -f uwm-cm-enable.yaml</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-metrics-for-the-multi-model-serving-platform_model-serving\">Viewing model-serving runtime metrics for the multi-model serving platform</h3>\n<div class=\"paragraph _abstract\">\n<p>After a cluster administrator has configured monitoring for the multi-model serving platform, non-admin users can use the OpenShift web console to view model-serving runtime metrics for the ModelMesh component.</p>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Log in to the OpenShift Container Platform web console.</p>\n</li>\n<li>\n<p>Switch to the <strong>Developer</strong> perspective.</p>\n</li>\n<li>\n<p>In the left menu, click <strong>Observe</strong>.</p>\n</li>\n<li>\n<p>As described in <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/building_applications/odc-monitoring-project-and-application-metrics-using-developer-perspective#odc-monitoring-your-project-metrics_monitoring-project-and-application-metrics-using-developer-perspective\" target=\"_blank\" rel=\"noopener\">Monitoring your project metrics</a>, use the web console to run queries for <code>modelmesh_*</code> metrics.</p>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_monitoring_model_performance\">Monitoring model performance</h3>\n<div class=\"paragraph\">\n<p>In the multi-model serving platform, you can view performance metrics for all models deployed on a model server and for a specific model that is deployed on the model server.</p>\n</div>\n<div class=\"sect3\">\n<h4 id=\"viewing-performance-metrics-for-model-server_model-serving\">Viewing performance metrics for all models on a model server</h4>\n<div class=\"paragraph _abstract\">\n<p>You can monitor the following metrics for all the models that are deployed on a model server:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>HTTP requests per 5 minutes</strong> - The number of HTTP requests that have failed or succeeded for all models on the server.</p>\n</li>\n<li>\n<p><strong>Average response time (ms)</strong> - For all models on the server, the average time it takes the model server to respond to requests.</p>\n</li>\n<li>\n<p><strong>CPU utilization (%)</strong> - The percentage of the CPU&#8217;s capacity that is currently being used by all models on the server.</p>\n</li>\n<li>\n<p><strong>Memory utilization (%)</strong> - The percentage of the system&#8217;s memory that is currently being used by all models on the server.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>You can specify a time range and a refresh interval for these metrics to help you determine, for example, when the peak usage hours are and how the models are performing at a specified time.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed Open Data Hub.</p>\n</li>\n<li>\n<p>On the OpenShift cluster where Open Data Hub is installed, user workload monitoring is enabled.</p>\n</li>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have deployed models on the multi-model serving platform.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard navigation menu, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that contains the data science models that you want to monitor.</p>\n</li>\n<li>\n<p>In the project details page, click the <strong>Models</strong> tab.</p>\n</li>\n<li>\n<p>In the row for the model server that you are interested in, click the action menu (&#8942;) and then select <strong>View model server metrics</strong>.</p>\n</li>\n<li>\n<p>Optional: On the metrics page for the model server, set the following options:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>Time range</strong> - Specifies how long to track the metrics. You can select one of these values: 1 hour, 24 hours, 7 days, and 30 days.</p>\n</li>\n<li>\n<p><strong>Refresh interval</strong> - Specifies how frequently the graphs on the metrics page are refreshed (to show the latest data). You can select one of these values: 15 seconds, 30 seconds, 1 minute, 5 minutes, 15 minutes, 30 minutes, 1 hour, 2 hours, and 1 day.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Scroll down to view data graphs for HTTP requests per 5 minutes, average response time, CPU utilization, and memory utilization.</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>On the metrics page for the model server, the graphs provide data on performance metrics.</p>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"viewing-http-request-metrics-for-a-deployed-model_model-serving\">Viewing HTTP request metrics for a deployed model</h4>\n<div class=\"paragraph _abstract\">\n<p>You can view a graph that illustrates the HTTP requests that have failed or succeeded for a specific model that is deployed on the multi-model serving platform.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed Open Data Hub.</p>\n</li>\n<li>\n<p>On the OpenShift cluster where Open Data Hub is installed, user workload monitoring is enabled.</p>\n</li>\n<li>\n<p>The following dashboard configuration options are set to the default values as shown:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>disablePerformanceMetrics:false\ndisableKServeMetrics:false</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>For more information, see <a href=\"https://opendatahub.io/docs/managing-odh/#ref-dashboard-configuration-options_dashboard\">Dashboard configuration options</a>.</p>\n</div>\n</li>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have deployed models on the multi-model serving platform.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard navigation menu, select <strong>Model Serving</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Deployed models</strong> page, select the model that you are interested in.</p>\n</li>\n<li>\n<p>Optional: On the <strong>Endpoint performance</strong> tab, set the following options:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>Time range</strong> - Specifies how long to track the metrics. You can select one of these values: 1 hour, 24 hours, 7 days, and 30 days.</p>\n</li>\n<li>\n<p><strong>Refresh interval</strong> - Specifies how frequently the graphs on the metrics page are refreshed (to show the latest data). You can select one of these values: 15 seconds, 30 seconds, 1 minute, 5 minutes, 15 minutes, 30 minutes, 1 hour, 2 hours, and 1 day.</p>\n</li>\n</ul>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>The <strong>Endpoint performance</strong> tab shows a graph of the HTTP metrics for the model.</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"serving-large-models_serving-large-models\">Serving large models</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>For deploying large models such as large language models (LLMs), Open Data Hub includes a single-model serving platform that is based on the KServe component. Because each model is deployed from its own model server, the single-model serving platform helps you to deploy, monitor, scale, and maintain large models that require more resources.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"about-the-single-model-serving-platform_serving-large-models\">About the single-model serving platform</h3>\n<div class=\"paragraph _abstract\">\n<p>The single-model serving platform consists of the following components:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><a href=\"https://github.com/opendatahub-io/kserve\" target=\"_blank\" rel=\"noopener\">KServe</a>: A Kubernetes custom resource definition (CRD) that orchestrates model serving for all types of models. It includes model-serving runtimes that implement the loading of given types of model servers. KServe handles the lifecycle of the deployment object, storage access, and networking setup.</p>\n</li>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/red_hat_openshift_serverless/1.33/html/about_openshift_serverless/index\" target=\"_blank\" rel=\"noopener\">Red&#160;Hat OpenShift Serverless</a>: A cloud-native development model that allows for serverless deployments of models. OpenShift Serverless is based on the open source <a href=\"https://knative.dev/docs/\" target=\"_blank\" rel=\"noopener\">Knative</a> project.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>To install the single-model serving platform, you have the following options:</p>\n</div>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">Automated installation</dt>\n<dd>\n<p>If you have not already created a <code>ServiceMeshControlPlane</code> or <code>KNativeServing</code> resource on your OpenShift cluster, you can configure the Open Data Hub Operator to install KServe and configure its dependencies.</p>\n</dd>\n<dt class=\"hdlist1\">Manual installation</dt>\n<dd>\n<p>If you have already created a <code>ServiceMeshControlPlane</code> or <code>KNativeServing</code> resource on your OpenShift cluster, you <em>cannot</em> configure the Open Data Hub Operator to install KServe and configure its dependencies. In this situation, you must install KServe manually.</p>\n</dd>\n</dl>\n</div>\n<div class=\"paragraph\">\n<p>When you have installed KServe, you can use the Open Data Hub dashboard to deploy models using preinstalled or custom model-serving runtimes.</p>\n</div>\n<div class=\"paragraph\">\n<p>Open Data Hub includes preinstalled runtimes for KServe. For more information, see <a href=\"https://opendatahub.io/docs/serving-models/#ref-supported-runtimes_serving-large-models\">Supported model-serving runtimes</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can also configure monitoring for the single-model serving platform and use Prometheus to scrape the available metrics.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"about-kserve-deployment-modes_serving-large-models\">About KServe deployment modes</h3>\n<div class=\"paragraph\">\n<p>By default, you can deploy models on the single-model serving platform with KServe by using <a href=\"https://docs.redhat.com/en/documentation/red_hat_openshift_serverless/1.33/html/about_openshift_serverless/index\" target=\"_blank\" rel=\"noopener\">Red&#160;Hat OpenShift Serverless</a>, which is a cloud-native development model that allows for serverless deployments of models. OpenShift Serverless is based on the open source <a href=\"https://knative.dev/docs/\" target=\"_blank\" rel=\"noopener\">Knative</a> project. In addition, serverless mode is dependent on the Red&#160;Hat OpenShift Serverless Operator.</p>\n</div>\n<div class=\"paragraph\">\n<p>Alternatively, you can use raw deployment mode, which is not dependent on the Red&#160;Hat OpenShift Serverless Operator. With raw deployment mode, you can deploy models with Kubernetes resources, such as <code>Deployment</code>, <code>Service</code>, <code>Ingress</code>, and <code>Horizontal Pod Autoscaler</code>.</p>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Deploying a machine learning model using KServe raw deployment mode is a Limited Availability feature. Limited Availability means that you can install and receive support for the feature only with specific approval from the Red&#160;Hat AI Business Unit. Without such approval, the feature is unsupported. In addition, this feature is only supported on Self-Managed deployments of single node OpenShift.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>There are both advantages and disadvantages to using each of these deployment modes:</p>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_serverless_mode\">Serverless mode</h4>\n<div class=\"paragraph\">\n<p>Advantages:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Enables autoscaling based on request volume:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Resources scale up automatically when receiving incoming requests.</p>\n</li>\n<li>\n<p>Optimizes resource usage and maintains performance during peak times.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Supports scale down to and from zero using Knative:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Allows resources to scale down completely when there are no incoming requests.</p>\n</li>\n<li>\n<p>Saves costs by not running idle resources.</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>Disadvantages:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Has customization limitations:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Serverless is limited to Knative, such as when mounting multiple volumes.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Dependency on Knative for scaling:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Introduces additional complexity in setup and management compared to traditional scaling methods.</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_raw_deployment_mode\">Raw deployment mode</h4>\n<div class=\"paragraph\">\n<p>Advantages:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Enables deployment with Kubernetes resources, such as <code>Deployment</code>, <code>Service</code>, <code>Ingress</code>, and <code>Horizontal Pod Autoscaler</code>:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Provides full control over Kubernetes resources, allowing for detailed customization and configuration of deployment settings.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Unlocks Knative limitations, such as being unable to mount multiple volumes:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Beneficial for applications requiring complex configurations or multiple storage mounts.</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>Disadvantages:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Does not support automatic scaling:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Does not support automatic scaling down to zero resources when idle.</p>\n</li>\n<li>\n<p>Might result in higher costs during periods of low traffic.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Requires manual management of scaling.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"installing-kserve_serving-large-models\">Installing KServe</h3>\n<div class=\"paragraph _abstract\">\n<p>To learn how to perform both automated and manual installation of KServe, see <a href=\"https://github.com/opendatahub-io/caikit-tgis-serving/tree/main/docs#installation\" target=\"_blank\" rel=\"noopener\">Installation</a> in the <a href=\"https://github.com/opendatahub-io/caikit-tgis-serving/tree/main/docs#installation\" target=\"_blank\" rel=\"noopener\">caikit-tgis-serving</a> repository.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"deploying-models-using-the-single-model-serving-platform_serving-large-models\">Deploying models by using the single-model serving platform</h3>\n<div class=\"paragraph _abstract\">\n<p>On the single-model serving platform, each model is deployed on its own model server. This helps you to deploy, monitor, scale, and maintain large models that require increased resources.</p>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you want to use the single-model serving platform to deploy a model from S3-compatible storage that uses a self-signed SSL certificate, you must install a certificate authority (CA) bundle on your OpenShift cluster. For more information, see <a href=\"https://opendatahub.io/docs/installing-open-data-hub/#understanding-certificates_certs\">Understanding certificates in Open Data Hub</a>.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"sect3\">\n<h4 id=\"enabling-the-single-model-serving-platform_serving-large-models\">Enabling the single-model serving platform</h4>\n<div class=\"paragraph _abstract\">\n<p>When you have installed KServe, you can use the Open Data Hub dashboard to enable the single-model serving platform. You can also use the dashboard to enable model-serving runtimes for the platform.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub as a user with Open Data Hub administrator privileges.</p>\n</li>\n<li>\n<p>You have installed KServe.</p>\n</li>\n<li>\n<p>Your cluster administrator has <em>not</em> edited the Open Data Hub dashboard configuration to disable the ability to select the single-model serving platform, which uses the KServe component. For more information, see <a href=\"https://docs.redhat.com/en/documentation/open_data_hub/2//html/managing_openshift_ai/customizing-the-dashboard#ref-dashboard-configuration-options_dashboard\">Dashboard configuration options</a>.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Enable the single-model serving platform as follows:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the left menu, click <strong>Settings</strong> &#8594; <strong>Cluster settings</strong>.</p>\n</li>\n<li>\n<p>Locate the <strong>Model serving platforms</strong> section.</p>\n</li>\n<li>\n<p>To enable the single-model serving platform for projects, select the <strong>Single-model serving platform</strong> checkbox.</p>\n</li>\n<li>\n<p>Click <strong>Save changes</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Enable preinstalled runtimes for the single-model serving platform as follows:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the left menu of the Open Data Hub dashboard, click <strong>Settings</strong> &#8594; <strong>Serving runtimes</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Serving runtimes</strong> page shows preinstalled runtimes and any custom runtimes that you have added.</p>\n</div>\n<div class=\"paragraph\">\n<p>For more information about preinstalled runtimes, see <a href=\"https://opendatahub.io/docs/serving-models/#ref-supported-runtimes_serving-large-models\">Supported runtimes</a>.</p>\n</div>\n</li>\n<li>\n<p>Set the runtime that you want to use to <strong>Enabled</strong>.</p>\n<div class=\"paragraph\">\n<p>The single-model serving platform is now available for model deployments.</p>\n</div>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models\">Adding a custom model-serving runtime for the single-model serving platform</h4>\n<div class=\"paragraph\">\n<p>A model-serving runtime adds support for a specified set of model frameworks and the model formats supported by those frameworks. You can use the <a href=\"https://opendatahub.io/docs/serving-models/#about-the-single-model-serving-platform_serving-large-models\">pre-installed runtimes</a> that are included with Open Data Hub. You can also add your own custom runtimes if the default runtimes do not meet your needs. For example, if the TGIS runtime does not support a model format that is supported by <a href=\"https://huggingface.co/docs/text-generation-inference/supported_models\" target=\"_blank\" rel=\"noopener\">Hugging Face Text Generation Inference (TGI)</a>, you can create a custom runtime to add support for the model.</p>\n</div>\n<div class=\"paragraph\">\n<p>As an administrator, you can use the Open Data Hub interface to add and enable a custom model-serving runtime. You can then choose the custom runtime when you deploy a model on the single-model serving platform.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nRed&#160;Hat does not provide support for custom runtimes. You are responsible for ensuring that you are licensed to use any custom runtimes that you add, and for correctly configuring and maintaining them.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist _abstract\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub as a user with Open Data Hub administrator privileges.</p>\n</li>\n<li>\n<p>You have built your custom runtime and added the image to a container image repository such as <a href=\"https://quay.io\" target=\"_blank\" rel=\"noopener\">Quay</a>.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Settings</strong> &gt; <strong>Serving runtimes</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Serving runtimes</strong> page opens and shows the model-serving runtimes that are already installed and enabled.</p>\n</div>\n</li>\n<li>\n<p>To add a custom runtime, choose one of the following options:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>To start with an existing runtime (for example,\n<strong>TGIS Standalone ServingRuntime for KServe</strong>), click the action menu (&#8942;) next to the existing runtime and then click <strong>Duplicate</strong>.</p>\n</li>\n<li>\n<p>To add a new custom runtime, click <strong>Add serving runtime</strong>.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>In the <strong>Select the model serving platforms this runtime supports</strong> list, select <strong>Single-model serving platform</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Select the API protocol this runtime supports</strong> list, select <strong>REST</strong> or <strong>gRPC</strong>.</p>\n</li>\n<li>\n<p>Optional: If you started a new runtime (rather than duplicating an existing one), add your code by choosing one of the following options:</p>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>Upload a YAML file</strong></p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click <strong>Upload files</strong>.</p>\n</li>\n<li>\n<p>In the file browser, select a YAML file on your computer.</p>\n<div class=\"paragraph\">\n<p>The embedded YAML editor opens and shows the contents of the file that you uploaded.</p>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p><strong>Enter YAML code directly in the editor</strong></p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click <strong>Start from scratch</strong>.</p>\n</li>\n<li>\n<p>Enter or paste YAML code directly in the embedded editor.</p>\n</li>\n</ol>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nIn many cases, creating a custom runtime will require adding new or custom parameters to the <code>env</code> section of the <code>ServingRuntime</code> specification.\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Click <strong>Add</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Serving runtimes</strong> page opens and shows the updated list of runtimes that are installed. Observe that the custom runtime that you added is automatically enabled. The API protocol that you specified when creating the runtime is shown.</p>\n</div>\n</li>\n<li>\n<p>Optional: To edit your custom runtime, click the action menu (&#8942;) and select <strong>Edit</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The custom model-serving runtime that you added is shown in an enabled state on the <strong>Serving runtimes</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"adding-a-tested-and-verified-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models\">Adding a tested and verified model-serving runtime for the single-model serving platform</h4>\n<div class=\"paragraph\">\n<p>In addition to preinstalled and custom model-serving runtimes, you can also use Red&#160;Hat tested and verified model-serving runtimes such as the <strong>NVIDIA Triton Inference Server</strong> to support your needs. For more information about Red&#160;Hat tested and verified runtimes, see <a href=\"https://access.redhat.com/articles/7089743\" target=\"_blank\" rel=\"noopener\">Tested and verified runtimes for Open Data Hub</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can use the Open Data Hub dashboard to add and enable the <strong>NVIDIA Triton Inference Server</strong> runtime for the single-model serving platform. You can then choose the runtime when you deploy a model on the single-model serving platform.</p>\n</div>\n<div class=\"ulist _abstract\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub as a user with Open Data Hub administrator privileges.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Settings</strong> &gt; <strong>Serving runtimes</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Serving runtimes</strong> page opens and shows the model-serving runtimes that are already installed and enabled.</p>\n</div>\n</li>\n<li>\n<p>Click <strong>Add serving runtime</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Select the model serving platforms this runtime supports</strong> list, select <strong>Single-model serving platform</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Select the API protocol this runtime supports</strong> list, select <strong>REST</strong> or <strong>gRPC</strong>.</p>\n</li>\n<li>\n<p>Click <strong>Start from scratch</strong>.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>If you selected the <strong>REST</strong> API protocol, enter or paste the following YAML code directly in the embedded editor.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: serving.kserve.io/v1alpha1\nkind: ServingRuntime\nmetadata:\n  name: triton-kserve-rest\n  labels:\n    opendatahub.io/dashboard: \"true\"\nspec:\n  annotations:\n    prometheus.kserve.io/path: /metrics\n    prometheus.kserve.io/port: \"8002\"\n  containers:\n    - args:\n        - tritonserver\n        - --model-store=/mnt/models\n        - --grpc-port=9000\n        - --http-port=8080\n        - --allow-grpc=true\n        - --allow-http=true\n      image: nvcr.io/nvidia/tritonserver:23.05-py3\n      name: kserve-container\n      resources:\n        limits:\n          cpu: \"1\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 2Gi\n      ports:\n        - containerPort: 8080\n          protocol: TCP\n  protocolVersions:\n    - v2\n    - grpc-v2\n  supportedModelFormats:\n    - autoSelect: true\n      name: tensorflow\n      version: \"1\"\n    - autoSelect: true\n      name: tensorflow\n      version: \"2\"\n    - autoSelect: true\n      name: onnx\n      version: \"1\"\n    - name: pytorch\n      version: \"1\"\n    - autoSelect: true\n      name: keras\n      version: \"2\"</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>If you selected the <strong>gRPC</strong> API protocol, enter or paste the following YAML code directly in the embedded editor.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: serving.kserve.io/v1alpha1\nkind: ServingRuntime\nmetadata:\n  name: triton-kserve-grpc\n  labels:\n    opendatahub.io/dashboard: \"true\"\nspec:\n  annotations:\n    prometheus.kserve.io/path: /metrics\n    prometheus.kserve.io/port: \"8002\"\n  containers:\n    - args:\n        - tritonserver\n        - --model-store=/mnt/models\n        - --grpc-port=9000\n        - --http-port=8080\n        - --allow-grpc=true\n        - --allow-http=true\n      image: nvcr.io/nvidia/tritonserver:23.05-py3\n      name: kserve-container\n      ports:\n        - containerPort: 9000\n          name: h2c\n          protocol: TCP\n      volumeMounts:\n        - mountPath: /dev/shm\n          name: shm\n      resources:\n        limits:\n          cpu: \"1\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 2Gi\n  protocolVersions:\n    - v2\n    - grpc-v2\n  supportedModelFormats:\n    - autoSelect: true\n      name: keras\n      version: \"2\"\n    - autoSelect: true\n      name: tensorflow\n      version: \"1\"\n    - autoSelect: true\n      name: tensorflow\n      version: \"2\"\n    - autoSelect: true\n      name: onnx\n      version: \"1\"\n    - name: pytorch\n      version: \"1\"\nvolumes:\n  - emptyDir: null\n    medium: Memory\n    sizeLimit: 2Gi\n    name: shm</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>In the <code>metadata.name</code> field, make sure that the value of the runtime you are adding does not match a runtime that you have already added).</p>\n</li>\n<li>\n<p>Optional: To use a custom display name for the runtime that you are adding, add a <code>metadata.annotations.openshift.io/display-name</code> field and specify a value, as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: serving.kserve.io/v1alpha1\nkind: ServingRuntime\nmetadata:\n  name: kserve-triton\n  annotations:\n    openshift.io/display-name: Triton ServingRuntime</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nIf you do not configure a custom display name for your runtime, Open Data Hub shows the value of the <code>metadata.name</code> field.\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Click <strong>Create</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Serving runtimes</strong> page opens and shows the updated list of runtimes that are installed. Observe that the runtime that you added is automatically enabled. The API protocol that you specified when creating the runtime is shown.</p>\n</div>\n</li>\n<li>\n<p>Optional: To edit the runtime, click the action menu (&#8942;) and select <strong>Edit</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The model-serving runtime that you added is shown in an enabled state on the <strong>Serving runtimes</strong> page.</p>\n</li>\n</ul>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"odhdocshome}/serving_models/serving-models#tested-and-verified-model-serving-runtimes_serving-large-models\">Tested and verified model-serving runtimes</a></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deploying-models-on-the-single-model-serving-platform_serving-large-models\">Deploying models on the single-model serving platform</h4>\n<div class=\"paragraph _abstract\">\n<p>When you have enabled the single-model serving platform, you can enable a pre-installed or custom model-serving runtime and start to deploy models on the platform.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<a href=\"https://github.com/IBM/text-generation-inference\" target=\"_blank\" rel=\"noopener\">Text Generation Inference Server (TGIS)</a> is based on an early fork of <a href=\"https://github.com/huggingface/text-generation-inference\" target=\"_blank\" rel=\"noopener\">Hugging Face TGI</a>. Red Hat will continue to develop the standalone TGIS runtime to support TGI models. If a model does not work in the current version of Open Data Hub, support might be added in a future version. In the meantime, you can also add your own, custom runtime to support a TGI model. For more information, see <a href=\"https://opendatahub.io/docs/serving-models/#adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models\">Adding a custom model-serving runtime for the single-model serving platform</a>.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have installed KServe.</p>\n</li>\n<li>\n<p>You have enabled the single-model serving platform.\nTo enable token authorization and external model routes for deployed models, you have added Authorino as an authorization provider.</p>\n</li>\n<li>\n<p>You have created a data science project.</p>\n</li>\n<li>\n<p>You have access to S3-compatible object storage.</p>\n</li>\n<li>\n<p>For the model that you want to deploy, you know the associated folder path in your S3-compatible object storage bucket.</p>\n</li>\n<li>\n<p>To use the Caikit-TGIS runtime, you have converted your model to Caikit format. For an example, see <a href=\"https://github.com/opendatahub-io/caikit-tgis-serving/blob/main/demo/kserve/built-tip.md#bootstrap-process\" target=\"_blank\" rel=\"noopener\">Converting Hugging Face Hub models to Caikit format</a> in the <a href=\"https://github.com/opendatahub-io/caikit-tgis-serving/tree/main\" target=\"_blank\" rel=\"noopener\">caikit-tgis-serving</a> repository.</p>\n</li>\n<li>\n<p>To use the vLLM runtime or use graphics processing units (GPUs) with your model server, you have enabled GPU support. This includes installing the Node Feature Discovery and NVIDIA GPU Operators. For more information, see <a href=\"https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html\" target=\"_blank\" rel=\"noopener\">NVIDIA GPU Operator on Red&#160;Hat OpenShift Container Platform</a> in the NVIDIA documentation.</p>\n</li>\n<li>\n<p>To deploy RHEL AI models:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>You have enabled the vLLM runtime.</p>\n</li>\n<li>\n<p>You have downloaded the model from the Red&#160;Hat container registry and uploaded it to S3-compatible object storage.</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left menu, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to deploy a model in.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Models</strong> tab.</p>\n</li>\n<li>\n<p>Perform one of the following actions:</p>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p>If you see a <strong>Single-model serving platform</strong> tile, click <strong>Deploy model</strong> on the tile.</p>\n</li>\n<li>\n<p>If you do not see any tiles, click the <strong>Deploy model</strong> button.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The <strong>Deploy model</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>In the <strong>Model deployment name</strong> field, enter a unique name for the model that you are deploying.</p>\n</li>\n<li>\n<p>In the <strong>Serving runtime</strong> field, select an enabled runtime.</p>\n</li>\n<li>\n<p>From the <strong>Model framework (name - version)</strong> list, select a value.</p>\n</li>\n<li>\n<p>In the <strong>Number of model server replicas to deploy</strong> field, specify a value.</p>\n</li>\n<li>\n<p>From the <strong>Model server size</strong> list, select a value.</p>\n</li>\n<li>\n<p>The following options are only available if you have enabled GPU support on your cluster and created an accelerator profile:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>From the <strong>Accelerator</strong> list, select an accelerator.</p>\n</li>\n<li>\n<p>If you selected an accelerator in the preceding step, specify the number of accelerators to use in the <strong>Number of accelerators</strong> field.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Optional: In the <strong>Model route</strong> section, select the <strong>Make deployed models available through an external route</strong> checkbox to make your deployed models available to external clients.</p>\n</li>\n<li>\n<p>To require token authorization for inference requests to the deployed model, perform the following actions:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Select <strong>Require token authorization</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Service account name</strong> field, enter the service account name that the token will be generated for.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>To specify the location of your model, perform one of the following sets of actions:</p>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>To use an existing data connection</strong></p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Select <strong>Existing data connection</strong>.</p>\n</li>\n<li>\n<p>From the <strong>Name</strong> list, select a data connection that you previously defined.</p>\n</li>\n<li>\n<p>In the <strong>Path</strong> field, enter the folder path that contains the model in your specified data source.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p><strong>To use a new data connection</strong></p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>To define a new data connection that your model can access, select <strong>New data connection</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Name</strong> field, enter a unique name for the data connection.</p>\n</li>\n<li>\n<p>In the <strong>Access key</strong> field, enter the access key ID for your S3-compatible object storage provider.</p>\n</li>\n<li>\n<p>In the <strong>Secret key</strong> field, enter the secret access key for the S3-compatible object storage account that you specified.</p>\n</li>\n<li>\n<p>In the <strong>Endpoint</strong> field, enter the endpoint of your S3-compatible object storage bucket.</p>\n</li>\n<li>\n<p>In the <strong>Region</strong> field, enter the default region of your S3-compatible object storage account.</p>\n</li>\n<li>\n<p>In the <strong>Bucket</strong> field, enter the name of your S3-compatible object storage bucket.</p>\n</li>\n<li>\n<p>In the <strong>Path</strong> field, enter the folder path in your S3-compatible object storage that contains your data file.</p>\n</li>\n</ol>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</li>\n<li>\n<p>Click <strong>Deploy</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Confirm that the deployed model is shown on the <strong>Models</strong> tab for the project, and on the <strong>Model Serving</strong> page of the dashboard with a checkmark in the <strong>Status</strong> column.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"using-oci-containers-for-model-storage_serving-large-models\">Using OCI containers for model storage</h4>\n<div class=\"paragraph _abstract\">\n<p>As an alternative to storing a model in an S3 bucket or URI, you can upload models to OCI containers. Using OCI containers for model storage can help you:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Reduce startup times by avoiding downloading the same model multiple times.</p>\n</li>\n<li>\n<p>Reduce disk space usage by reducing the number of models downloaded locally.</p>\n</li>\n<li>\n<p>Improve model performance by allowing pre-fetched images.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>This guide shows you how to manually deploy a MobileNet v2-7 model in an ONNX format, stored in an OCI image on an OpenVINO model server.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have a model in the ONNX format.</p>\n</li>\n</ul>\n</div>\n<div class=\"sect4\">\n<h5 id=\"_creating_an_oci_image_and_storing_a_model_in_the_container_image\">Creating an OCI image and storing a model in the container image</h5>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From your local machine, create a temporary directory to store both the downloaded model and support files to create the OCI image:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>cd $(mktemp -d)</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Create a <code>models</code> folder inside the temporary directory and download your model:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>mkdir -p models/1\n\nDOWNLOAD_URL=https://github.com/onnx/models/raw/main/validated/vision/classification/mobilenet/model/mobilenetv2-7.onnx\ncurl -L $DOWNLOAD_URL -O --output-dir models/1/</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>The subdirectory <code>1</code> is used because OpenVINO requires numbered subdirectories for model versioning. If you are not using OpenVINO, you do not need to create the <code>1</code> subdirectory to use OCI container images.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Create a Docker file named <code>Containerfile</code> with the following contents:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>FROM registry.access.redhat.com/ubi9/ubi-micro:latest\nCOPY --chown=0:0 models /models\nRUN chmod -R a=rX /models\n\n# nobody user\nUSER 65534</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p>In this example, <code>ubi9-micro</code> is used as a base container image. You cannot use empty images that do not provide a shell, such as <code>scratch</code>, because KServe uses the shell to ensure the model files are accessible to the model server.</p>\n</li>\n<li>\n<p>Ownership of the copied model files and read permissions are granted to the <code>root</code> group. OpenShift runs containers with a random user ID and the <code>root</code> group ID. Changing ownership of the group ensures that the model server can access them.</p>\n</li>\n</ul>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Confirm that the models follow the directory structure shown using the <code>tree</code> command:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>tree\n\n.\n Containerfile\n models\n     1\n         mobilenetv2-7.onnx</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Create the OCI container image with Podman, and upload it to a registry. For\nexample, using Quay as the registry:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>podman build --format=oci -t quay.io/&lt;user_name&gt;/&lt;repository_name&gt;:&lt;tag_name&gt; .\npodman push quay.io/&lt;user_name&gt;/&lt;repository_name&gt;:&lt;tag_name&gt;</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If your repository is private, ensure you are authenticated to the registry before uploading your container image.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect4\">\n<h5 id=\"_deploying_a_model_stored_in_an_oci_image_from_a_public_repository\">Deploying a model stored in an OCI image from a public repository</h5>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>By default in KServe, models are exposed outside the cluster and not protected with authorization.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>Create a namespace to deploy the model:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc new-project oci-model-example</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Use the Open Data Hub project <code>kserve-ovms</code> template to create a <code>ServingRuntime</code> resource and configure the OpenVINO model server in the new namespace:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc process -n opendatahub -o yaml kserve-ovms | oc apply -f -</code></pre>\n</div>\n</div>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Verify that the <code>ServingRuntime</code> has been created with the <code>kserve-ovms</code> name:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc get servingruntimes\n\nNAME          DISABLED   MODELTYPE     CONTAINERS         AGE\nkserve-ovms              openvino_ir   kserve-container   1m</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</div>\n</div>\n</li>\n<li>\n<p>Create an <code>InferenceService</code> YAML resource with the following values:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: sample-isvc-using-oci\nspec:\n  predictor:\n    model:\n      runtime: kserve-ovms # Ensure this matches the name of the ServingRuntime resource\n      modelFormat:\n        name: onnx\n      storageUri: oci://quay.io/&lt;user_name&gt;/&lt;repository_name&gt;:&lt;tag_name&gt;</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>The <code>ServingRuntime</code> and <code>InferenceService</code> configurations do not set any resource limits.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>After you create the <code>InferenceService</code> resource, KServe deploys the model stored in the OCI image referred to by the <code>storageUri</code> field. Check the status of the deployment with the following command:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc get inferenceservice\n\nNAME                    URL                                                       READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                     AGE\nsample-isvc-using-oci   https://sample-isvc-using-oci-oci-model-example.example   True           100                              sample-isvc-using-oci-predictor-00001   1m</code></pre>\n</div>\n</div>\n</div>\n<div class=\"sect4\">\n<h5 id=\"_deploying_a_model_stored_in_an_oci_image_from_a_private_repository\">Deploying a model stored in an OCI image from a private repository</h5>\n<div class=\"paragraph\">\n<p>To deploy a model stored from a private OCI repository, you must configure an image pull secret. For more information about creating an image pull secret, see <a href=\"https://docs.openshift.com/container-platform/latest/openshift_images/managing_images/using-image-pull-secrets.html\" target=\"_blank\" rel=\"noopener\">Using image pull secrets</a>.</p>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>Follow the steps in the previous section for deploying a model. However, when creating the <code>InferenceService</code> in step 3, specify your pull secret in the <code>spec.predictor.imagePullSecrets</code> field:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: sample-isvc-using-private-oci\nspec:\n  predictor:\n    model:\n      runtime: kserve-ovms\n      modelFormat:\n        name: onnx\n      storageUri: oci://quay.io/&lt;user_name&gt;/&lt;repository_name&gt;:&lt;tag_name&gt;\n    imagePullSecrets: # Specify image pull secrets to use for fetching container images (including OCI model images)\n    - name: &lt;pull-secret-name&gt;</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://kserve.github.io/website/latest/modelserving/storage/oci/\">Serving models with OCI images</a></p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"accessing-inference-endpoint-for-deployed-model_serving-large-models\">Accessing the inference endpoint for a deployed model</h4>\n<div class=\"paragraph _abstract\">\n<p>To make inference requests to your deployed model, you must know how to access the inference endpoint that is available.</p>\n</div>\n<div class=\"paragraph\">\n<p>For a list of paths to use with the supported runtimes and example commands, see <a href=\"https://opendatahub.io/docs/serving-models/#inference-endpoints_serving-large-models\">Inference endpoints</a>.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have deployed a model by using the single-model serving platform.</p>\n</li>\n<li>\n<p>If you enabled token authorization for your deployed model, you have the associated token value.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Model Serving</strong>.</p>\n<div class=\"paragraph\">\n<p>The inference endpoint for the model is shown in the <strong>Inference endpoint</strong> field.</p>\n</div>\n</li>\n<li>\n<p>Depending on what action you want to perform with the model (and if the model supports that action), copy the inference endpoint and then add a path to the end of the URL.</p>\n</li>\n<li>\n<p>Use the endpoint to make API requests to your deployed model.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://github.com/IBM/text-generation-inference\" target=\"_blank\" rel=\"noopener\">Text Generation Inference Server (TGIS)</a></p>\n</li>\n<li>\n<p><a href=\"https://caikit.readthedocs.io/en/latest/autoapi/caikit/index.html\" target=\"_blank\" rel=\"noopener\">Caikit API documentation</a></p>\n</li>\n<li>\n<p><a href=\"https://github.com/caikit/caikit-nlp/tree/main\" target=\"_blank\" rel=\"noopener\">Caikit NLP GitHub project</a></p>\n</li>\n<li>\n<p><a href=\"https://docs.openvino.ai/2023.3/ovms_docs_rest_api_kfs.html\" target=\"_blank\" rel=\"noopener\">OpenVINO KServe-compatible REST API documentation</a></p>\n</li>\n<li>\n<p><a href=\"https://platform.openai.com/docs/api-reference/introduction\">OpenAI API documentation</a></p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"configuring-monitoring-for-the-single-model-serving-platform_serving-large-models\">Configuring monitoring for the single-model serving platform</h3>\n<div class=\"paragraph _abstract\">\n<p>The single-model serving platform includes metrics for <a href=\"https://opendatahub.io/docs/serving-models/#about-the-single-model-serving-platform_serving-large-models\">supported runtimes</a> of the KServe component. KServe does not generate its own metrics, and relies on the underlying model-serving runtimes to provide them. The set of available metrics for a deployed model depends on its model-serving runtime.</p>\n</div>\n<div class=\"paragraph\">\n<p>In addition to runtime metrics for KServe, you can also configure monitoring for OpenShift Service Mesh. The OpenShift Service Mesh metrics help you to understand dependencies and traffic flow between components in the mesh.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have cluster administrator privileges for your OpenShift Container Platform cluster.</p>\n</li>\n<li>\n<p>You have created OpenShift Service Mesh and Knative Serving instances and installed KServe.</p>\n</li>\n<li>\n<p>You have downloaded and installed the OpenShift command-line interface (CLI). See <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a>.</p>\n</li>\n<li>\n<p>You are familiar with <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/monitoring/configuring-the-monitoring-stack#creating-user-defined-workload-monitoring-configmap_configuring-the-monitoring-stack\">creating a config map</a> for monitoring a user-defined workflow. You will perform similar steps in this procedure.</p>\n</li>\n<li>\n<p>You are familiar with <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/monitoring/enabling-monitoring-for-user-defined-projects\">enabling monitoring</a> for user-defined projects in OpenShift. You will perform similar steps in this procedure.</p>\n</li>\n<li>\n<p>You have <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/monitoring/enabling-monitoring-for-user-defined-projects#granting-users-permission-to-monitor-user-defined-projects_enabling-monitoring-for-user-defined-projects\">assigned</a> the <code>monitoring-rules-view</code> role to users that will monitor metrics.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In a terminal window, if you are not already logged in to your OpenShift cluster as a cluster administrator, log in to the OpenShift CLI as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc login <em>&lt;openshift_cluster_url&gt;</em> -u <em>&lt;admin_username&gt;</em> -p <em>&lt;password&gt;</em></code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Define a <code>ConfigMap</code> object in a YAML file called <code>uwm-cm-conf.yaml</code> with the following contents:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: user-workload-monitoring-config\n  namespace: openshift-user-workload-monitoring\ndata:\n  config.yaml: |\n    prometheus:\n      logLevel: debug\n      retention: 15d</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The <code>user-workload-monitoring-config</code> object configures the components that monitor user-defined projects.  Observe that the retention time is set to the recommended value of 15 days.</p>\n</div>\n</li>\n<li>\n<p>Apply the configuration to create the <code>user-workload-monitoring-config</code> object.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc apply -f uwm-cm-conf.yaml</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Define another <code>ConfigMap</code> object in a YAML file called <code>uwm-cm-enable.yaml</code> with the following contents:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cluster-monitoring-config\n  namespace: openshift-monitoring\ndata:\n  config.yaml: |\n    enableUserWorkload: true</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The <code>cluster-monitoring-config</code> object enables monitoring for user-defined projects.</p>\n</div>\n</li>\n<li>\n<p>Apply the configuration to create the <code>cluster-monitoring-config</code> object.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc apply -f uwm-cm-enable.yaml</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Create <code>ServiceMonitor</code> and <code>PodMonitor</code> objects to monitor metrics in the service mesh control plane as follows:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Create an <code>istiod-monitor.yaml</code> YAML file with the following contents:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: istiod-monitor\n  namespace: istio-system\nspec:\n  targetLabels:\n  - app\n  selector:\n    matchLabels:\n      istio: pilot\n  endpoints:\n  - port: http-monitoring\n    interval: 30s</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Deploy the <code>ServiceMonitor</code> CR in the specified <code>istio-system</code> namespace.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc apply -f istiod-monitor.yaml</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>You see the following output:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>servicemonitor.monitoring.coreos.com/istiod-monitor created</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Create an <code>istio-proxies-monitor.yaml</code> YAML file with the following contents:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: monitoring.coreos.com/v1\nkind: PodMonitor\nmetadata:\n  name: istio-proxies-monitor\n  namespace: istio-system\nspec:\n  selector:\n    matchExpressions:\n    - key: istio-prometheus-ignore\n      operator: DoesNotExist\n  podMetricsEndpoints:\n  - path: /stats/prometheus\n    interval: 30s</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Deploy the <code>PodMonitor</code> CR in the specified <code>istio-system</code> namespace.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc apply -f istio-proxies-monitor.yaml</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>You see the following output:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>podmonitor.monitoring.coreos.com/istio-proxies-monitor created</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-metrics-for-the-single-model-serving-platform_serving-large-models\">Viewing model-serving runtime metrics for the single-model serving platform</h3>\n<div class=\"paragraph _abstract\">\n<p>When a cluster administrator has configured monitoring for the single-model serving platform, non-admin users can use the OpenShift web console to view model-serving runtime metrics for the KServe component.</p>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Log in to the OpenShift Container Platform web console.</p>\n</li>\n<li>\n<p>Switch to the <strong>Developer</strong> perspective.</p>\n</li>\n<li>\n<p>In the left menu, click <strong>Observe</strong>.</p>\n</li>\n<li>\n<p>As described in <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/building_applications/odc-monitoring-project-and-application-metrics-using-developer-perspective#odc-monitoring-your-project-metrics_monitoring-project-and-application-metrics-using-developer-perspective\" target=\"_blank\" rel=\"noopener\">Monitoring your project metrics</a>, use the web console to run queries for model-serving runtime metrics. You can also run queries for metrics that are related to OpenShift Service Mesh. Some examples are shown.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>The following query displays the number of successful inference requests over a period of time for a model deployed with the vLLM runtime:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>sum(increase(vllm:request_success_total{namespace=<em>${namespace}</em>,model_name=<em>${model_name}</em>}[${rate_interval}]))</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Certain vLLM metrics are available only after an inference request is processed by a deployed model. To generate and view these metrics, you must first make an inference request to the model.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>The following query displays the number of successful inference requests over a period of time for a model deployed with the standalone TGIS runtime:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>sum(increase(tgi_request_success{namespace=${namespace}, pod=~<em>${model_name}-predictor-.*</em>}[${rate_interval}]))</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>The following query displays the number of successful inference requests over a period of time for a model deployed with the Caikit Standalone runtime:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>sum(increase(predict_rpc_count_total{namespace=<em>${namespace}</em>,code=<em>OK</em>,model_id=<em>${model_name}</em>}[${rate_interval}]))</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>The following query displays the number of successful inference requests over a period of time for a model deployed with the OpenVINO Model Server runtime:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>sum(increase(ovms_requests_success{namespace=<em>${namespace}</em>,name=<em>${model_name}</em>}[${rate_interval}]))</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://docs.openvino.ai/2024/ovms_docs_metrics.html#available-metrics-families\" target=\"_blank\" rel=\"noopener\">OVMS metrics</a></p>\n</li>\n<li>\n<p><a href=\"https://github.com/IBM/text-generation-inference?tab=readme-ov-file#metrics\" target=\"_blank\" rel=\"noopener\">TGIS metrics</a></p>\n</li>\n<li>\n<p><a href=\"https://docs.vllm.ai/en/latest/serving/metrics.html\" target=\"_blank\" rel=\"noopener\">vLLM metrics</a></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_monitoring_model_performance_2\">Monitoring model performance</h3>\n<div class=\"paragraph\">\n<p>In the single-model serving platform, you can view performance metrics for a specific model that is deployed on the platform.</p>\n</div>\n<div class=\"sect3\">\n<h4 id=\"viewing-performance-metrics-for-deployed-model_serving-large-models\">Viewing performance metrics for a deployed model</h4>\n<div class=\"paragraph _abstract\">\n<p>You can monitor the following metrics for a specific model that is deployed on the single-model serving platform:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>Number of requests</strong> - The number of requests that have failed or succeeded for a specific model.</p>\n</li>\n<li>\n<p><strong>Average response time (ms)</strong> - The average time it takes a specific model to respond to requests.</p>\n</li>\n<li>\n<p><strong>CPU utilization (%)</strong> - The percentage of the CPU limit per model replica that is currently utilized by a specific model.</p>\n</li>\n<li>\n<p><strong>Memory utilization (%)</strong> - The percentage of the memory limit per model replica that is utilized by a specific model.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>You can specify a time range and a refresh interval for these metrics to help you determine, for example, when the peak usage hours are and how the model is performing at a specified time.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed Open Data Hub.</p>\n</li>\n<li>\n<p>A cluster admin has enabled user workload monitoring (UWM) for user-defined projects on your OpenShift cluster. For more information, see <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/monitoring/enabling-monitoring-for-user-defined-projects\">Enabling monitoring for user-defined projects</a> and <a href=\"https://docs.redhat.com/en/documentation/open_data_hub/2/html/serving_models/serving-large-models_serving-large-models#configuring-monitoring-for-the-single-model-serving-platform_serving-large-models\">Configuring monitoring for the single-model serving platform</a>.</p>\n</li>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>The following dashboard configuration options are set to the default values as shown:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>disablePerformanceMetrics:false\ndisableKServeMetrics:false</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>For more information, see <a href=\"https://opendatahub.io/docs/managing-odh/#ref-dashboard-configuration-options_dashboard\">Dashboard configuration options</a>.</p>\n</div>\n</li>\n<li>\n<p>You have deployed a model on the single-model serving platform by using a preinstalled runtime.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Metrics are only supported for models deployed by using a preinstalled model-serving runtime or a custom runtime that is duplicated from a preinstalled runtime.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard navigation menu, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that contains the data science models that you want to monitor.</p>\n</li>\n<li>\n<p>In the project details page, click the <strong>Models</strong> tab.</p>\n</li>\n<li>\n<p>Select the model that you are interested in.</p>\n</li>\n<li>\n<p>On the <strong>Endpoint performance</strong> tab, set the following options:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>Time range</strong> - Specifies how long to track the metrics. You can select one of these values: 1 hour, 24 hours, 7 days, and 30 days.</p>\n</li>\n<li>\n<p><strong>Refresh interval</strong> - Specifies how frequently the graphs on the metrics page are refreshed (to show the latest data). You can select one of these values: 15 seconds, 30 seconds, 1 minute, 5 minutes, 15 minutes, 30 minutes, 1 hour, 2 hours, and 1 day.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Scroll down to view data graphs for number of requests, average response time, CPU utilization, and memory utilization.</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>The <strong>Endpoint performance</strong> tab shows graphs of metrics for the model.</p>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_optimizing_model_serving_runtimes\">Optimizing model-serving runtimes</h3>\n<div class=\"paragraph\">\n<p>You can optionally enhance the preinstalled model-serving runtimes available in Open Data Hub to leverage additional benefits and capabilities, such as optimized inferencing, reduced latency, and fine-tuned resource allocation.</p>\n</div>\n<div class=\"sect3\">\n<h4 id=\"optimizing-the-vllm-runtime_serving-large-models\">Optimizing the vLLM model-serving runtime</h4>\n<div class=\"paragraph\">\n<p>You can configure the <strong>vLLM ServingRuntime for KServe</strong> runtime to use speculative decoding, a parallel processing technique to optimize inferencing time for large language models (LLMs).</p>\n</div>\n<div class=\"paragraph\">\n<p>You can also configure the runtime to support inferencing for vision-language models (VLMs). VLMs are a subset of multi-modal models that integrate both visual and textual data.</p>\n</div>\n<div class=\"paragraph\">\n<p>To configure the <strong>vLLM ServingRuntime for KServe</strong> runtime for speculative decoding or multi-modal inferencing, you must add additional arguments in the vLLM model-serving runtime.</p>\n</div>\n<div class=\"ulist _abstract\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub as a user with Open Data Hub administrator privileges.</p>\n</li>\n<li>\n<p>If you used the pre-installed <strong>vLLM ServingRuntime for KServe</strong> runtime, you duplicated the runtime to create a custom version. For more information about duplicating the pre-installed vLLM runtime, see <a href=\"https://opendatahub.io/docs/serving-models/#adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models\">Adding a custom model-serving runtime for the single-model serving platform</a>.</p>\n</li>\n<li>\n<p>If you are using the vLLM model-serving runtime for speculative decoding with a draft model, you have stored the original model and the speculative model in the same folder within your S3-compatible object storage.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Settings</strong> &gt; <strong>Serving runtimes</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Serving runtimes</strong> page opens and shows the model-serving runtimes that are already installed and enabled.</p>\n</div>\n</li>\n<li>\n<p>Find the custom vLLM model-serving runtime that you created, click the action menu (&#8942;) next to the runtime and select <strong>Edit</strong>.</p>\n<div class=\"paragraph\">\n<p>The embedded YAML editor opens and shows the contents of the custom model-serving runtime.</p>\n</div>\n</li>\n<li>\n<p>To configure the vLLM model-serving runtime for speculative decoding by matching n-grams in the prompt:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Add the following arguments:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>containers:\n  - args:\n      - --speculative-model=[ngram]\n      - --num-speculative-tokens=&lt;NUM_SPECULATIVE_TOKENS&gt;\n      - --ngram-prompt-lookup-max=&lt;NGRAM_PROMPT_LOOKUP_MAX&gt;\n      - --use-v2-block-manager</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Replace <code>&lt;NUM_SPECULATIVE_TOKENS&gt;</code> and <code>&lt;NGRAM_PROMPT_LOOKUP_MAX&gt;</code> with your own values.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Inferencing throughput varies depending on the model used for speculating with n-grams.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>To configure the vLLM model-serving runtime for speculative decoding with a draft model:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Remove the <code>--model</code> argument:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>containers:\n  - args:\n      - --model=/mnt/models</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Add the following arguments:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>containers:\n  - args:\n      - --port=8080\n      - --served-model-name={{.Name}}\n      - --distributed-executor-backend=mp\n      - --model=/mnt/models/&lt;path_to_original_model&gt;\n      - --speculative-model=/mnt/models/&lt;path_to_speculative_model&gt;\n      - --num-speculative-tokens=&lt;NUM_SPECULATIVE_TOKENS&gt;\n      - --use-v2-block-manager</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Replace <code>&lt;path_to_speculative_model&gt;</code> and <code>&lt;path_to_original_model&gt;</code> with the paths to the speculative model and original model on your S3-compatible object storage.</p>\n</li>\n<li>\n<p>Replace <code>&lt;NUM_SPECULATIVE_TOKENS&gt;</code> with your own value.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>To configure the vLLM model-serving runtime for multi-modal inferencing:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Add the following arguments:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>containers:\n  - args:\n      - --trust-remote-code</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Only use the <code>--trust-remote-code</code> argument with models from trusted sources.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Update</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Serving runtimes</strong> page opens and shows the list of runtimes that are installed. Confirm that the custom model-serving runtime you updated is shown.</p>\n</div>\n</li>\n<li>\n<p>Deploy the model by using the custom runtime as described in <a href=\"https://opendatahub.io/docs/serving-models/#deploying-models-using-the-single-model-serving-platform_serving-large-models\">Deploying models on the single-model serving platform</a>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>If you have configured the vLLM model-serving runtime for speculative decoding, use the following example command to verify API requests to your deployed model:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>curl -v https://&lt;inference_endpoint_url&gt;:443/v1/chat/completions\n-H \"Content-Type: application/json\"\n-H \"Authorization: Bearer &lt;token&gt;\"</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>If you have configured the vLLM model-serving runtime for multi-modal inferencing, use the following example command to verify API requests to the vision-language model (VLM) that you have deployed:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>curl -v https://&lt;inference_endpoint_url&gt;:443/v1/chat/completions\n-H \"Content-Type: application/json\"\n-H \"Authorization: Bearer &lt;token&gt;\"\n-d '{\"model\":\"&lt;model_name&gt;\",\n     \"messages\":\n        [{\"role\":\"&lt;role&gt;\",\n          \"content\":\n             [{\"type\":\"text\", \"text\":\"&lt;text&gt;\"\n              },\n              {\"type\":\"image_url\", \"image_url\":\"&lt;image_url_link&gt;\"\n              }\n             ]\n         }\n        ]\n    }'</code></pre>\n</div>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://docs.vllm.ai/en/latest/models/engine_args.html\">vLLM Engine Arguments</a></p>\n</li>\n<li>\n<p><a href=\"https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\">OpenAI Compatible Server</a></p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_performance_tuning_on_the_single_model_serving_platform\">Performance tuning on the single-model serving platform</h3>\n<div class=\"paragraph\">\n<p>Certain performance issues might require you to tune the parameters of your inference service or model-serving runtime.</p>\n</div>\n<div class=\"sect3\">\n<h4 id=\"resolving-cuda-oom-errors-for-the-single-model-serving-platform_serving-large-models\">Resolving CUDA out-of-memory errors</h4>\n<div class=\"paragraph _abstract\">\n<p>In certain cases, depending on the model and hardware accelerator used, the TGIS memory auto-tuning algorithm might underestimate the amount of GPU memory needed to process long sequences. This miscalculation can lead to Compute Unified Architecture (CUDA) out-of-memory (OOM) error responses from the model server. In such cases, you must update or add additional parameters in the TGIS model-serving runtime, as described in the following procedure.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub as a user with Open Data Hub administrator privileges.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Settings</strong> &gt; <strong>Serving runtimes</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Serving runtimes</strong> page opens and shows the model-serving runtimes that are already installed and enabled.</p>\n</div>\n</li>\n<li>\n<p>Based on the runtime that you used to deploy your model, perform one of the following actions:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>If you used the pre-installed <strong>TGIS Standalone ServingRuntime for KServe</strong> runtime, duplicate the runtime to create a custom version and then follow the remainder of this procedure. For more information about duplicating the pre-installed TGIS runtime, see <a href=\"https://docs.redhat.com/en/documentation/open_data_hub/2/html/serving_models/serving-large-models_serving-large-models#adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models\">Adding a custom model-serving runtime for the single-model serving platform</a>.</p>\n</li>\n<li>\n<p>If you were already using a custom TGIS runtime, click the action menu (&#8942;) next to the runtime and select <strong>Edit</strong>.</p>\n<div class=\"paragraph\">\n<p>The embedded YAML editor opens and shows the contents of the custom model-serving runtime.</p>\n</div>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Add or update the <code>BATCH_SAFETY_MARGIN</code> environment variable and set the value to 30. Similarly, add or update the <code>ESTIMATE_MEMORY_BATCH_SIZE</code> environment variable and set the value to 8.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>spec:\n  containers:\n    env:\n    - name: BATCH_SAFETY_MARGIN\n      value: 30\n    - name: ESTIMATE_MEMORY_BATCH\n      value: 8</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>The <code>BATCH_SAFETY_MARGIN</code> parameter sets a percentage of free GPU memory to hold back as a safety margin to avoid OOM conditions. The default value of <code>BATCH_SAFETY_MARGIN</code> is <code>20</code>. The <code>ESTIMATE_MEMORY_BATCH_SIZE</code> parameter sets the batch size used in the memory auto-tuning algorithm. The default value of <code>ESTIMATE_MEMORY_BATCH_SIZE</code>  is <code>16</code>.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Click <strong>Update</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Serving runtimes</strong> page opens and shows the list of runtimes that are installed. Observe that the custom model-serving runtime you updated is shown.</p>\n</div>\n</li>\n<li>\n<p>To redeploy the model for the parameter updates to take effect, perform the following actions:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Model Serving</strong> &gt; <strong>Deployed Models</strong>.</p>\n</li>\n<li>\n<p>Find the model you want to redeploy, click the action menu () next to the model, and select <strong>Delete</strong>.</p>\n</li>\n<li>\n<p>Redeploy the model as described in <a href=\"https://opendatahub.io/docs/serving-models/#deploying-models-on-the-single-model-serving-platform_serving-large-models\">Deploying models on the single-model serving platform</a>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>You receive successful responses from the model server and no longer see CUDA OOM errors.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"supported-model-serving-runtimes_serving-large-models\">Supported model-serving runtimes</h3>\n<div class=\"paragraph _abstract\">\n<p>Open Data Hub includes several preinstalled model-serving runtimes. You can use preinstalled model-serving runtimes to start serving models without modifying or defining the runtime yourself. You can also add a custom runtime to support a model.</p>\n</div>\n<div class=\"paragraph\">\n<p>For help adding a custom runtime, see <a href=\"https://opendatahub.io/docs/serving-models/#adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models\">Adding a custom model-serving runtime for the single-model serving platform</a>.</p>\n</div>\n<table class=\"tableblock frame-all grid-all stretch\">\n<caption class=\"title\">Table 1. Model-serving runtimes</caption>\n<colgroup>\n<col style=\"width: 33.3333%;\">\n<col style=\"width: 33.3333%;\">\n<col style=\"width: 33.3334%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Name</th>\n<th class=\"tableblock halign-left valign-top\">Description</th>\n<th class=\"tableblock halign-left valign-top\">Exported model format</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Caikit Text Generation Inference Server (Caikit-TGIS) ServingRuntime for KServe (1)</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">A composite runtime for serving models in the Caikit format</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Caikit Text Generation</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Caikit Standalone ServingRuntime for KServe (2)</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">A runtime for serving models in the Caikit embeddings format for embeddings tasks</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Caikit Embeddings</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">OpenVINO Model Server</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">A scalable, high-performance runtime for serving models that are optimized for Intel architectures</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">PyTorch, TensorFlow, OpenVINO IR, PaddlePaddle, MXNet, Caffe, Kaldi</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Text Generation Inference Server (TGIS) Standalone ServingRuntime for KServe (3)</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">A runtime for serving TGI-enabled models</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">PyTorch Model Formats</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">vLLM ServingRuntime for KServe</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">A high-throughput and memory-efficient inference and serving runtime for large language models</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><a href=\"https://docs.vllm.ai/en/latest/models/supported_models.html\" target=\"_blank\" rel=\"noopener\">Supported models</a></p></td>\n</tr>\n</tbody>\n</table>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>The composite Caikit-TGIS runtime is based on <a href=\"https://github.com/opendatahub-io/caikit\" target=\"_blank\" rel=\"noopener\">Caikit</a> and <a href=\"https://github.com/IBM/text-generation-inference\" target=\"_blank\" rel=\"noopener\">Text Generation Inference Server (TGIS)</a>. To use this runtime, you must convert your models to Caikit format. For an example, see <a href=\"https://github.com/opendatahub-io/caikit-tgis-serving/blob/main/demo/kserve/built-tip.md#bootstrap-process\" target=\"_blank\" rel=\"noopener\">Converting Hugging Face Hub models to Caikit format</a> in the <a href=\"https://github.com/opendatahub-io/caikit-tgis-serving/tree/main\" target=\"_blank\" rel=\"noopener\">caikit-tgis-serving</a> repository.</p>\n</li>\n<li>\n<p>The Caikit Standalone runtime is based on <a href=\"https://github.com/caikit/caikit-nlp/tree/main\" target=\"_blank\" rel=\"noopener\">Caikit NLP</a>. To use this runtime, you must convert your models to the Caikit embeddings format. For an example, see <a href=\"https://github.com/caikit/caikit-nlp/blob/main/tests/modules/text_embedding/test_embedding.py\" target=\"_blank\" rel=\"noopener\">Tests for text embedding module</a>.</p>\n</li>\n<li>\n<p><a href=\"https://github.com/IBM/text-generation-inference\" target=\"_blank\" rel=\"noopener\">Text Generation Inference Server (TGIS)</a> is based on an early fork of <a href=\"https://github.com/huggingface/text-generation-inference\" target=\"_blank\" rel=\"noopener\">Hugging Face TGI</a>. Red&#160;Hat will continue to develop the standalone TGIS runtime to support TGI models. If a model is incompatible in the current version of Open Data Hub, support might be added in a future version. In the meantime, you can also add your own custom runtime to support a TGI model. For more information, see <a href=\"https://opendatahub.io/docs/serving-models/#adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models\">Adding a custom model-serving runtime for the single-model serving platform</a>.</p>\n</li>\n</ol>\n</div>\n<table class=\"tableblock frame-all grid-all stretch\">\n<caption class=\"title\">Table 2. Deployment requirements</caption>\n<colgroup>\n<col style=\"width: 16.6666%;\">\n<col style=\"width: 16.6666%;\">\n<col style=\"width: 16.6666%;\">\n<col style=\"width: 16.6666%;\">\n<col style=\"width: 16.6666%;\">\n<col style=\"width: 16.667%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Name</th>\n<th class=\"tableblock halign-left valign-top\">Default protocol</th>\n<th class=\"tableblock halign-left valign-top\">Additonal protocol</th>\n<th class=\"tableblock halign-left valign-top\">Model mesh support</th>\n<th class=\"tableblock halign-left valign-top\">Single node OpenShift support</th>\n<th class=\"tableblock halign-left valign-top\">Deployment mode</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Caikit Text Generation Inference Server (Caikit-TGIS) ServingRuntime for KServe</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">REST</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">gRPC</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">No</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Yes</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Raw and serverless</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Caikit Standalone ServingRuntime for KServe</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">REST</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">gRPC</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">No</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Yes</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Raw and serverless</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">OpenVINO Model Server</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">REST</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">None</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Yes</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Yes</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Raw and serverless</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Text Generation Inference Server (TGIS) Standalone ServingRuntime for KServe</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">gRPC</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">None</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">No</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Yes</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Raw and serverless</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">vLLM ServingRuntime for KServe</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">REST</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">None</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">No</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Yes</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Raw and serverless</p></td>\n</tr>\n</tbody>\n</table>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://opendatahub.io/docs/serving-models/#inference-endpoints_serving-large-models\">Inference endpoints</a></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"tested-verified-runtimes_serving-large-models\">Tested and verified model-serving runtimes</h3>\n<div class=\"paragraph _abstract\">\n<p>Tested and verified runtimes are community versions of model-serving runtimes that have been tested and verified against specific versions of Open Data Hub.</p>\n</div>\n<div class=\"paragraph\">\n<p>Red&#160;Hat tests the current version of a tested and verified runtime each time there is a new version of Open Data Hub. If a new version of a tested and verified runtime is released in the middle of an Open Data Hub release cycle, it will be tested and verified in an upcoming release.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Tested and verified runtimes are not directly supported by Red&#160;Hat. You are responsible for ensuring that you are licensed to use any tested and verified runtimes that you add, and for correctly configuring and maintaining them.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<table class=\"tableblock frame-all grid-all stretch\">\n<caption class=\"title\">Table 3. Model-serving runtimes</caption>\n<colgroup>\n<col style=\"width: 33.3333%;\">\n<col style=\"width: 33.3333%;\">\n<col style=\"width: 33.3334%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Name</th>\n<th class=\"tableblock halign-left valign-top\">Description</th>\n<th class=\"tableblock halign-left valign-top\">Exported model format</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">NVIDIA Triton Inference Server</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">An open-source inference-serving software for fast and scalable AI in applications.</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">TensorRT, TensorFlow, PyTorch, ONNX, OpenVINO, Python, RAPIDS FIL, and more</p></td>\n</tr>\n</tbody>\n</table>\n<table class=\"tableblock frame-all grid-all stretch\">\n<caption class=\"title\">Table 4. Deployment requirements</caption>\n<colgroup>\n<col style=\"width: 16.6666%;\">\n<col style=\"width: 16.6666%;\">\n<col style=\"width: 16.6666%;\">\n<col style=\"width: 16.6666%;\">\n<col style=\"width: 16.6666%;\">\n<col style=\"width: 16.667%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Name</th>\n<th class=\"tableblock halign-left valign-top\">Default protocol</th>\n<th class=\"tableblock halign-left valign-top\">Additonal protocol</th>\n<th class=\"tableblock halign-left valign-top\">Model mesh support</th>\n<th class=\"tableblock halign-left valign-top\">Single node OpenShift support</th>\n<th class=\"tableblock halign-left valign-top\">Deployment mode</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">NVIDIA Triton Inference Server</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">gRPC</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">REST</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Yes</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Yes</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Raw and serverless</p></td>\n</tr>\n</tbody>\n</table>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://opendatahub.io/docs/serving-models/#inference-endpoints_serving-large-models\">Inference endpoints</a></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"inference-endpoints_serving-large-models\">Inference endpoints</h3>\n<div class=\"paragraph _abstract\">\n<p>These examples show how to use inference endpoints to query the model.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you enabled token authorization when deploying the model, add the <code>Authorization</code> header and specify a token value.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_caikit_tgis_servingruntime_for_kserve\">Caikit TGIS ServingRuntime for KServe</h4>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>:443/api/v1/task/text-generation</code></p>\n</li>\n<li>\n<p><code>:443/api/v1/task/server-streaming-text-generation</code></p>\n</li>\n</ul>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Example command</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code>curl --json '{\"model_id\": \"&lt;model_name&gt;\", \"inputs\": \"&lt;text&gt;\"}' \\\nhttps://&lt;inference_endpoint_url&gt;:443/api/v1/task/server-streaming-text-generation \\\n-H 'Authorization: Bearer &lt;token&gt;'</code></pre>\n</div>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_caikit_standalone_servingruntime_for_kserve\">Caikit Standalone ServingRuntime for KServe</h4>\n<div class=\"paragraph\">\n<p>If you are serving multiple models, you can query <code>/info/models</code> or <code>:443 caikit.runtime.info.InfoService/GetModelsInfo</code> to view a list of served models.</p>\n</div>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"ulist\">\n<div class=\"title\">REST endpoints</div>\n<ul>\n<li>\n<p><code>/api/v1/task/embedding</code></p>\n</li>\n<li>\n<p><code>/api/v1/task/embedding-tasks</code></p>\n</li>\n<li>\n<p><code>/api/v1/task/sentence-similarity</code></p>\n</li>\n<li>\n<p><code>/api/v1/task/sentence-similarity-tasks</code></p>\n</li>\n<li>\n<p><code>/api/v1/task/rerank</code></p>\n</li>\n<li>\n<p><code>/api/v1/task/rerank-tasks</code></p>\n</li>\n<li>\n<p><code>/info/models</code></p>\n</li>\n<li>\n<p><code>/info/version</code></p>\n</li>\n<li>\n<p><code>/info/runtime</code></p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"ulist\">\n<div class=\"title\">gRPC endpoints</div>\n<ul>\n<li>\n<p><code>:443 caikit.runtime.Nlp.NlpService/EmbeddingTaskPredict</code></p>\n</li>\n<li>\n<p><code>:443 caikit.runtime.Nlp.NlpService/EmbeddingTasksPredict</code></p>\n</li>\n<li>\n<p><code>:443 caikit.runtime.Nlp.NlpService/SentenceSimilarityTaskPredict</code></p>\n</li>\n<li>\n<p><code>:443 caikit.runtime.Nlp.NlpService/SentenceSimilarityTasksPredict</code></p>\n</li>\n<li>\n<p><code>:443 caikit.runtime.Nlp.NlpService/RerankTaskPredict</code></p>\n</li>\n<li>\n<p><code>:443 caikit.runtime.Nlp.NlpService/RerankTasksPredict</code></p>\n</li>\n<li>\n<p><code>:443 caikit.runtime.info.InfoService/GetModelsInfo</code></p>\n</li>\n<li>\n<p><code>:443 caikit.runtime.info.InfoService/GetRuntimeInfo</code></p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>By default, the Caikit Standalone Runtime exposes REST endpoints. To use gRPC protocol, manually deploy a custom Caikit Standalone ServingRuntime. For more information, see <a href=\"https://opendatahub.io/docs/serving-models/#adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models\">Adding a custom model-serving runtime for the single-model serving platform</a>.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>An example manifest is available in the <a href=\"https://github.com/opendatahub-io/caikit-tgis-serving/blob/main/demo/kserve/custom-manifests/caikit/caikit-standalone/caikit-standalone-servingruntime-grpc.yaml\" target=\"_blank\" rel=\"noopener\">caikit-tgis-serving GitHub repository</a>.</p>\n</div>\n<div class=\"openblock\">\n<div class=\"title\">Example command</div>\n<div class=\"content\">\n<div class=\"paragraph\">\n<p><strong>REST</strong></p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>curl -H 'Content-Type: application/json' -d '{\"inputs\": \"&lt;text&gt;\", \"model_id\": \"&lt;model_id&gt;\"}' &lt;inference_endpoint_url&gt;/api/v1/task/embedding -H 'Authorization: Bearer &lt;token&gt;'</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p><strong>gRPC</strong></p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>grpcurl -d '{\"text\": \"&lt;text&gt;\"}' -H \\\"mm-model-id: &lt;model_id&gt;\\\" &lt;inference_endpoint_url&gt;:443 caikit.runtime.Nlp.NlpService/EmbeddingTaskPredict -H 'Authorization: Bearer &lt;token&gt;'</code></pre>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_tgis_standalone_servingruntime_for_kserve\">TGIS Standalone ServingRuntime for KServe</h4>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>:443 fmaas.GenerationService/Generate</code></p>\n</li>\n<li>\n<p><code>:443 fmaas.GenerationService/GenerateStream</code></p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>To query the endpoint for the TGIS standalone runtime, you must also download the files in the <a href=\"https://github.com/opendatahub-io/text-generation-inference/blob/main/proto\" target=\"_blank\" rel=\"noopener\">proto</a> directory of the Open Data Hub <code>text-generation-inference</code> repository.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Example command</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code>grpcurl -proto text-generation-inference/proto/generation.proto -d \\\n'{\"requests\": [{\"text\":\"&lt;text&gt;\"}]}' \\\n-insecure &lt;inference_endpoint_url&gt;:443 fmaas.GenerationService/Generate \\\n-H 'Authorization: Bearer &lt;token&gt;'</code></pre>\n</div>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_openvino_model_server\">OpenVINO Model Server</h4>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>/v2/models/&lt;model-name&gt;/infer</code></p>\n</li>\n</ul>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Example command</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code>curl -ks &lt;inference_endpoint_url&gt;/v2/models/&lt;model_name&gt;/infer -d \\\n'{ \"model_name\": \"&lt;model_name&gt;\", \\\n\"inputs\": [{ \"name\": \"&lt;name_of_model_input&gt;\", \"shape\": [&lt;shape&gt;], \"datatype\": \"&lt;data_type&gt;\", \"data\": [&lt;data&gt;] }]}' \\\n-H 'Authorization: Bearer &lt;token&gt;'</code></pre>\n</div>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_vllm_servingruntime_for_kserve\">vLLM ServingRuntime for KServe</h4>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>:443/version</code></p>\n</li>\n<li>\n<p><code>:443/docs</code></p>\n</li>\n<li>\n<p><code>:443/v1/models</code></p>\n</li>\n<li>\n<p><code>:443/v1/chat/completions</code></p>\n</li>\n<li>\n<p><code>:443/v1/completions</code></p>\n</li>\n<li>\n<p><code>:443/v1/embeddings</code></p>\n</li>\n<li>\n<p><code>:443/tokenize</code></p>\n</li>\n<li>\n<p><code>:443/detokenize</code></p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p>The vLLM runtime is compatible with the OpenAI REST API. For a list of models that the vLLM runtime supports, see <a href=\"https://docs.vllm.ai/en/latest/models/supported_models.html\">Supported models</a>.</p>\n</li>\n<li>\n<p>To use the embeddings inference endpoint in vLLM, you must use an embeddings model that the vLLM supports. You cannot use the embeddings endpoint with generative models. For more information, see <a href=\"https://github.com/vllm-project/vllm/pull/3734\">Supported embeddings models in vLLM</a>.</p>\n</li>\n<li>\n<p>As of vLLM v0.5.5, you must provide a chat template while querying a model using the <code>/v1/chat/completions</code> endpoint. If your model does not include a predefined chat template, you can use the <code>chat-template</code> command-line parameter to specify a chat template in your custom vLLM runtime, as shown in the example. Replace <code>&lt;CHAT_TEMPLATE&gt;</code> with the path to your template.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>containers:\n  - args:\n      - --chat-template=&lt;CHAT_TEMPLATE&gt;</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>You can use the chat templates that are available as <code>.jinja</code> files <a href=\"https://github.com/opendatahub-io/vllm/tree/main/examples\">here</a> or with the vLLM image under <code>/apps/data/template</code>. For more information, see <a href=\"https://huggingface.co/docs/transformers/main/chat_templating\">Chat templates</a>.</p>\n</div>\n</li>\n</ul>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>As indicated by the paths shown, the single-model serving platform uses the HTTPS port of your OpenShift router (usually port 443) to serve external API requests.</p>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Example command</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code>curl -v https://&lt;inference_endpoint_url&gt;:443/v1/chat/completions -H \\\n\"Content-Type: application/json\" -d '{ \\\n\"messages\": [{ \\\n\"role\": \"&lt;role&gt;\", \\\n\"content\": \"&lt;content&gt;\" \\\n}] -H 'Authorization: Bearer &lt;token&gt;'</code></pre>\n</div>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_nvidia_triton_inference_server\">NVIDIA Triton Inference Server</h4>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"ulist\">\n<div class=\"title\">REST endpoints</div>\n<ul>\n<li>\n<p><code>v2/models/[/versions/&lt;model_version&gt;]/infer</code></p>\n</li>\n<li>\n<p><code>v2/models/&lt;model_name&gt;[/versions/&lt;model_version&gt;]</code></p>\n</li>\n<li>\n<p><code>v2/health/ready</code></p>\n</li>\n<li>\n<p><code>v2/health/live</code></p>\n</li>\n<li>\n<p><code>v2/models/&lt;model_name&gt;[/versions/]/ready</code></p>\n</li>\n<li>\n<p><code>v2</code></p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Example command</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code>curl -ks &lt;inference_endpoint_url&gt;/v2/models/&lt;model_name&gt;/infer -d /\n'{ \"model_name\": \"&lt;model_name&gt;\", \\\n   \"inputs\": \\\n\t[{ \"name\": \"&lt;name_of_model_input&gt;\", \\\n           \"shape\": [&lt;shape&gt;], \\\n           \"datatype\": \"&lt;data_type&gt;\", \\\n           \"data\": [&lt;data&gt;] \\\n         }]}' -H 'Authorization: Bearer &lt;token&gt;'</code></pre>\n</div>\n</div>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"ulist\">\n<div class=\"title\">gRPC endpoints</div>\n<ul>\n<li>\n<p><code>:443 inference.GRPCInferenceService/ModelInfer</code></p>\n</li>\n<li>\n<p><code>:443 inference.GRPCInferenceService/ModelReady</code></p>\n</li>\n<li>\n<p><code>:443 inference.GRPCInferenceService/ModelMetadata</code></p>\n</li>\n<li>\n<p><code>:443 inference.GRPCInferenceService/ServerReady</code></p>\n</li>\n<li>\n<p><code>:443 inference.GRPCInferenceService/ServerLive</code></p>\n</li>\n<li>\n<p><code>:443 inference.GRPCInferenceService/ServerMetadata</code></p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Example command</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code>grpcurl -cacert ./openshift_ca_istio_knative.crt \\\n        -proto ./grpc_predict_v2.proto \\\n        -d @ \\\n        -H \"Authorization: Bearer &lt;token&gt;\" \\\n        &lt;inference_endpoint_url&gt;:443 \\\n        inference.GRPCInferenceService/ModelMetadata</code></pre>\n</div>\n</div>\n</div>\n<div class=\"sect3 _additional-resources\">\n<h4 id=\"_additional_resources\">Additional resources</h4>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><a href=\"https://github.com/IBM/text-generation-inference\" target=\"_blank\" rel=\"noopener\">Text Generation Inference Server (TGIS)</a></p>\n</li>\n<li>\n<p><a href=\"https://caikit.readthedocs.io/en/latest/autoapi/caikit/index.html\" target=\"_blank\" rel=\"noopener\">Caikit API documentation</a></p>\n</li>\n<li>\n<p><a href=\"https://github.com/caikit/caikit-nlp\" target=\"_blank\" rel=\"noopener\">Caikit NLP GitHub project</a></p>\n</li>\n<li>\n<p><a href=\"https://docs.openvino.ai/2023.3/ovms_docs_rest_api_kfs.html\" target=\"_blank\" rel=\"noopener\">OpenVINO KServe-compatible REST API documentation</a></p>\n</li>\n<li>\n<p><a href=\"https://platform.openai.com/docs/api-reference/introduction\" target=\"_blank\" rel=\"noopener\">OpenAI API documentation</a></p>\n</li>\n<li>\n<p><a href=\"https://kserve.github.io/website/master/modelserving/data_plane/v2_protocol/\">Open Inference Protocol</a></p>\n</li>\n<li>\n<p><a href=\"https://opendatahub.io/docs/serving-models/#supported-runtimes_serving-large-models\">Supported model-serving runtimes</a></p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"about-the-NVIDIA-NIM-model-serving-platform_serving-large-models\">About the NVIDIA NIM model serving platform</h3>\n<div class=\"paragraph _abstract\">\n<p>You can deploy models using NVIDIA NIM inference services on the <strong>NVIDIA NIM model serving platform</strong>.</p>\n</div>\n<div class=\"paragraph\">\n<p>NVIDIA NIM, part of NVIDIA AI Enterprise, is a set of microservices designed for secure, reliable deployment of high performance AI model inferencing across clouds, data centers and workstations.</p>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://docs.nvidia.com/nim/index.html\">NVIDIA NIM</a></p>\n</li>\n</ul>\n</div>\n<div class=\"sect3\">\n<h4 id=\"enabling-the-nvidia-nim-model-serving-platform_serving-large-models\">Enabling the NVIDIA NIM model serving platform</h4>\n<div class=\"paragraph _abstract\">\n<p>As an administrator, you can use the Open Data Hub dashboard to enable the NVIDIA NIM model serving platform.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub as an administrator.</p>\n</li>\n<li>\n<p>You have enabled the single-model serving platform as described in <a href=\"https://opendatahub.io/docs/serving-models/#deploying-models-using-the-single-model-serving-platform_serving-large-models\" target=\"_blank\" rel=\"noopener\">Enabling the single-model serving platform</a>.</p>\n</li>\n<li>\n<p>The following Open Data Hub dashboard configuration is enabled.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>disableNIMModelServing:false</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>For more information, see <a href=\"https://opendatahub.io/docs/managing-resources/#ref-dashboard-configuration-options_dashboard\">Dashboard configuration options</a>.</p>\n</div>\n</li>\n<li>\n<p>You have enabled GPU support. For more information, see <a href=\"https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html\" target=\"_blank\" rel=\"noopener\">NVIDIA GPU Operator on Red&#160;Hat OpenShift Container Platform</a> in the NVIDIA documentation.</p>\n</li>\n<li>\n<p>You have access to the NVIDIA AI Enterprise license key.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>On the Open Data Hub home page, click <strong>Explore</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Explore</strong> page, find the <strong>NVIDIA NIM</strong> tile.</p>\n</li>\n<li>\n<p>Click <strong>Enable</strong> on the application tile.</p>\n</li>\n<li>\n<p>Enter the NVIDIA AI Enterprise license key and then click <strong>Submit</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The NVIDIA NIM application that you enabled appears on the <strong>Enabled</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deploying-models-on-the-NVIDIA-NIM-model-serving-platform_serving-large-models\">Deploying models on the NVIDIA NIM model serving platform</h4>\n<div class=\"paragraph _abstract\">\n<p>When you have enabled the <strong>NVIDIA NIM model serving platform</strong>, you can start to deploy NVIDIA-optimized models on the platform.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have enabled the <strong>NVIDIA NIM model serving platform</strong>.</p>\n</li>\n<li>\n<p>You have created a data science project.</p>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left menu, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to deploy a model in.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Models</strong> tab.</p>\n</li>\n<li>\n<p>Find the <strong>NVIDIA NIM model serving platform</strong> tile, then click <strong>Deploy model</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Deploy model</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Configure properties for deploying your model as follows:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Model deployment name</strong> field, enter a unique name for the deployment.</p>\n</li>\n<li>\n<p>From the <strong>NVIDIA NIM</strong> list, select the NVIDIA NIM model that you want to deploy.</p>\n</li>\n<li>\n<p>In the <strong>NVIDIA NIM storage size</strong> field, specify the size of the cluster storage instance that will be created to store the NVIDIA NIM model.</p>\n</li>\n<li>\n<p>In the <strong>Number of model server replicas to deploy</strong> field, specify a value.</p>\n</li>\n<li>\n<p>From the <strong>Model server size</strong> list, select a value.</p>\n</li>\n<li>\n<p>From the <strong>Accelerator</strong> list, select the <strong>NVIDIA GPU</strong> accelerator.</p>\n<div class=\"paragraph\">\n<p>The <strong>Number of accelerators</strong> field appears.</p>\n</div>\n</li>\n<li>\n<p>In the <strong>Number of accelerators</strong> field, specify the number of accelerators to use. The default value is 1.</p>\n</li>\n<li>\n<p>Click <strong>Deploy</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Confirm that the deployed model is shown on the <strong>Models</strong> tab for the project, and on the <strong>Model Serving</strong> page of the dashboard with a checkmark in the <strong>Status</strong> column.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n</div>","id":"51c2e86f-2eb0-5cb6-856d-e1fd131f5ce9","document":{"title":"Serving models"}},"markdownRemark":null},"pageContext":{"id":"51c2e86f-2eb0-5cb6-856d-e1fd131f5ce9"}},"staticQueryHashes":["2604506565"],"slicesMap":{}}