{"componentChunkName":"component---src-templates-docs-page-tsx","path":"/docs/working-with-data-science-pipelines/","result":{"data":{"allFile":{"edges":[{"node":{"childAsciidoc":{"fields":{"slug":"/docs/README/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/getting-started-with-open-data-hub/"},"sections":[{"parentId":null,"name":"Overview","level":1,"index":0,"id":"overview-for-getting-started_get-started"},{"parentId":"overview-for-getting-started_get-started","name":"Data science workflow","level":2,"index":0,"id":"_data_science_workflow"},{"parentId":"overview-for-getting-started_get-started","name":"About this guide","level":2,"index":1,"id":"_about_this_guide"},{"parentId":null,"name":"Logging in to Open Data Hub","level":1,"index":1,"id":"logging-in_get-started"},{"parentId":null,"name":"Creating a data science project","level":1,"index":2,"id":"creating-a-data-science-project_get-started"},{"parentId":null,"name":"Creating a workbench and selecting an IDE","level":1,"index":3,"id":"creating-a-workbench-select-ide_get-started"},{"parentId":"creating-a-workbench-select-ide_get-started","name":"About workbench images","level":2,"index":0,"id":"about-workbench-images_get-started"},{"parentId":"creating-a-workbench-select-ide_get-started","name":"Creating a workbench","level":2,"index":1,"id":"creating-a-project-workbench_get-started"},{"parentId":null,"name":"Next steps","level":1,"index":4,"id":"next-steps_get-started"},{"parentId":"next-steps_get-started","name":"Additional resources","level":2,"index":0,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/installing-open-data-hub/"},"sections":[{"parentId":null,"name":"Installing Open Data Hub version 2","level":1,"index":0,"id":"installing-odh-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing the Open Data Hub Operator version 2","level":2,"index":0,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing Open Data Hub components","level":2,"index":1,"id":"installing-odh-components_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Accessing the Open Data Hub dashboard","level":2,"index":2,"id":"accessing-the-odh-dashboard_installv2"},{"parentId":null,"name":"Installing Open Data Hub version 1","level":1,"index":1,"id":"installing-odh-v1_installv1"},{"parentId":"installing-odh-v1_installv1","name":"Installing the Open Data Hub Operator version 1","level":2,"index":0,"id":"installing-the-odh-operator-v1_installv1"},{"parentId":"installing-odh-v1_installv1","name":"Creating a new project for your Open Data Hub instance","level":2,"index":1,"id":"creating-a-new-project-for-your-odh-instance_installv1"},{"parentId":"installing-odh-v1_installv1","name":"Adding an Open Data Hub instance","level":2,"index":2,"id":"adding-an-odh-instance_installv1"},{"parentId":"installing-odh-v1_installv1","name":"Accessing the Open Data Hub dashboard","level":2,"index":3,"id":"accessing-the-odh-dashboard_installv1"},{"parentId":null,"name":"Configuring the Open Data Hub Operator logger","level":0,"index":2,"id":"configuring-the-odh-operator-logger_operator-log"},{"parentId":"configuring-the-odh-operator-logger_operator-log","name":"Configuring the Open Data Hub Operator logger","level":1,"index":0,"id":"configuring-the-operator-logger_operator-log"},{"parentId":"configuring-the-operator-logger_operator-log","name":"Viewing the Open Data Hub Operator log","level":2,"index":0,"id":"_viewing_the_open_data_hub_operator_log"},{"parentId":"configuring-the-odh-operator-logger_operator-log","name":"Working with certificates","level":1,"index":1,"id":"working-with-certificates_certs"},{"parentId":"working-with-certificates_certs","name":"Understanding certificates in Open Data Hub","level":2,"index":0,"id":"understanding-certificates_certs"},{"parentId":"understanding-certificates_certs","name":"How CA bundles are injected","level":3,"index":0,"id":"_how_ca_bundles_are_injected"},{"parentId":"understanding-certificates_certs","name":"How the ConfigMap is managed","level":3,"index":1,"id":"_how_the_configmap_is_managed"},{"parentId":"working-with-certificates_certs","name":"Adding a CA bundle","level":2,"index":1,"id":"adding-a-ca-bundle_certs"},{"parentId":"working-with-certificates_certs","name":"Removing a CA bundle","level":2,"index":2,"id":"removing-a-ca-bundle_certs"},{"parentId":"working-with-certificates_certs","name":"Removing a CA bundle from a namespace","level":2,"index":3,"id":"removing-a-ca-bundle-from-a-namespace_certs"},{"parentId":"working-with-certificates_certs","name":"Managing certificates","level":2,"index":4,"id":"managing-certificates_certs"},{"parentId":"working-with-certificates_certs","name":"Using self-signed certificates with Open Data Hub components","level":2,"index":5,"id":"_using_self_signed_certificates_with_open_data_hub_components"},{"parentId":"_using_self_signed_certificates_with_open_data_hub_components","name":"Using certificates with data science pipelines","level":3,"index":0,"id":"using-certificates-with-data-science-pipelines_certs"},{"parentId":"using-certificates-with-data-science-pipelines_certs","name":"Providing a CA bundle only for data science pipelines","level":4,"index":0,"id":"_providing_a_ca_bundle_only_for_data_science_pipelines"},{"parentId":"_using_self_signed_certificates_with_open_data_hub_components","name":"Using certificates with workbenches","level":3,"index":1,"id":"using-certificates-with-workbenches_certs"},{"parentId":"using-certificates-with-workbenches_certs","name":"Creating data science pipelines with Elyra and self-signed certificates","level":4,"index":0,"id":"_creating_data_science_pipelines_with_elyra_and_self_signed_certificates"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/managing-users/"},"sections":[{"parentId":null,"name":"Adding users","level":1,"index":0,"id":"adding-users"},{"parentId":"adding-users","name":"Overview of user types and permissions","level":2,"index":0,"id":"overview-of-user-types-and-permissions_managing-users"},{"parentId":"adding-users","name":"Defining Open Data Hub administrator and user groups","level":2,"index":1,"id":"defining-data-science-admin-and-user-groups_managing-users"},{"parentId":"adding-users","name":"Adding users to specialized Open Data Hub user groups","level":2,"index":2,"id":"adding-users-to-specialized-data-science-user-groups_managing-users"},{"parentId":"adding-users","name":"Viewing Open Data Hub users","level":2,"index":3,"id":"viewing-data-science-users_managing-users"},{"parentId":null,"name":"Deleting users and their resources","level":1,"index":1,"id":"deleting-users"},{"parentId":"deleting-users","name":"About deleting users and their resources","level":2,"index":0,"id":"about-deleting-users-and-resources_managing-users"},{"parentId":"deleting-users","name":"Backing up storage data","level":2,"index":1,"id":"backing-up-storage-data_managing-users"},{"parentId":"deleting-users","name":"Stopping notebook servers owned by other users","level":2,"index":2,"id":"stopping-notebook-servers-owned-by-other-users_managing-users"},{"parentId":"deleting-users","name":"Revoking user access to Jupyter","level":2,"index":3,"id":"revoking-user-access-to-jupyter_managing-users"},{"parentId":"deleting-users","name":"Cleaning up after deleting users","level":2,"index":4,"id":"cleaning-up-after-deleting-users_managing-users"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/managing-resources/"},"sections":[{"parentId":null,"name":"Customizing the dashboard","level":1,"index":0,"id":"customizing-the-dashboard"},{"parentId":"customizing-the-dashboard","name":"Editing the dashboard configuration file","level":2,"index":0,"id":"editing-the-dashboard-configuration-file_dashboard"},{"parentId":"customizing-the-dashboard","name":"Dashboard configuration options","level":2,"index":1,"id":"ref-dashboard-configuration-options_dashboard"},{"parentId":null,"name":"Managing applications that show in the dashboard","level":1,"index":1,"id":"managing-applications-that-show-in-the-dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Adding an application to the dashboard","level":2,"index":0,"id":"adding-an-application-to-the-dashboard_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Preventing users from adding applications to the dashboard","level":2,"index":1,"id":"preventing-users-from-adding-applications-to-the-dashboard_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Disabling applications connected to Open Data Hub","level":2,"index":2,"id":"disabling-applications-connected_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Showing or hiding information about enabled applications","level":2,"index":3,"id":"showing-hiding-information-about-enabled-applications_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Hiding the default Jupyter application","level":2,"index":4,"id":"hiding-the-default-jupyter-application_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Troubleshooting common problems in Jupyter for administrators","level":2,"index":5,"id":"troubleshooting-common-problems-in-jupyter-for-administrators_dashboard"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_dashboard","name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":3,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_dashboard","name":"A user&#8217;s notebook server does not start","level":3,"index":1,"id":"_a_users_notebook_server_does_not_start"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_dashboard","name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":3,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"},{"parentId":null,"name":"Managing cluster resources","level":1,"index":2,"id":"managing-cluster-resources"},{"parentId":"managing-cluster-resources","name":"Configuring the default PVC size for your cluster","level":2,"index":0,"id":"configuring-the-default-pvc-size-for-your-cluster_managing-resources"},{"parentId":"managing-cluster-resources","name":"Restoring the default PVC size for your cluster","level":2,"index":1,"id":"restoring-the-default-pvc-size-for-your-cluster_managing-resources"},{"parentId":"managing-cluster-resources","name":"Overview of accelerators","level":2,"index":2,"id":"overview-of-accelerators_managing-resources"},{"parentId":"overview-of-accelerators_managing-resources","name":"Enabling GPU support in Open Data Hub","level":3,"index":0,"id":"enabling-gpu-support_managing-resources"},{"parentId":"overview-of-accelerators_managing-resources","name":"Enabling Intel Gaudi AI accelerators","level":3,"index":1,"id":"enabling-intel-gaudi-ai-accelerators_managing-resources"},{"parentId":"managing-cluster-resources","name":"Allocating additional resources to Open Data Hub users","level":2,"index":3,"id":"allocating-additional-resources-to-data-science-users_managing-resources"},{"parentId":"managing-cluster-resources","name":"Troubleshooting common problems with distributed workloads for administrators","level":2,"index":4,"id":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-resources"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-resources","name":"A user&#8217;s Ray cluster is in a suspended state","level":3,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-resources","name":"A user&#8217;s Ray cluster is in a failed state","level":3,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-resources","name":"A user receives a <strong>failed to call webhook</strong> error message for the CodeFlare Operator","level":3,"index":2,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-resources","name":"A user receives a <strong>failed to call webhook</strong> error message for Kueue","level":3,"index":3,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-resources","name":"A user&#8217;s Ray cluster does not start","level":3,"index":4,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-resources","name":"A user receives a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":3,"index":5,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-resources","name":"A user receives a <strong>local_queue provided does not exist</strong> error message","level":3,"index":6,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-resources","name":"A user cannot create a Ray cluster or submit jobs","level":3,"index":7,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-resources","name":"The user&#8217;s pod provisioned by Kueue is terminated before the user&#8217;s image is pulled","level":3,"index":8,"id":"_the_users_pod_provisioned_by_kueue_is_terminated_before_the_users_image_is_pulled"},{"parentId":null,"name":"Managing Jupyter notebook servers","level":1,"index":3,"id":"managing-notebook-servers"},{"parentId":"managing-notebook-servers","name":"Accessing the Jupyter administration interface","level":2,"index":0,"id":"accessing-the-jupyter-administration-interface_managing-resources"},{"parentId":"managing-notebook-servers","name":"Starting notebook servers owned by other users","level":2,"index":1,"id":"starting-notebook-servers-owned-by-other-users_managing-resources"},{"parentId":"managing-notebook-servers","name":"Accessing notebook servers owned by other users","level":2,"index":2,"id":"accessing-notebook-servers-owned-by-other-users_managing-resources"},{"parentId":"managing-notebook-servers","name":"Stopping notebook servers owned by other users","level":2,"index":3,"id":"stopping-notebook-servers-owned-by-other-users_managing-resources"},{"parentId":"managing-notebook-servers","name":"Stopping idle notebooks","level":2,"index":4,"id":"stopping-idle-notebooks_managing-resources"},{"parentId":"managing-notebook-servers","name":"Adding notebook pod tolerations","level":2,"index":5,"id":"adding-notebook-pod-tolerations_managing-resources"},{"parentId":"managing-notebook-servers","name":"Configuring a custom notebook image","level":2,"index":6,"id":"configuring-a-custom-notebook-image_managing-resources"},{"parentId":null,"name":"Backing up storage data","level":1,"index":4,"id":"backing-up-storage-data_managing-resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/serving-models/"},"sections":[{"parentId":null,"name":"About model serving","level":1,"index":0,"id":"about-model-serving_about-model-serving"},{"parentId":null,"name":"Serving small and medium-sized models","level":1,"index":1,"id":"serving-small-and-medium-sized-models_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Configuring model servers","level":2,"index":0,"id":"_configuring_model_servers"},{"parentId":"_configuring_model_servers","name":"Enabling the multi-model serving platform","level":3,"index":0,"id":"enabling-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a custom model-serving runtime for the multi-model serving platform","level":3,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a model server for the multi-model serving platform","level":3,"index":2,"id":"adding-a-model-server-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Deleting a model server","level":3,"index":3,"id":"deleting-a-model-server_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Working with deployed models","level":2,"index":1,"id":"_working_with_deployed_models"},{"parentId":"_working_with_deployed_models","name":"Deploying a model by using the multi-model serving platform","level":3,"index":0,"id":"deploying-a-model-using-the-multi-model-serving-platform_model-serving"},{"parentId":"_working_with_deployed_models","name":"Viewing a deployed model","level":3,"index":1,"id":"viewing-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Updating the deployment properties of a deployed model","level":3,"index":2,"id":"updating-the-deployment-properties-of-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Deleting a deployed model","level":3,"index":3,"id":"deleting-a-deployed-model_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Configuring monitoring for the multi-model serving platform","level":2,"index":2,"id":"configuring-monitoring-for-the-multi-model-serving-platform_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Viewing model-serving runtime metrics for the multi-model serving platform","level":2,"index":3,"id":"viewing-metrics-for-the-multi-model-serving-platform_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Monitoring model performance","level":2,"index":4,"id":"_monitoring_model_performance"},{"parentId":"_monitoring_model_performance","name":"Viewing performance metrics for all models on a model server","level":3,"index":0,"id":"viewing-performance-metrics-for-model-server_model-serving"},{"parentId":"_monitoring_model_performance","name":"Viewing HTTP request metrics for a deployed model","level":3,"index":1,"id":"viewing-http-request-metrics-for-a-deployed-model_model-serving"},{"parentId":null,"name":"Serving large models","level":1,"index":2,"id":"serving-large-models_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"About the single-model serving platform","level":2,"index":0,"id":"about-the-single-model-serving-platform_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Installing KServe","level":2,"index":1,"id":"installing-kserve_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Deploying models by using the single-model serving platform","level":2,"index":2,"id":"deploying-models-using-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Enabling the single-model serving platform","level":3,"index":0,"id":"enabling-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Adding a custom model-serving runtime for the single-model serving platform","level":3,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Deploying models on the single-model serving platform","level":3,"index":2,"id":"deploying-models-on-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Accessing the inference endpoint for a deployed model","level":3,"index":3,"id":"accessing-inference-endpoint-for-deployed-model_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Configuring monitoring for the single-model serving platform","level":2,"index":3,"id":"configuring-monitoring-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Viewing model-serving runtime metrics for the single-model serving platform","level":2,"index":4,"id":"viewing-metrics-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Performance tuning on the single-model serving platform","level":2,"index":5,"id":"_performance_tuning_on_the_single_model_serving_platform"},{"parentId":"_performance_tuning_on_the_single_model_serving_platform","name":"Resolving CUDA out-of-memory errors","level":3,"index":0,"id":"resolving-cuda-oom-errors-for-the-single-model-serving-platform_serving-large-models"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/monitoring-data-science-models/"},"sections":[{"parentId":null,"name":"Enabling the TrustyAI Service for a data science project","level":1,"index":0,"id":"enabling-trustyai-service_monitor"},{"parentId":"enabling-trustyai-service_monitor","name":"Enabling the TrustyAI Service by using the dashboard","level":2,"index":0,"id":"enabling-trustyai-service-using-dashboard_monitor"},{"parentId":"enabling-trustyai-service_monitor","name":"Enabling the TrustyAI Service by using the CLI","level":2,"index":1,"id":"enabling-trustyai-service-using-cli_monitor"},{"parentId":null,"name":"Authenticating the TrustyAI service","level":1,"index":1,"id":"authenticating-trustyai-service_monitor"},{"parentId":null,"name":"Sending training data to a model","level":1,"index":2,"id":"sending-training-data-to-a-model_monitor"},{"parentId":null,"name":"Configuring bias metrics for a model","level":1,"index":3,"id":"configuring-bias-metrics-for-a-model_bias-monitoring"},{"parentId":"configuring-bias-metrics-for-a-model_bias-monitoring","name":"Creating a bias metric","level":2,"index":0,"id":"creating-a-bias-metric_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the dashboard","level":3,"index":0,"id":"creating-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the CLI","level":3,"index":1,"id":"creating-a-bias-metric-using-cli_bias-monitoring"},{"parentId":"configuring-bias-metrics-for-a-model_bias-monitoring","name":"Duplicating a bias metric","level":2,"index":1,"id":"duplicating-a-bias-metric_bias-monitoring"},{"parentId":"configuring-bias-metrics-for-a-model_bias-monitoring","name":"Deleting a bias metric","level":2,"index":2,"id":"deleting-a-bias-metric_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the dashboard","level":3,"index":0,"id":"deleting-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the CLI","level":3,"index":1,"id":"deleting-a-bias-metric-using-cli_bias-monitoring"},{"parentId":null,"name":"Viewing bias metrics for a model","level":1,"index":4,"id":"viewing-bias-metrics_monitor"},{"parentId":null,"name":"Supported bias metrics","level":1,"index":5,"id":"supported-bias-metrics_monitor"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-in-your-data-science-ide/"},"sections":[{"parentId":null,"name":"Accessing your workbench IDE","level":1,"index":0,"id":"accessing-your-workbench-ide_ide"},{"parentId":null,"name":"Working in JupyterLab","level":1,"index":1,"id":"_working_in_jupyterlab"},{"parentId":"_working_in_jupyterlab","name":"Creating and importing notebooks","level":2,"index":0,"id":"creating-and-importing-notebooks_ide"},{"parentId":"creating-and-importing-notebooks_ide","name":"Creating a new notebook","level":3,"index":0,"id":"creating-a-new-notebook_ide"},{"parentId":"creating-and-importing-notebooks_ide","name":"Uploading an existing notebook file from local storage","level":3,"index":1,"id":"uploading-an-existing-notebook-file-from-local-storage_ide"},{"parentId":"creating-and-importing-notebooks_ide","name":"Additional resources","level":3,"index":2,"id":"_additional_resources"},{"parentId":"_working_in_jupyterlab","name":"Collaborating on notebooks by using Git","level":2,"index":1,"id":"collaborating-on-notebooks-by-using-git_ide"},{"parentId":"collaborating-on-notebooks-by-using-git_ide","name":"Uploading an existing notebook file from a Git repository by using JupyterLab","level":3,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_ide"},{"parentId":"collaborating-on-notebooks-by-using-git_ide","name":"Uploading an existing notebook file from a Git repository by using the command line interface","level":3,"index":1,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_ide"},{"parentId":"collaborating-on-notebooks-by-using-git_ide","name":"Updating your project with changes from a remote Git repository","level":3,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_ide"},{"parentId":"collaborating-on-notebooks-by-using-git_ide","name":"Pushing project changes to a Git repository","level":3,"index":3,"id":"pushing-project-changes-to-a-git-repository_ide"},{"parentId":"_working_in_jupyterlab","name":"Managing Python packages","level":2,"index":2,"id":"managing-python-packages_ide"},{"parentId":"managing-python-packages_ide","name":"Viewing Python packages installed on your notebook server","level":3,"index":0,"id":"viewing-python-packages-installed-on-your-notebook-server_ide"},{"parentId":"managing-python-packages_ide","name":"Installing Python packages on your notebook server","level":3,"index":1,"id":"installing-python-packages-on-your-notebook-server_ide"},{"parentId":"_working_in_jupyterlab","name":"Troubleshooting common problems in Jupyter for users","level":2,"index":3,"id":"troubleshooting-common-problems-in-jupyter-for-users_ide"},{"parentId":null,"name":"Working in code-server","level":1,"index":2,"id":"working-in-code-server_ide"},{"parentId":"working-in-code-server_ide","name":"Installing extensions with code-server","level":2,"index":0,"id":"_installing_extensions_with_code_server"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/upgrading-open-data-hub/"},"sections":[{"parentId":null,"name":"Overview of upgrading Open Data Hub","level":1,"index":0,"id":"overview-of-upgrading-odh_upgrade"},{"parentId":null,"name":"Requirements for upgrading Open Data Hub","level":1,"index":1,"id":"requirements-for-upgrading-odh_upgrade"},{"parentId":null,"name":"Upgrading Open Data Hub version 1 to version 2","level":1,"index":2,"id":"upgrading-odh-v1-to-v2_upgradev1"},{"parentId":"upgrading-odh-v1-to-v2_upgradev1","name":"Upgrading the Open Data Hub Operator version 1","level":2,"index":0,"id":"upgrading-the-odh-operator-v1_upgradev1"},{"parentId":"upgrading-odh-v1-to-v2_upgradev1","name":"Installing Open Data Hub components","level":2,"index":1,"id":"installing-odh-components_upgradev1"},{"parentId":"upgrading-odh-v1-to-v2_upgradev1","name":"Accessing the Open Data Hub dashboard","level":2,"index":2,"id":"accessing-the-odh-dashboard_upgradev1"},{"parentId":null,"name":"Upgrading Open Data Hub version 2.0 to version 2.2","level":1,"index":3,"id":"upgrading-odh-v2_upgradev2"},{"parentId":"upgrading-odh-v2_upgradev2","name":"Installing Open Data Hub version 2","level":2,"index":0,"id":"installing-odh-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing the Open Data Hub Operator version 2","level":3,"index":0,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing Open Data Hub components","level":3,"index":1,"id":"installing-odh-components_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Accessing the Open Data Hub dashboard","level":3,"index":2,"id":"accessing-the-odh-dashboard_installv2"},{"parentId":"upgrading-odh-v2_upgradev2","name":"Adding a CA bundle after upgrading","level":2,"index":1,"id":"adding-a-ca-bundle-after-upgrading_upgradev2"},{"parentId":null,"name":"Adding a CA bundle after upgrading","level":1,"index":4,"id":"adding-a-ca-bundle-after-upgrading_upgradev2"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-accelerators/"},"sections":[{"parentId":null,"name":"Overview of accelerators","level":1,"index":0,"id":"overview-of-accelerators_accelerators"},{"parentId":null,"name":"Working with accelerator profiles","level":1,"index":1,"id":"working-with-accelerator-profiles_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Creating an accelerator profile","level":2,"index":0,"id":"creating-an-accelerator-profile_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Updating an accelerator profile","level":2,"index":1,"id":"updating-an-accelerator-profile_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Deleting an accelerator profile","level":2,"index":2,"id":"deleting-an-accelerator-profile_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Viewing accelerator profiles","level":2,"index":3,"id":"viewing-accelerator-profiles_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Configuring a recommended accelerator for notebook images","level":2,"index":4,"id":"configuring-a-recommended-accelerator-for-notebook-images_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Configuring a recommended accelerator for serving runtimes","level":2,"index":5,"id":"configuring-a-recommended-accelerator-for-serving-runtimes_accelerators"},{"parentId":null,"name":"Intel Gaudi AI Accelerator integration","level":1,"index":2,"id":"intel-gaudi-ai-accelerator-integration_accelerators"},{"parentId":"intel-gaudi-ai-accelerator-integration_accelerators","name":"Enabling Intel Gaudi AI accelerators","level":2,"index":0,"id":"enabling-intel-gaudi-ai-accelerators_accelerators"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-on-data-science-projects/"},"sections":[{"parentId":null,"name":"Using data science projects","level":1,"index":0,"id":"using-data-science-projects_projects"},{"parentId":"using-data-science-projects_projects","name":"Creating a data science project","level":2,"index":0,"id":"creating-a-data-science-project_projects"},{"parentId":"using-data-science-projects_projects","name":"Updating a data science project","level":2,"index":1,"id":"updating-a-data-science-project_projects"},{"parentId":"using-data-science-projects_projects","name":"Deleting a data science project","level":2,"index":2,"id":"deleting-a-data-science-project_projects"},{"parentId":null,"name":"Using project workbenches","level":1,"index":1,"id":"using-project-workbenches_projects"},{"parentId":"using-project-workbenches_projects","name":"Creating a workbench and selecting an IDE","level":2,"index":0,"id":"creating-a-workbench-select-ide_projects"},{"parentId":"creating-a-workbench-select-ide_projects","name":"About workbench images","level":3,"index":0,"id":"about-workbench-images_projects"},{"parentId":"creating-a-workbench-select-ide_projects","name":"Creating a workbench","level":3,"index":1,"id":"creating-a-project-workbench_projects"},{"parentId":"using-project-workbenches_projects","name":"Starting a workbench","level":2,"index":1,"id":"starting-a-workbench_projects"},{"parentId":"using-project-workbenches_projects","name":"Updating a project workbench","level":2,"index":2,"id":"updating-a-project-workbench_projects"},{"parentId":"using-project-workbenches_projects","name":"Deleting a workbench from a data science project","level":2,"index":3,"id":"deleting-a-workbench-from-a-data-science-project_projects"},{"parentId":null,"name":"Using data connections","level":1,"index":2,"id":"using-data-connections_projects"},{"parentId":"using-data-connections_projects","name":"Adding a data connection to your data science project","level":2,"index":0,"id":"adding-a-data-connection-to-your-data-science-project_projects"},{"parentId":"using-data-connections_projects","name":"Deleting a data connection","level":2,"index":1,"id":"deleting-a-data-connection_projects"},{"parentId":"using-data-connections_projects","name":"Updating a connected data source","level":2,"index":2,"id":"updating-a-connected-data-source_projects"},{"parentId":null,"name":"Configuring cluster storage","level":1,"index":3,"id":"configuring-cluster-storage_projects"},{"parentId":"configuring-cluster-storage_projects","name":"Adding cluster storage to your data science project","level":2,"index":0,"id":"adding-cluster-storage-to-your-data-science-project_projects"},{"parentId":"configuring-cluster-storage_projects","name":"Updating cluster storage","level":2,"index":1,"id":"updating-cluster-storage_projects"},{"parentId":"configuring-cluster-storage_projects","name":"Deleting cluster storage from a data science project","level":2,"index":2,"id":"deleting-cluster-storage-from-a-data-science-project_projects"},{"parentId":null,"name":"Managing access to data science projects","level":1,"index":4,"id":"managing-access-to-data-science-projects_projects"},{"parentId":"managing-access-to-data-science-projects_projects","name":"Configuring access to a data science project","level":2,"index":0,"id":"configuring-access-to-a-data-science-project_projects"},{"parentId":"managing-access-to-data-science-projects_projects","name":"Sharing access to a data science project","level":2,"index":1,"id":"sharing-access-to-a-data-science-project_projects"},{"parentId":"managing-access-to-data-science-projects_projects","name":"Updating access to a data science project","level":2,"index":2,"id":"updating-access-to-a-data-science-project_projects"},{"parentId":"managing-access-to-data-science-projects_projects","name":"Removing access to a data science project","level":2,"index":3,"id":"removing-access-to-a-data-science-project_projects"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-connected-applications/"},"sections":[{"parentId":null,"name":"Viewing applications that are connected to Open Data Hub","level":1,"index":0,"id":"viewing-connected-applications_connected-apps"},{"parentId":null,"name":"Enabling applications that are connected to Open Data Hub","level":1,"index":1,"id":"enabling-applications-connected_connected-apps"},{"parentId":null,"name":"Removing disabled applications from the dashboard","level":1,"index":2,"id":"removing-disabled-applications_connected-apps"},{"parentId":null,"name":"Using the Jupyter application","level":1,"index":3,"id":"_using_the_jupyter_application"},{"parentId":"_using_the_jupyter_application","name":"Starting a Jupyter notebook server","level":2,"index":0,"id":"starting-a-jupyter-notebook-server_connected-apps"},{"parentId":"_using_the_jupyter_application","name":"Creating and importing notebooks","level":2,"index":1,"id":"creating-and-importing-notebooks_connected-apps"},{"parentId":"creating-and-importing-notebooks_connected-apps","name":"Creating a new notebook","level":3,"index":0,"id":"creating-a-new-notebook_connected-apps"},{"parentId":"creating-and-importing-notebooks_connected-apps","name":"Uploading an existing notebook file from local storage","level":3,"index":1,"id":"uploading-an-existing-notebook-file-from-local-storage_connected-apps"},{"parentId":"creating-and-importing-notebooks_connected-apps","name":"Additional resources","level":3,"index":2,"id":"_additional_resources"},{"parentId":"_using_the_jupyter_application","name":"Collaborating on notebooks by using Git","level":2,"index":2,"id":"collaborating-on-notebooks-by-using-git_connected-apps"},{"parentId":"collaborating-on-notebooks-by-using-git_connected-apps","name":"Uploading an existing notebook file from a Git repository by using JupyterLab","level":3,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_connected-apps"},{"parentId":"collaborating-on-notebooks-by-using-git_connected-apps","name":"Uploading an existing notebook file from a Git repository by using the command line interface","level":3,"index":1,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_connected-apps"},{"parentId":"collaborating-on-notebooks-by-using-git_connected-apps","name":"Updating your project with changes from a remote Git repository","level":3,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_connected-apps"},{"parentId":"collaborating-on-notebooks-by-using-git_connected-apps","name":"Pushing project changes to a Git repository","level":3,"index":3,"id":"pushing-project-changes-to-a-git-repository_connected-apps"},{"parentId":"_using_the_jupyter_application","name":"Managing Python packages","level":2,"index":3,"id":"managing-python-packages_connected-apps"},{"parentId":"managing-python-packages_connected-apps","name":"Viewing Python packages installed on your notebook server","level":3,"index":0,"id":"viewing-python-packages-installed-on-your-notebook-server_connected-apps"},{"parentId":"managing-python-packages_connected-apps","name":"Installing Python packages on your notebook server","level":3,"index":1,"id":"installing-python-packages-on-your-notebook-server_connected-apps"},{"parentId":"_using_the_jupyter_application","name":"Updating notebook server settings by restarting your server","level":2,"index":4,"id":"updating-notebook-server-settings-by-restarting-your-server_connected-apps"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Enabling data science pipelines 2.0","level":1,"index":0,"id":"enabling-data-science-pipelines-2_ds-pipelines"},{"parentId":"enabling-data-science-pipelines-2_ds-pipelines","name":"Installing Open Data Hub with data science pipelines 2.0","level":2,"index":0,"id":"_installing_open_data_hub_with_data_science_pipelines_2_0"},{"parentId":"enabling-data-science-pipelines-2_ds-pipelines","name":"Upgrading to data science pipelines 2.0","level":2,"index":1,"id":"_upgrading_to_data_science_pipelines_2_0"},{"parentId":null,"name":"Managing data science pipelines","level":1,"index":1,"id":"managing-data-science-pipelines_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Configuring a pipeline server","level":2,"index":0,"id":"configuring-a-pipeline-server_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Defining a pipeline","level":2,"index":1,"id":"defining-a-pipeline_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Importing a data science pipeline","level":2,"index":2,"id":"importing-a-data-science-pipeline_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Deleting a data science pipeline","level":2,"index":3,"id":"deleting-a-data-science-pipeline_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Deleting a pipeline server","level":2,"index":4,"id":"deleting-a-pipeline-server_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Viewing the details of a pipeline server","level":2,"index":5,"id":"viewing-the-details-of-a-pipeline-server_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Viewing existing pipelines","level":2,"index":6,"id":"viewing-existing-pipelines_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Overview of pipeline versions","level":2,"index":7,"id":"overview-of-pipeline-versions_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Uploading a pipeline version","level":2,"index":8,"id":"uploading-a-pipeline-version_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Deleting a pipeline version","level":2,"index":9,"id":"deleting-a-pipeline-version_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Viewing the details of a pipeline version","level":2,"index":10,"id":"viewing-the-details-of-a-pipeline-version_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Downloading a data science pipeline version","level":2,"index":11,"id":"downloading-a-data-science-pipeline-version_ds-pipelines"},{"parentId":null,"name":"Managing pipeline experiments","level":1,"index":2,"id":"managing-pipeline-experiments_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Overview of pipeline experiments","level":2,"index":0,"id":"overview-of-pipeline-experiments_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Creating a pipeline experiment","level":2,"index":1,"id":"creating-a-pipeline-experiment_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Archiving a pipeline experiment","level":2,"index":2,"id":"archiving-a-pipeline-experiment_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Deleting an archived pipeline experiment","level":2,"index":3,"id":"deleting-an-archived-pipeline-experiment_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Restoring an archived pipeline experiment","level":2,"index":4,"id":"restoring-an-archived-pipeline-experiment_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Viewing pipeline task executions","level":2,"index":5,"id":"viewing-pipeline-task-executions_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Viewing pipeline artifacts","level":2,"index":6,"id":"viewing-pipeline-artifacts_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Comparing runs","level":2,"index":7,"id":"comparing-runs_ds-pipelines"},{"parentId":null,"name":"Managing pipeline runs","level":1,"index":3,"id":"managing-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Overview of pipeline runs","level":2,"index":0,"id":"overview-of-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Storing data with data science pipelines","level":2,"index":1,"id":"storing-data-with-data-science-pipelines_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Viewing active pipeline runs","level":2,"index":2,"id":"viewing-active-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Executing a pipeline run","level":2,"index":3,"id":"executing-a-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Stopping an active pipeline run","level":2,"index":4,"id":"stopping-an-active-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Duplicating an active pipeline run","level":2,"index":5,"id":"duplicating-an-active-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Viewing scheduled pipeline runs","level":2,"index":6,"id":"viewing-scheduled-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Scheduling a pipeline run using a cron job","level":2,"index":7,"id":"scheduling-a-pipeline-run-using-a-cron-job_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Scheduling a pipeline run","level":2,"index":8,"id":"scheduling-a-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Duplicating a scheduled pipeline run","level":2,"index":9,"id":"duplicating-a-scheduled-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Deleting a scheduled pipeline run","level":2,"index":10,"id":"deleting-a-scheduled-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Viewing the details of a pipeline run","level":2,"index":11,"id":"viewing-the-details-of-a-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Viewing archived pipeline runs","level":2,"index":12,"id":"viewing-archived-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Archiving a pipeline run","level":2,"index":13,"id":"archiving-a-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Restoring an archived pipeline run","level":2,"index":14,"id":"restoring-an-archived-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Deleting an archived pipeline run","level":2,"index":15,"id":"deleting-an-archived-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Duplicating an archived pipeline run","level":2,"index":16,"id":"duplicating-an-archived-pipeline-run_ds-pipelines"},{"parentId":null,"name":"Working with pipeline logs","level":1,"index":4,"id":"working-with-pipeline-logs_ds-pipelines"},{"parentId":"working-with-pipeline-logs_ds-pipelines","name":"About pipeline logs","level":2,"index":0,"id":"about-pipeline-logs_ds-pipelines"},{"parentId":"working-with-pipeline-logs_ds-pipelines","name":"Viewing pipeline step logs","level":2,"index":1,"id":"viewing-pipeline-step-logs_ds-pipelines"},{"parentId":"working-with-pipeline-logs_ds-pipelines","name":"Downloading pipeline step logs","level":2,"index":2,"id":"downloading-pipeline-step-logs_ds-pipelines"},{"parentId":null,"name":"Working with pipelines in JupyterLab","level":1,"index":5,"id":"working-with-pipelines-in-jupyterlab_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Overview of pipelines in JupyterLab","level":2,"index":0,"id":"overview-of-pipelines-in-jupyterlab_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Accessing the pipeline editor","level":2,"index":1,"id":"accessing-the-pipeline-editor_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Creating a runtime configuration","level":2,"index":2,"id":"creating-a-runtime-configuration_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Updating a runtime configuration","level":2,"index":3,"id":"updating-a-runtime-configuration_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Deleting a runtime configuration","level":2,"index":4,"id":"deleting-a-runtime-configuration_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Duplicating a runtime configuration","level":2,"index":5,"id":"duplicating-a-runtime-configuration_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Running a pipeline in JupyterLab","level":2,"index":6,"id":"running-a-pipeline-in-jupyterlab_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Exporting a pipeline in JupyterLab","level":2,"index":7,"id":"exporting-a-pipeline-in-jupyterlab_ds-pipelines"},{"parentId":null,"name":"Additional resources","level":1,"index":6,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-distributed-workloads/"},"sections":[{"parentId":null,"name":"Overview of distributed workloads","level":1,"index":0,"id":"overview-of-distributed-workloads_distributed-workloads"},{"parentId":"overview-of-distributed-workloads_distributed-workloads","name":"Overview of Kueue resources","level":2,"index":0,"id":"overview-of-kueue-resources_distributed-workloads"},{"parentId":"overview-of-kueue-resources_distributed-workloads","name":"Resource flavor","level":3,"index":0,"id":"_resource_flavor"},{"parentId":"overview-of-kueue-resources_distributed-workloads","name":"Cluster queue","level":3,"index":1,"id":"_cluster_queue"},{"parentId":"overview-of-kueue-resources_distributed-workloads","name":"Local queue","level":3,"index":2,"id":"_local_queue"},{"parentId":null,"name":"Configuring distributed workloads","level":1,"index":1,"id":"configuring-distributed-workloads_distributed-workloads"},{"parentId":"configuring-distributed-workloads_distributed-workloads","name":"Configuring the distributed workloads components","level":2,"index":0,"id":"configuring-the-distributed-workloads-components_distributed-workloads"},{"parentId":"configuring-distributed-workloads_distributed-workloads","name":"Configuring quota management for distributed workloads","level":2,"index":1,"id":"configuring-quota-management-for-distributed-workloads_distributed-workloads"},{"parentId":null,"name":"Running distributed workloads","level":1,"index":2,"id":"running-distributed-workloads_distributed-workloads"},{"parentId":"running-distributed-workloads_distributed-workloads","name":"Running distributed data science workloads from notebooks","level":2,"index":0,"id":"running-distributed-data-science-workloads-from-notebooks_distributed-workloads"},{"parentId":"running-distributed-workloads_distributed-workloads","name":"Running distributed data science workloads from data science pipelines","level":2,"index":1,"id":"running-distributed-data-science-workloads-from-ds-pipelines_distributed-workloads"},{"parentId":null,"name":"Monitoring distributed workloads","level":1,"index":3,"id":"monitoring-distributed-workloads_distributed-workloads"},{"parentId":"monitoring-distributed-workloads_distributed-workloads","name":"Viewing project metrics for distributed workloads","level":2,"index":0,"id":"viewing-project-metrics-for-distributed-workloads_distributed-workloads"},{"parentId":"monitoring-distributed-workloads_distributed-workloads","name":"Viewing the status of distributed workloads","level":2,"index":1,"id":"viewing-the-status-of-distributed-workloads_distributed-workloads"},{"parentId":null,"name":"Tuning a model by using the Training Operator","level":1,"index":4,"id":"tuning-a-model-by-using-the-training-operator_distributed-workloads"},{"parentId":"tuning-a-model-by-using-the-training-operator_distributed-workloads","name":"Configuring the Training Operator permissions when not using Kueue","level":2,"index":0,"id":"configuring-the-training-operator-permissions-when-not-using-kueue_distributed-workloads"},{"parentId":"tuning-a-model-by-using-the-training-operator_distributed-workloads","name":"Configuring the training job","level":2,"index":1,"id":"configuring-the-training-job_distributed-workloads"},{"parentId":"tuning-a-model-by-using-the-training-operator_distributed-workloads","name":"Running the training job","level":2,"index":2,"id":"running-the-training-job_distributed-workloads"},{"parentId":"tuning-a-model-by-using-the-training-operator_distributed-workloads","name":"Monitoring the training job","level":2,"index":3,"id":"monitoring-the-training-job_distributed-workloads"},{"parentId":null,"name":"Troubleshooting common problems with distributed workloads for users","level":1,"index":5,"id":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My Ray cluster is in a suspended state","level":2,"index":0,"id":"_my_ray_cluster_is_in_a_suspended_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My Ray cluster is in a failed state","level":2,"index":1,"id":"_my_ray_cluster_is_in_a_failed_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a <strong>failed to call webhook</strong> error message for the CodeFlare Operator","level":2,"index":2,"id":"_i_see_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a <strong>failed to call webhook</strong> error message for Kueue","level":2,"index":3,"id":"_i_see_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My Ray cluster doesn&#8217;t start","level":2,"index":4,"id":"_my_ray_cluster_doesnt_start"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":2,"index":5,"id":"_i_see_a_default_local_queue_not_found_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a <strong>local_queue provided does not exist</strong> error message","level":2,"index":6,"id":"_i_see_a_local_queue_provided_does_not_exist_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I cannot create a Ray cluster or submit jobs","level":2,"index":7,"id":"_i_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My pod provisioned by Kueue is terminated before my image is pulled","level":2,"index":8,"id":"_my_pod_provisioned_by_kueue_is_terminated_before_my_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/_artifacts/document-attributes-global/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/adding-users/"},"sections":[{"parentId":null,"name":"Overview of user types and permissions","level":1,"index":0,"id":"overview-of-user-types-and-permissions_{context}"},{"parentId":null,"name":"Defining {productname-short} administrator and user groups","level":1,"index":1,"id":"defining-data-science-admin-and-user-groups_{context}"},{"parentId":null,"name":"Adding users to specialized {productname-short} user groups","level":1,"index":2,"id":"adding-users-to-specialized-data-science-user-groups_{context}"},{"parentId":null,"name":"Viewing {productname-short} users","level":1,"index":3,"id":"viewing-data-science-users_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/collaborating-on-notebooks-by-using-git/"},"sections":[{"parentId":null,"name":"Uploading an existing notebook file from a Git repository by using JupyterLab","level":1,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_{context}"},{"parentId":null,"name":"Uploading an existing notebook file from a Git repository by using the command line interface","level":1,"index":1,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_{context}"},{"parentId":null,"name":"Updating your project with changes from a remote Git repository","level":1,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_{context}"},{"parentId":null,"name":"Pushing project changes to a Git repository","level":1,"index":3,"id":"pushing-project-changes-to-a-git-repository_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-bias-metrics-for-a-model/"},"sections":[{"parentId":null,"name":"Creating a bias metric","level":1,"index":0,"id":"creating-a-bias-metric_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the dashboard","level":2,"index":0,"id":"creating-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the CLI","level":2,"index":1,"id":"creating-a-bias-metric-using-cli_bias-monitoring"},{"parentId":null,"name":"Duplicating a bias metric","level":1,"index":1,"id":"duplicating-a-bias-metric_bias-monitoring"},{"parentId":null,"name":"Deleting a bias metric","level":1,"index":2,"id":"deleting-a-bias-metric_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the dashboard","level":2,"index":0,"id":"deleting-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the CLI","level":2,"index":1,"id":"deleting-a-bias-metric-using-cli_bias-monitoring"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-cluster-storage/"},"sections":[{"parentId":null,"name":"Adding cluster storage to your data science project","level":1,"index":0,"id":"adding-cluster-storage-to-your-data-science-project_{context}"},{"parentId":null,"name":"Updating cluster storage","level":1,"index":1,"id":"updating-cluster-storage_{context}"},{"parentId":null,"name":"Deleting cluster storage from a data science project","level":1,"index":2,"id":"deleting-cluster-storage-from-a-data-science-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-distributed-workloads/"},"sections":[{"parentId":null,"name":"Configuring the distributed workloads components","level":1,"index":0,"id":"configuring-the-distributed-workloads-components_{context}"},{"parentId":null,"name":"Configuring quota management for distributed workloads","level":1,"index":1,"id":"configuring-quota-management-for-distributed-workloads_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-the-odh-operator-logger/"},"sections":[{"parentId":null,"name":"Configuring the {productname-short} Operator logger","level":1,"index":0,"id":"configuring-the-operator-logger_operator-log"},{"parentId":"configuring-the-operator-logger_operator-log","name":"Viewing the {productname-short} Operator log","level":2,"index":0,"id":"_viewing_the_productname_short_operator_log"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/creating-and-importing-notebooks/"},"sections":[{"parentId":null,"name":"Creating a new notebook","level":1,"index":0,"id":"creating-a-new-notebook_{context}"},{"parentId":null,"name":"Uploading an existing notebook file from local storage","level":1,"index":1,"id":"uploading-an-existing-notebook-file-from-local-storage_{context}"},{"parentId":null,"name":"Additional resources","level":1,"index":2,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/customizing-the-dashboard/"},"sections":[{"parentId":null,"name":"Editing the dashboard configuration file","level":1,"index":0,"id":"editing-the-dashboard-configuration-file_dashboard"},{"parentId":null,"name":"Dashboard configuration options","level":1,"index":1,"id":"ref-dashboard-configuration-options_dashboard"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/deleting-users/"},"sections":[{"parentId":null,"name":"About deleting users and their resources","level":1,"index":0,"id":"about-deleting-users-and-resources_{context}"},{"parentId":null,"name":"Backing up storage data","level":1,"index":1,"id":"backing-up-storage-data_{context}"},{"parentId":null,"name":"Stopping notebook servers owned by other users","level":1,"index":2,"id":"stopping-notebook-servers-owned-by-other-users_{context}"},{"parentId":null,"name":"Revoking user access to Jupyter","level":1,"index":3,"id":"revoking-user-access-to-jupyter_{context}"},{"parentId":null,"name":"Cleaning up after deleting users","level":1,"index":4,"id":"cleaning-up-after-deleting-users_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/installing-odh-v1/"},"sections":[{"parentId":null,"name":"Installing the Open Data Hub Operator version 1","level":1,"index":0,"id":"installing-the-odh-operator-v1_installv1"},{"parentId":null,"name":"Creating a new project for your Open Data Hub instance","level":1,"index":1,"id":"creating-a-new-project-for-your-odh-instance_installv1"},{"parentId":null,"name":"Adding an Open Data Hub instance","level":1,"index":2,"id":"adding-an-odh-instance_installv1"},{"parentId":null,"name":"Accessing the Open Data Hub dashboard","level":1,"index":3,"id":"accessing-the-odh-dashboard_installv1"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/installing-odh-v2/"},"sections":[{"parentId":null,"name":"Installing the Open Data Hub Operator version 2","level":1,"index":0,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":null,"name":"Installing Open Data Hub components","level":1,"index":1,"id":"installing-odh-components_installv2"},{"parentId":null,"name":"Accessing the Open Data Hub dashboard","level":1,"index":2,"id":"accessing-the-odh-dashboard_installv2"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-access-to-data-science-projects/"},"sections":[{"parentId":null,"name":"Configuring access to a data science project","level":1,"index":0,"id":"configuring-access-to-a-data-science-project_{context}"},{"parentId":null,"name":"Sharing access to a data science project","level":1,"index":1,"id":"sharing-access-to-a-data-science-project_{context}"},{"parentId":null,"name":"Updating access to a data science project","level":1,"index":2,"id":"updating-access-to-a-data-science-project_{context}"},{"parentId":null,"name":"Removing access to a data science project","level":1,"index":3,"id":"removing-access-to-a-data-science-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-applications-that-show-in-the-dashboard/"},"sections":[{"parentId":null,"name":"Adding an application to the dashboard","level":1,"index":0,"id":"adding-an-application-to-the-dashboard_dashboard"},{"parentId":null,"name":"Preventing users from adding applications to the dashboard","level":1,"index":1,"id":"preventing-users-from-adding-applications-to-the-dashboard_dashboard"},{"parentId":null,"name":"Disabling applications connected to {productname-short}","level":1,"index":2,"id":"disabling-applications-connected_dashboard"},{"parentId":null,"name":"Showing or hiding information about enabled applications","level":1,"index":3,"id":"showing-hiding-information-about-enabled-applications_dashboard"},{"parentId":null,"name":"Hiding the default Jupyter application","level":1,"index":4,"id":"hiding-the-default-jupyter-application_dashboard"},{"parentId":null,"name":"Troubleshooting common problems in Jupyter for administrators","level":1,"index":5,"id":"troubleshooting-common-problems-in-jupyter-for-administrators_dashboard"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_dashboard","name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":2,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_dashboard","name":"A user&#8217;s notebook server does not start","level":2,"index":1,"id":"_a_users_notebook_server_does_not_start"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_dashboard","name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":2,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-cluster-resources/"},"sections":[{"parentId":null,"name":"Configuring the default PVC size for your cluster","level":1,"index":0,"id":"configuring-the-default-pvc-size-for-your-cluster_{context}"},{"parentId":null,"name":"Restoring the default PVC size for your cluster","level":1,"index":1,"id":"restoring-the-default-pvc-size-for-your-cluster_{context}"},{"parentId":null,"name":"Overview of accelerators","level":1,"index":2,"id":"overview-of-accelerators_{context}"},{"parentId":"overview-of-accelerators_{context}","name":"Enabling GPU support in {productname-short}","level":2,"index":0,"id":"enabling-gpu-support_{context}"},{"parentId":"overview-of-accelerators_{context}","name":"Enabling Intel Gaudi AI accelerators","level":2,"index":1,"id":"enabling-intel-gaudi-ai-accelerators_{context}"},{"parentId":null,"name":"Allocating additional resources to {productname-short} users","level":1,"index":3,"id":"allocating-additional-resources-to-data-science-users_{context}"},{"parentId":null,"name":"Troubleshooting common problems with distributed workloads for administrators","level":1,"index":4,"id":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user&#8217;s Ray cluster is in a suspended state","level":2,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user&#8217;s Ray cluster is in a failed state","level":2,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user receives a <strong>failed to call webhook</strong> error message for the CodeFlare Operator","level":2,"index":2,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user receives a <strong>failed to call webhook</strong> error message for Kueue","level":2,"index":3,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user&#8217;s Ray cluster does not start","level":2,"index":4,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user receives a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":2,"index":5,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user receives a <strong>local_queue provided does not exist</strong> error message","level":2,"index":6,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user cannot create a Ray cluster or submit jobs","level":2,"index":7,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"The user&#8217;s pod provisioned by Kueue is terminated before the user&#8217;s image is pulled","level":2,"index":8,"id":"_the_users_pod_provisioned_by_kueue_is_terminated_before_the_users_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Configuring a pipeline server","level":1,"index":0,"id":"configuring-a-pipeline-server_{context}"},{"parentId":null,"name":"Defining a pipeline","level":1,"index":1,"id":"defining-a-pipeline_{context}"},{"parentId":null,"name":"Importing a data science pipeline","level":1,"index":2,"id":"importing-a-data-science-pipeline_{context}"},{"parentId":null,"name":"Deleting a data science pipeline","level":1,"index":3,"id":"deleting-a-data-science-pipeline_{context}"},{"parentId":null,"name":"Deleting a pipeline server","level":1,"index":4,"id":"deleting-a-pipeline-server_{context}"},{"parentId":null,"name":"Viewing the details of a pipeline server","level":1,"index":5,"id":"viewing-the-details-of-a-pipeline-server_{context}"},{"parentId":null,"name":"Viewing existing pipelines","level":1,"index":6,"id":"viewing-existing-pipelines_{context}"},{"parentId":null,"name":"Overview of pipeline versions","level":1,"index":7,"id":"overview-of-pipeline-versions_{context}"},{"parentId":null,"name":"Uploading a pipeline version","level":1,"index":8,"id":"uploading-a-pipeline-version_{context}"},{"parentId":null,"name":"Deleting a pipeline version","level":1,"index":9,"id":"deleting-a-pipeline-version_{context}"},{"parentId":null,"name":"Viewing the details of a pipeline version","level":1,"index":10,"id":"viewing-the-details-of-a-pipeline-version_{context}"},{"parentId":null,"name":"Downloading a data science pipeline version","level":1,"index":11,"id":"downloading-a-data-science-pipeline-version_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-notebook-servers/"},"sections":[{"parentId":null,"name":"Accessing the Jupyter administration interface","level":1,"index":0,"id":"accessing-the-jupyter-administration-interface_{context}"},{"parentId":null,"name":"Starting notebook servers owned by other users","level":1,"index":1,"id":"starting-notebook-servers-owned-by-other-users_{context}"},{"parentId":null,"name":"Accessing notebook servers owned by other users","level":1,"index":2,"id":"accessing-notebook-servers-owned-by-other-users_{context}"},{"parentId":null,"name":"Stopping notebook servers owned by other users","level":1,"index":3,"id":"stopping-notebook-servers-owned-by-other-users_{context}"},{"parentId":null,"name":"Stopping idle notebooks","level":1,"index":4,"id":"stopping-idle-notebooks_{context}"},{"parentId":null,"name":"Adding notebook pod tolerations","level":1,"index":5,"id":"adding-notebook-pod-tolerations_{context}"},{"parentId":null,"name":"Configuring a custom notebook image","level":1,"index":6,"id":"configuring-a-custom-notebook-image_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-pipeline-experiments/"},"sections":[{"parentId":null,"name":"Overview of pipeline experiments","level":1,"index":0,"id":"overview-of-pipeline-experiments_{context}"},{"parentId":null,"name":"Creating a pipeline experiment","level":1,"index":1,"id":"creating-a-pipeline-experiment_{context}"},{"parentId":null,"name":"Archiving a pipeline experiment","level":1,"index":2,"id":"archiving-a-pipeline-experiment_{context}"},{"parentId":null,"name":"Deleting an archived pipeline experiment","level":1,"index":3,"id":"deleting-an-archived-pipeline-experiment_{context}"},{"parentId":null,"name":"Restoring an archived pipeline experiment","level":1,"index":4,"id":"restoring-an-archived-pipeline-experiment_{context}"},{"parentId":null,"name":"Viewing pipeline task executions","level":1,"index":5,"id":"viewing-pipeline-task-executions_{context}"},{"parentId":null,"name":"Viewing pipeline artifacts","level":1,"index":6,"id":"viewing-pipeline-artifacts_{context}"},{"parentId":null,"name":"Comparing runs","level":1,"index":7,"id":"comparing-runs_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-pipeline-runs/"},"sections":[{"parentId":null,"name":"Overview of pipeline runs","level":1,"index":0,"id":"overview-of-pipeline-runs_{context}"},{"parentId":null,"name":"Storing data with data science pipelines","level":1,"index":1,"id":"storing-data-with-data-science-pipelines_{context}"},{"parentId":null,"name":"Viewing active pipeline runs","level":1,"index":2,"id":"viewing-active-pipeline-runs_{context}"},{"parentId":null,"name":"Executing a pipeline run","level":1,"index":3,"id":"executing-a-pipeline-run_{context}"},{"parentId":null,"name":"Stopping an active pipeline run","level":1,"index":4,"id":"stopping-an-active-pipeline-run_{context}"},{"parentId":null,"name":"Duplicating an active pipeline run","level":1,"index":5,"id":"duplicating-an-active-pipeline-run_{context}"},{"parentId":null,"name":"Viewing scheduled pipeline runs","level":1,"index":6,"id":"viewing-scheduled-pipeline-runs_{context}"},{"parentId":null,"name":"Scheduling a pipeline run using a cron job","level":1,"index":7,"id":"scheduling-a-pipeline-run-using-a-cron-job_{context}"},{"parentId":null,"name":"Scheduling a pipeline run","level":1,"index":8,"id":"scheduling-a-pipeline-run_{context}"},{"parentId":null,"name":"Duplicating a scheduled pipeline run","level":1,"index":9,"id":"duplicating-a-scheduled-pipeline-run_{context}"},{"parentId":null,"name":"Deleting a scheduled pipeline run","level":1,"index":10,"id":"deleting-a-scheduled-pipeline-run_{context}"},{"parentId":null,"name":"Viewing the details of a pipeline run","level":1,"index":11,"id":"viewing-the-details-of-a-pipeline-run_{context}"},{"parentId":null,"name":"Viewing archived pipeline runs","level":1,"index":12,"id":"viewing-archived-pipeline-runs_{context}"},{"parentId":null,"name":"Archiving a pipeline run","level":1,"index":13,"id":"archiving-a-pipeline-run_{context}"},{"parentId":null,"name":"Restoring an archived pipeline run","level":1,"index":14,"id":"restoring-an-archived-pipeline-run_{context}"},{"parentId":null,"name":"Deleting an archived pipeline run","level":1,"index":15,"id":"deleting-an-archived-pipeline-run_{context}"},{"parentId":null,"name":"Duplicating an archived pipeline run","level":1,"index":16,"id":"duplicating-an-archived-pipeline-run_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-python-packages/"},"sections":[{"parentId":null,"name":"Viewing Python packages installed on your notebook server","level":1,"index":0,"id":"viewing-python-packages-installed-on-your-notebook-server_{context}"},{"parentId":null,"name":"Installing Python packages on your notebook server","level":1,"index":1,"id":"installing-python-packages-on-your-notebook-server_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-distributed-workloads/"},"sections":[{"parentId":null,"name":"Viewing project metrics for distributed workloads","level":1,"index":0,"id":"viewing-project-metrics-for-distributed-workloads_{context}"},{"parentId":null,"name":"Viewing the status of distributed workloads","level":1,"index":1,"id":"viewing-the-status-of-distributed-workloads_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-model-performance/"},"sections":[{"parentId":null,"name":"Viewing performance metrics for all models on a model server","level":1,"index":0,"id":"viewing-performance-metrics-for-model-server_monitoring-model-performance"},{"parentId":null,"name":"Viewing HTTP request metrics for a deployed model","level":1,"index":1,"id":"viewing-http-request-metrics-for-a-deployed-model_monitoring-model-performance"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/running-distributed-workloads/"},"sections":[{"parentId":null,"name":"Running distributed data science workloads from notebooks","level":1,"index":0,"id":"running-distributed-data-science-workloads-from-notebooks_{context}"},{"parentId":null,"name":"Running distributed data science workloads from data science pipelines","level":1,"index":1,"id":"running-distributed-data-science-workloads-from-ds-pipelines_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/serving-large-models/"},"sections":[{"parentId":null,"name":"About the single-model serving platform","level":1,"index":0,"id":"about-the-single-model-serving-platform_serving-large-models"},{"parentId":null,"name":"Installing KServe","level":1,"index":1,"id":"installing-kserve_serving-large-models"},{"parentId":null,"name":"Deploying models by using the single-model serving platform","level":1,"index":2,"id":"deploying-models-using-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Enabling the single-model serving platform","level":2,"index":0,"id":"enabling-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Adding a custom model-serving runtime for the single-model serving platform","level":2,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Deploying models on the single-model serving platform","level":2,"index":2,"id":"deploying-models-on-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Accessing the inference endpoint for a deployed model","level":2,"index":3,"id":"accessing-inference-endpoint-for-deployed-model_serving-large-models"},{"parentId":null,"name":"Configuring monitoring for the single-model serving platform","level":1,"index":3,"id":"configuring-monitoring-for-the-single-model-serving-platform_serving-large-models"},{"parentId":null,"name":"Viewing model-serving runtime metrics for the single-model serving platform","level":1,"index":4,"id":"viewing-metrics-for-the-single-model-serving-platform_serving-large-models"},{"parentId":null,"name":"Performance tuning on the single-model serving platform","level":1,"index":5,"id":"_performance_tuning_on_the_single_model_serving_platform"},{"parentId":"_performance_tuning_on_the_single_model_serving_platform","name":"Resolving CUDA out-of-memory errors","level":2,"index":0,"id":"resolving-cuda-oom-errors-for-the-single-model-serving-platform_serving-large-models"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/serving-small-and-medium-sized-models/"},"sections":[{"parentId":null,"name":"Configuring model servers","level":1,"index":0,"id":"_configuring_model_servers"},{"parentId":"_configuring_model_servers","name":"Enabling the multi-model serving platform","level":2,"index":0,"id":"enabling-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a custom model-serving runtime for the multi-model serving platform","level":2,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a model server for the multi-model serving platform","level":2,"index":2,"id":"adding-a-model-server-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Deleting a model server","level":2,"index":3,"id":"deleting-a-model-server_model-serving"},{"parentId":null,"name":"Working with deployed models","level":1,"index":1,"id":"_working_with_deployed_models"},{"parentId":"_working_with_deployed_models","name":"Deploying a model by using the multi-model serving platform","level":2,"index":0,"id":"deploying-a-model-using-the-multi-model-serving-platform_model-serving"},{"parentId":"_working_with_deployed_models","name":"Viewing a deployed model","level":2,"index":1,"id":"viewing-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Updating the deployment properties of a deployed model","level":2,"index":2,"id":"updating-the-deployment-properties-of-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Deleting a deployed model","level":2,"index":3,"id":"deleting-a-deployed-model_model-serving"},{"parentId":null,"name":"Configuring monitoring for the multi-model serving platform","level":1,"index":2,"id":"configuring-monitoring-for-the-multi-model-serving-platform_model-serving"},{"parentId":null,"name":"Viewing model-serving runtime metrics for the multi-model serving platform","level":1,"index":3,"id":"viewing-metrics-for-the-multi-model-serving-platform_model-serving"},{"parentId":null,"name":"Monitoring model performance","level":1,"index":4,"id":"_monitoring_model_performance"},{"parentId":"_monitoring_model_performance","name":"Viewing performance metrics for all models on a model server","level":2,"index":0,"id":"viewing-performance-metrics-for-model-server_model-serving"},{"parentId":"_monitoring_model_performance","name":"Viewing HTTP request metrics for a deployed model","level":2,"index":1,"id":"viewing-http-request-metrics-for-a-deployed-model_model-serving"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/tuning-a-model-by-using-the-training-operator/"},"sections":[{"parentId":null,"name":"Configuring the Training Operator permissions when not using Kueue","level":1,"index":0,"id":"configuring-the-training-operator-permissions-when-not-using-kueue_{context}"},{"parentId":null,"name":"Configuring the training job","level":1,"index":1,"id":"configuring-the-training-job_{context}"},{"parentId":null,"name":"Running the training job","level":1,"index":2,"id":"running-the-training-job_{context}"},{"parentId":null,"name":"Monitoring the training job","level":1,"index":3,"id":"monitoring-the-training-job_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/upgrading-odh-v1-to-v2/"},"sections":[{"parentId":null,"name":"Upgrading the Open Data Hub Operator version 1","level":1,"index":0,"id":"upgrading-the-odh-operator-v1_upgradev1"},{"parentId":null,"name":"Installing Open Data Hub components","level":1,"index":1,"id":"installing-odh-components_upgradev1"},{"parentId":null,"name":"Accessing the Open Data Hub dashboard","level":1,"index":2,"id":"accessing-the-odh-dashboard_upgradev1"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/upgrading-odh-v2/"},"sections":[{"parentId":null,"name":"Installing Open Data Hub version 2","level":1,"index":0,"id":"installing-odh-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing the Open Data Hub Operator version 2","level":2,"index":0,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing Open Data Hub components","level":2,"index":1,"id":"installing-odh-components_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Accessing the Open Data Hub dashboard","level":2,"index":2,"id":"accessing-the-odh-dashboard_installv2"},{"parentId":null,"name":"Adding a CA bundle after upgrading","level":1,"index":1,"id":"adding-a-ca-bundle-after-upgrading_upgradev2"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-data-science-projects/"},"sections":[{"parentId":null,"name":"Creating a data science project","level":1,"index":0,"id":"creating-a-data-science-project_{context}"},{"parentId":null,"name":"Updating a data science project","level":1,"index":1,"id":"updating-a-data-science-project_{context}"},{"parentId":null,"name":"Deleting a data science project","level":1,"index":2,"id":"deleting-a-data-science-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-data-connections/"},"sections":[{"parentId":null,"name":"Adding a data connection to your data science project","level":1,"index":0,"id":"adding-a-data-connection-to-your-data-science-project_{context}"},{"parentId":null,"name":"Deleting a data connection","level":1,"index":1,"id":"deleting-a-data-connection_{context}"},{"parentId":null,"name":"Updating a connected data source","level":1,"index":2,"id":"updating-a-connected-data-source_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-project-workbenches/"},"sections":[{"parentId":null,"name":"Creating a workbench and selecting an IDE","level":1,"index":0,"id":"creating-a-workbench-select-ide_{context}"},{"parentId":"creating-a-workbench-select-ide_{context}","name":"About workbench images","level":2,"index":0,"id":"about-workbench-images_{context}"},{"parentId":"creating-a-workbench-select-ide_{context}","name":"Creating a workbench","level":2,"index":1,"id":"creating-a-project-workbench_{context}"},{"parentId":null,"name":"Starting a workbench","level":1,"index":1,"id":"starting-a-workbench_{context}"},{"parentId":null,"name":"Updating a project workbench","level":1,"index":2,"id":"updating-a-project-workbench_{context}"},{"parentId":null,"name":"Deleting a workbench from a data science project","level":1,"index":3,"id":"deleting-a-workbench-from-a-data-science-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-certificates/"},"sections":[{"parentId":null,"name":"Understanding certificates in {productname-short}","level":1,"index":0,"id":"understanding-certificates_certs"},{"parentId":"understanding-certificates_certs","name":"How CA bundles are injected","level":2,"index":0,"id":"_how_ca_bundles_are_injected"},{"parentId":"understanding-certificates_certs","name":"How the ConfigMap is managed","level":2,"index":1,"id":"_how_the_configmap_is_managed"},{"parentId":null,"name":"Adding a CA bundle","level":1,"index":1,"id":"adding-a-ca-bundle_certs"},{"parentId":null,"name":"Removing a CA bundle","level":1,"index":2,"id":"removing-a-ca-bundle_certs"},{"parentId":null,"name":"Removing a CA bundle from a namespace","level":1,"index":3,"id":"removing-a-ca-bundle-from-a-namespace_certs"},{"parentId":null,"name":"Managing certificates","level":1,"index":4,"id":"managing-certificates_certs"},{"parentId":null,"name":"Using self-signed certificates with {productname-short} components","level":1,"index":5,"id":"_using_self_signed_certificates_with_productname_short_components"},{"parentId":"_using_self_signed_certificates_with_productname_short_components","name":"Using certificates with data science pipelines","level":2,"index":0,"id":"using-certificates-with-data-science-pipelines_certs"},{"parentId":"using-certificates-with-data-science-pipelines_certs","name":"Providing a CA bundle only for data science pipelines","level":3,"index":0,"id":"_providing_a_ca_bundle_only_for_data_science_pipelines"},{"parentId":"_using_self_signed_certificates_with_productname_short_components","name":"Using certificates with workbenches","level":2,"index":1,"id":"using-certificates-with-workbenches_certs"},{"parentId":"using-certificates-with-workbenches_certs","name":"Creating data science pipelines with Elyra and self-signed certificates","level":3,"index":0,"id":"_creating_data_science_pipelines_with_elyra_and_self_signed_certificates"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-pipeline-logs/"},"sections":[{"parentId":null,"name":"About pipeline logs","level":1,"index":0,"id":"about-pipeline-logs_{context}"},{"parentId":null,"name":"Viewing pipeline step logs","level":1,"index":1,"id":"viewing-pipeline-step-logs_{context}"},{"parentId":null,"name":"Downloading pipeline step logs","level":1,"index":2,"id":"downloading-pipeline-step-logs_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-pipelines-in-jupyterlab/"},"sections":[{"parentId":null,"name":"Overview of pipelines in JupyterLab","level":1,"index":0,"id":"overview-of-pipelines-in-jupyterlab_{context}"},{"parentId":null,"name":"Accessing the pipeline editor","level":1,"index":1,"id":"accessing-the-pipeline-editor_{context}"},{"parentId":null,"name":"Creating a runtime configuration","level":1,"index":2,"id":"creating-a-runtime-configuration_{context}"},{"parentId":null,"name":"Updating a runtime configuration","level":1,"index":3,"id":"updating-a-runtime-configuration_{context}"},{"parentId":null,"name":"Deleting a runtime configuration","level":1,"index":4,"id":"deleting-a-runtime-configuration_{context}"},{"parentId":null,"name":"Duplicating a runtime configuration","level":1,"index":5,"id":"duplicating-a-runtime-configuration_{context}"},{"parentId":null,"name":"Running a pipeline in JupyterLab","level":1,"index":6,"id":"running-a-pipeline-in-jupyterlab_{context}"},{"parentId":null,"name":"Exporting a pipeline in JupyterLab","level":1,"index":7,"id":"exporting-a-pipeline-in-jupyterlab_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-deleting-users-and-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-model-serving/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-pipeline-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-authorization-token-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-workbench-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-inference-endpoint-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-jupyter-administration-interface/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-odh-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-your-workbench-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-pipeline-editor/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-ca-bundle-after-upgrading/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-data-connection-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-an-application-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-model-server-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-an-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-cluster-storage-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-notebook-pod-tolerations/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/allocating-additional-resources-to-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/authenticating-trustyai-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/backing-up-storage-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/cleaning-up-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/cleaning-up-after-deleting-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/comparing-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-custom-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-recommended-accelerator-for-notebook-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-recommended-accelerator-for-serving-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-monitoring-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-monitoring-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-quota-management-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-codeflare-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-operator-logger/"},"sections":[{"parentId":null,"name":"Viewing the {productname-short} Operator log","level":1,"index":0,"id":"_viewing_the_productname_short_operator_log"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-distributed-workloads-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-training-operator-permissions-when-not-using-kueue/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-training-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric/"},"sections":[{"parentId":null,"name":"Creating a bias metric by using the dashboard","level":1,"index":0,"id":"creating-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Creating a bias metric by using the CLI","level":1,"index":1,"id":"creating-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-new-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-new-project-for-your-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-workbench-select-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/defining-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/defining-data-science-admin-and-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric/"},"sections":[{"parentId":null,"name":"Deleting a bias metric by using the dashboard","level":1,"index":0,"id":"deleting-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Deleting a bias metric by using the CLI","level":1,"index":1,"id":"deleting-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-data-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-workbench-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-cluster-storage-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-model-using-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-on-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-using-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-a-data-science-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/disabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-bias-metric/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-the-dashboard-configuration-file/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-data-science-pipelines-2/"},"sections":[{"parentId":null,"name":"Installing {productname-short} with data science pipelines 2.0","level":1,"index":0,"id":"_installing_productname_short_with_data_science_pipelines_2_0"},{"parentId":null,"name":"Upgrading to data science pipelines 2.0","level":1,"index":1,"id":"_upgrading_to_data_science_pipelines_2_0"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-gpu-support-in-data-science/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-intel-gaudi-ai-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-trustyai-service-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-trustyai-service-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-trustyai-service/"},"sections":[{"parentId":null,"name":"Enabling the TrustyAI Service by using the dashboard","level":1,"index":0,"id":"enabling-trustyai-service-using-dashboard_{context}"},{"parentId":null,"name":"Enabling the TrustyAI Service by using the CLI","level":1,"index":1,"id":"enabling-trustyai-service-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/exporting-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/executing-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/hiding-information-about-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/hiding-the-default-jupyter-application/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/importing-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-python-packages-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-odh-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/intel-gaudi-ai-accelerator-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-the-odh-operator-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/logging-in/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/making-inference-requests-to-models-deployed-on-single-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/managing-certificates/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/monitoring-the-training-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-for-getting-started/"},"sections":[{"parentId":null,"name":"Data science workflow","level":1,"index":0,"id":"_data_science_workflow"},{"parentId":null,"name":"About this guide","level":1,"index":1,"id":"_about_this_guide"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/next-steps-getting-started/"},"sections":[{"parentId":null,"name":"Additional resources","level":1,"index":0,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-experiments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-kueue-resources/"},"sections":[{"parentId":null,"name":"Resource flavor","level":1,"index":0,"id":"_resource_flavor"},{"parentId":null,"name":"Cluster queue","level":1,"index":1,"id":"_cluster_queue"},{"parentId":null,"name":"Local queue","level":1,"index":2,"id":"_local_queue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipelines-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-user-types-and-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/preventing-users-from-adding-applications-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/pushing-project-changes-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-a-ca-bundle-from-a-namespace/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-dashboard-configuration-options/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-a-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-disabled-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requirements-for-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/resolving-cuda-oom-errors/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/revoking-user-access-to-jupyter/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-disconnected-env/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-from-ds-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-the-training-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-from-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/scheduling-a-pipeline-run-using-a-cron-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/sending-training-data-to-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/scheduling-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/sharing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/showing-hiding-information-about-enabled-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-a-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-users-to-specialized-data-science-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-idle-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/storing-data-with-data-science-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-in-jupyter-for-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-with-distributed-workloads-for-administrators/"},"sections":[{"parentId":null,"name":"A user&#8217;s Ray cluster is in a suspended state","level":1,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"A user&#8217;s Ray cluster is in a failed state","level":1,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"A user receives a <strong>failed to call webhook</strong> error message for the CodeFlare Operator","level":1,"index":2,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":null,"name":"A user receives a <strong>failed to call webhook</strong> error message for Kueue","level":1,"index":3,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"A user&#8217;s Ray cluster does not start","level":1,"index":4,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":null,"name":"A user receives a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":1,"index":5,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"A user receives a <strong>local_queue provided does not exist</strong> error message","level":1,"index":6,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"A user cannot create a Ray cluster or submit jobs","level":1,"index":7,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"The user&#8217;s pod provisioned by Kueue is terminated before the user&#8217;s image is pulled","level":1,"index":8,"id":"_the_users_pod_provisioned_by_kueue_is_terminated_before_the_users_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-with-distributed-workloads-for-users/"},"sections":[{"parentId":null,"name":"My Ray cluster is in a suspended state","level":1,"index":0,"id":"_my_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"My Ray cluster is in a failed state","level":1,"index":1,"id":"_my_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"I see a <strong>failed to call webhook</strong> error message for the CodeFlare Operator","level":1,"index":2,"id":"_i_see_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":null,"name":"I see a <strong>failed to call webhook</strong> error message for Kueue","level":1,"index":3,"id":"_i_see_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"My Ray cluster doesn&#8217;t start","level":1,"index":4,"id":"_my_ray_cluster_doesnt_start"},{"parentId":null,"name":"I see a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":1,"index":5,"id":"_i_see_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"I see a <strong>local_queue provided does not exist</strong> error message","level":1,"index":6,"id":"_i_see_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"I cannot create a Ray cluster or submit jobs","level":1,"index":7,"id":"_i_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"My pod provisioned by Kueue is terminated before my image is pulled","level":1,"index":8,"id":"_my_pod_provisioned_by_kueue_is_terminated_before_my_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/understanding-certificates/"},"sections":[{"parentId":null,"name":"How CA bundles are injected","level":1,"index":0,"id":"_how_ca_bundles_are_injected"},{"parentId":null,"name":"How the ConfigMap is managed","level":1,"index":1,"id":"_how_the_configmap_is_managed"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-connected-data-source/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-cluster-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-a-jupyter-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-notebook-server-settings-by-restarting-your-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-the-deployment-properties-of-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-your-project-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/upgrading-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-certificates-with-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Providing a CA bundle only for data science pipelines","level":1,"index":0,"id":"_providing_a_ca_bundle_only_for_data_science_pipelines"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-certificates-with-workbenches/"},"sections":[{"parentId":null,"name":"Creating data science pipelines with Elyra and self-signed certificates","level":1,"index":0,"id":"_creating_data_science_pipelines_with_elyra_and_self_signed_certificates"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-active-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-archived-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-connected-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-existing-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-http-request-metrics-for-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-metrics-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-performance-metrics-for-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-pipeline-artifacts/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-pipeline-task-executions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-python-packages-installed-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-project-metrics-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-scheduled-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-status-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/working-in-code-server/"},"sections":[{"parentId":null,"name":"Installing extensions with code-server","level":1,"index":0,"id":"_installing_extensions_with_code_server"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/working-with-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-in-jupyter-for-administrators/"},"sections":[{"parentId":null,"name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":1,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":null,"name":"A user&#8217;s notebook server does not start","level":1,"index":1,"id":"_a_users_notebook_server_does_not_start"},{"parentId":null,"name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":1,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-metrics-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-deleting-users-and-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-model-serving/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-pipeline-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-workbench-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-authorization-token-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-jupyter-administration-interface/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-inference-endpoint-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-odh-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-pipeline-editor/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-your-workbench-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-ca-bundle-after-upgrading/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-data-connection-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-model-server-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-an-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-an-application-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-cluster-storage-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-notebook-pod-tolerations/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/allocating-additional-resources-to-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-users-to-specialized-data-science-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/authenticating-trustyai-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/backing-up-storage-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/cleaning-up-after-deleting-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/cleaning-up-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/comparing-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-custom-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-recommended-accelerator-for-notebook-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-recommended-accelerator-for-serving-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-monitoring-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-monitoring-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-quota-management-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-codeflare-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-operator-logger/"},"sections":[{"parentId":null,"name":"Viewing the {productname-short} Operator log","level":1,"index":0,"id":"_viewing_the_productname_short_operator_log"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-distributed-workloads-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-training-operator-permissions-when-not-using-kueue/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-training-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric/"},"sections":[{"parentId":null,"name":"Creating a bias metric by using the dashboard","level":1,"index":0,"id":"creating-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Creating a bias metric by using the CLI","level":1,"index":1,"id":"creating-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-new-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-new-project-for-your-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-workbench-select-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/defining-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/defining-data-science-admin-and-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric/"},"sections":[{"parentId":null,"name":"Deleting a bias metric by using the dashboard","level":1,"index":0,"id":"deleting-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Deleting a bias metric by using the CLI","level":1,"index":1,"id":"deleting-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-data-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-workbench-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-cluster-storage-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-model-using-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-using-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/disabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-on-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-a-data-science-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-bias-metric/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-the-dashboard-configuration-file/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-data-science-pipelines-2/"},"sections":[{"parentId":null,"name":"Installing {productname-short} with data science pipelines 2.0","level":1,"index":0,"id":"_installing_productname_short_with_data_science_pipelines_2_0"},{"parentId":null,"name":"Upgrading to data science pipelines 2.0","level":1,"index":1,"id":"_upgrading_to_data_science_pipelines_2_0"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-gpu-support-in-data-science/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-intel-gaudi-ai-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-trustyai-service/"},"sections":[{"parentId":null,"name":"Enabling the TrustyAI Service by using the dashboard","level":1,"index":0,"id":"enabling-trustyai-service-using-dashboard_{context}"},{"parentId":null,"name":"Enabling the TrustyAI Service by using the CLI","level":1,"index":1,"id":"enabling-trustyai-service-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-trustyai-service-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/executing-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-trustyai-service-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/hiding-information-about-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/exporting-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/hiding-the-default-jupyter-application/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/importing-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-odh-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-the-odh-operator-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-python-packages-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/logging-in/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/making-inference-requests-to-models-deployed-on-single-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/intel-gaudi-ai-accelerator-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/managing-certificates/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/monitoring-the-training-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-for-getting-started/"},"sections":[{"parentId":null,"name":"Data science workflow","level":1,"index":0,"id":"_data_science_workflow"},{"parentId":null,"name":"About this guide","level":1,"index":1,"id":"_about_this_guide"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/next-steps-getting-started/"},"sections":[{"parentId":null,"name":"Additional resources","level":1,"index":0,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-experiments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-kueue-resources/"},"sections":[{"parentId":null,"name":"Resource flavor","level":1,"index":0,"id":"_resource_flavor"},{"parentId":null,"name":"Cluster queue","level":1,"index":1,"id":"_cluster_queue"},{"parentId":null,"name":"Local queue","level":1,"index":2,"id":"_local_queue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipelines-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/preventing-users-from-adding-applications-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-dashboard-configuration-options/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-user-types-and-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/pushing-project-changes-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-a-ca-bundle-from-a-namespace/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-a-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-disabled-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/resolving-cuda-oom-errors/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requirements-for-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/revoking-user-access-to-jupyter/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-disconnected-env/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-from-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/scheduling-a-pipeline-run-using-a-cron-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-from-ds-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-the-training-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/scheduling-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/sending-training-data-to-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/sharing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/showing-hiding-information-about-enabled-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-a-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-idle-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-a-jupyter-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/storing-data-with-data-science-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/supported-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-in-jupyter-for-administrators/"},"sections":[{"parentId":null,"name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":1,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":null,"name":"A user&#8217;s notebook server does not start","level":1,"index":1,"id":"_a_users_notebook_server_does_not_start"},{"parentId":null,"name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":1,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-in-jupyter-for-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-with-distributed-workloads-for-users/"},"sections":[{"parentId":null,"name":"My Ray cluster is in a suspended state","level":1,"index":0,"id":"_my_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"My Ray cluster is in a failed state","level":1,"index":1,"id":"_my_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"I see a <strong>failed to call webhook</strong> error message for the CodeFlare Operator","level":1,"index":2,"id":"_i_see_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":null,"name":"I see a <strong>failed to call webhook</strong> error message for Kueue","level":1,"index":3,"id":"_i_see_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"My Ray cluster doesn&#8217;t start","level":1,"index":4,"id":"_my_ray_cluster_doesnt_start"},{"parentId":null,"name":"I see a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":1,"index":5,"id":"_i_see_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"I see a <strong>local_queue provided does not exist</strong> error message","level":1,"index":6,"id":"_i_see_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"I cannot create a Ray cluster or submit jobs","level":1,"index":7,"id":"_i_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"My pod provisioned by Kueue is terminated before my image is pulled","level":1,"index":8,"id":"_my_pod_provisioned_by_kueue_is_terminated_before_my_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-connected-data-source/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/understanding-certificates/"},"sections":[{"parentId":null,"name":"How CA bundles are injected","level":1,"index":0,"id":"_how_ca_bundles_are_injected"},{"parentId":null,"name":"How the ConfigMap is managed","level":1,"index":1,"id":"_how_the_configmap_is_managed"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-with-distributed-workloads-for-administrators/"},"sections":[{"parentId":null,"name":"A user&#8217;s Ray cluster is in a suspended state","level":1,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"A user&#8217;s Ray cluster is in a failed state","level":1,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"A user receives a <strong>failed to call webhook</strong> error message for the CodeFlare Operator","level":1,"index":2,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":null,"name":"A user receives a <strong>failed to call webhook</strong> error message for Kueue","level":1,"index":3,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"A user&#8217;s Ray cluster does not start","level":1,"index":4,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":null,"name":"A user receives a <strong>Default Local Queue &#8230;&#8203; not found</strong> error message","level":1,"index":5,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"A user receives a <strong>local_queue provided does not exist</strong> error message","level":1,"index":6,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"A user cannot create a Ray cluster or submit jobs","level":1,"index":7,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"The user&#8217;s pod provisioned by Kueue is terminated before the user&#8217;s image is pulled","level":1,"index":8,"id":"_the_users_pod_provisioned_by_kueue_is_terminated_before_the_users_image_is_pulled"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-cluster-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-notebook-server-settings-by-restarting-your-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-the-deployment-properties-of-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-your-project-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/upgrading-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-certificates-with-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Providing a CA bundle only for data science pipelines","level":1,"index":0,"id":"_providing_a_ca_bundle_only_for_data_science_pipelines"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-certificates-with-workbenches/"},"sections":[{"parentId":null,"name":"Creating data science pipelines with Elyra and self-signed certificates","level":1,"index":0,"id":"_creating_data_science_pipelines_with_elyra_and_self_signed_certificates"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-active-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-archived-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-connected-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-existing-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-http-request-metrics-for-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-pipeline-artifacts/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/supported-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-pipeline-task-executions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-performance-metrics-for-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-project-metrics-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-python-packages-installed-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-scheduled-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-status-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/working-in-code-server/"},"sections":[{"parentId":null,"name":"Installing extensions with code-server","level":1,"index":0,"id":"_installing_extensions_with_code_server"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/working-with-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-metrics-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-metrics-for-the-multi-model-serving-platform/"},"sections":null}}}]},"asciidoc":{"html":"<div id=\"toc\" class=\"toc\">\n<div id=\"toctitle\">Table of Contents</div>\n<ul class=\"sectlevel1\">\n<li><a href=\"#enabling-data-science-pipelines-2_ds-pipelines\">Enabling data science pipelines 2.0</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#_installing_open_data_hub_with_data_science_pipelines_2_0\">Installing Open Data Hub with data science pipelines 2.0</a></li>\n<li><a href=\"#_upgrading_to_data_science_pipelines_2_0\">Upgrading to data science pipelines 2.0</a></li>\n</ul>\n</li>\n<li><a href=\"#managing-data-science-pipelines_ds-pipelines\">Managing data science pipelines</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#configuring-a-pipeline-server_ds-pipelines\">Configuring a pipeline server</a></li>\n<li><a href=\"#defining-a-pipeline_ds-pipelines\">Defining a pipeline</a></li>\n<li><a href=\"#importing-a-data-science-pipeline_ds-pipelines\">Importing a data science pipeline</a></li>\n<li><a href=\"#deleting-a-data-science-pipeline_ds-pipelines\">Deleting a data science pipeline</a></li>\n<li><a href=\"#deleting-a-pipeline-server_ds-pipelines\">Deleting a pipeline server</a></li>\n<li><a href=\"#viewing-the-details-of-a-pipeline-server_ds-pipelines\">Viewing the details of a pipeline server</a></li>\n<li><a href=\"#viewing-existing-pipelines_ds-pipelines\">Viewing existing pipelines</a></li>\n<li><a href=\"#overview-of-pipeline-versions_ds-pipelines\">Overview of pipeline versions</a></li>\n<li><a href=\"#uploading-a-pipeline-version_ds-pipelines\">Uploading a pipeline version</a></li>\n<li><a href=\"#deleting-a-pipeline-version_ds-pipelines\">Deleting a pipeline version</a></li>\n<li><a href=\"#viewing-the-details-of-a-pipeline-version_ds-pipelines\">Viewing the details of a pipeline version</a></li>\n<li><a href=\"#downloading-a-data-science-pipeline-version_ds-pipelines\">Downloading a data science pipeline version</a></li>\n</ul>\n</li>\n<li><a href=\"#managing-pipeline-experiments_ds-pipelines\">Managing pipeline experiments</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#overview-of-pipeline-experiments_ds-pipelines\">Overview of pipeline experiments</a></li>\n<li><a href=\"#creating-a-pipeline-experiment_ds-pipelines\">Creating a pipeline experiment</a></li>\n<li><a href=\"#archiving-a-pipeline-experiment_ds-pipelines\">Archiving a pipeline experiment</a></li>\n<li><a href=\"#deleting-an-archived-pipeline-experiment_ds-pipelines\">Deleting an archived pipeline experiment</a></li>\n<li><a href=\"#restoring-an-archived-pipeline-experiment_ds-pipelines\">Restoring an archived pipeline experiment</a></li>\n<li><a href=\"#viewing-pipeline-task-executions_ds-pipelines\">Viewing pipeline task executions</a></li>\n<li><a href=\"#viewing-pipeline-artifacts_ds-pipelines\">Viewing pipeline artifacts</a></li>\n<li><a href=\"#comparing-runs_ds-pipelines\">Comparing runs</a></li>\n</ul>\n</li>\n<li><a href=\"#managing-pipeline-runs_ds-pipelines\">Managing pipeline runs</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#overview-of-pipeline-runs_ds-pipelines\">Overview of pipeline runs</a></li>\n<li><a href=\"#storing-data-with-data-science-pipelines_ds-pipelines\">Storing data with data science pipelines</a></li>\n<li><a href=\"#viewing-active-pipeline-runs_ds-pipelines\">Viewing active pipeline runs</a></li>\n<li><a href=\"#executing-a-pipeline-run_ds-pipelines\">Executing a pipeline run</a></li>\n<li><a href=\"#stopping-an-active-pipeline-run_ds-pipelines\">Stopping an active pipeline run</a></li>\n<li><a href=\"#duplicating-an-active-pipeline-run_ds-pipelines\">Duplicating an active pipeline run</a></li>\n<li><a href=\"#viewing-scheduled-pipeline-runs_ds-pipelines\">Viewing scheduled pipeline runs</a></li>\n<li><a href=\"#scheduling-a-pipeline-run-using-a-cron-job_ds-pipelines\">Scheduling a pipeline run using a cron job</a></li>\n<li><a href=\"#scheduling-a-pipeline-run_ds-pipelines\">Scheduling a pipeline run</a></li>\n<li><a href=\"#duplicating-a-scheduled-pipeline-run_ds-pipelines\">Duplicating a scheduled pipeline run</a></li>\n<li><a href=\"#deleting-a-scheduled-pipeline-run_ds-pipelines\">Deleting a scheduled pipeline run</a></li>\n<li><a href=\"#viewing-the-details-of-a-pipeline-run_ds-pipelines\">Viewing the details of a pipeline run</a></li>\n<li><a href=\"#viewing-archived-pipeline-runs_ds-pipelines\">Viewing archived pipeline runs</a></li>\n<li><a href=\"#archiving-a-pipeline-run_ds-pipelines\">Archiving a pipeline run</a></li>\n<li><a href=\"#restoring-an-archived-pipeline-run_ds-pipelines\">Restoring an archived pipeline run</a></li>\n<li><a href=\"#deleting-an-archived-pipeline-run_ds-pipelines\">Deleting an archived pipeline run</a></li>\n<li><a href=\"#duplicating-an-archived-pipeline-run_ds-pipelines\">Duplicating an archived pipeline run</a></li>\n</ul>\n</li>\n<li><a href=\"#working-with-pipeline-logs_ds-pipelines\">Working with pipeline logs</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#about-pipeline-logs_ds-pipelines\">About pipeline logs</a></li>\n<li><a href=\"#viewing-pipeline-step-logs_ds-pipelines\">Viewing pipeline step logs</a></li>\n<li><a href=\"#downloading-pipeline-step-logs_ds-pipelines\">Downloading pipeline step logs</a></li>\n</ul>\n</li>\n<li><a href=\"#working-with-pipelines-in-jupyterlab_ds-pipelines\">Working with pipelines in JupyterLab</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#overview-of-pipelines-in-jupyterlab_ds-pipelines\">Overview of pipelines in JupyterLab</a></li>\n<li><a href=\"#accessing-the-pipeline-editor_ds-pipelines\">Accessing the pipeline editor</a></li>\n<li><a href=\"#creating-a-runtime-configuration_ds-pipelines\">Creating a runtime configuration</a></li>\n<li><a href=\"#updating-a-runtime-configuration_ds-pipelines\">Updating a runtime configuration</a></li>\n<li><a href=\"#deleting-a-runtime-configuration_ds-pipelines\">Deleting a runtime configuration</a></li>\n<li><a href=\"#duplicating-a-runtime-configuration_ds-pipelines\">Duplicating a runtime configuration</a></li>\n<li><a href=\"#running-a-pipeline-in-jupyterlab_ds-pipelines\">Running a pipeline in JupyterLab</a></li>\n<li><a href=\"#exporting-a-pipeline-in-jupyterlab_ds-pipelines\">Exporting a pipeline in JupyterLab</a></li>\n</ul>\n</li>\n<li><a href=\"#_additional_resources\">Additional resources</a></li>\n</ul>\n</div>\n<div id=\"preamble\">\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>As a data scientist, you can enhance your data science projects on Open Data Hub by building portable machine learning (ML) workflows with data science pipelines, using Docker containers. This enables you to standardize and automate machine learning workflows to enable you to develop and deploy your data science models.</p>\n</div>\n<div class=\"paragraph\">\n<p>For example, the steps in a machine learning workflow might include items such as data extraction, data processing, feature extraction, model training, model validation, and model serving. Automating these activities enables your organization to develop a continuous process of retraining and updating a model based on newly received data. This can help resolve challenges related to building an integrated machine learning deployment and continuously operating it in production.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can also use the Elyra JupyterLab extension to create and run data science pipelines within JupyterLab. For more information, see <a href=\"https://opendatahub.io/docs/working-with-data-science-pipelines/#working-with-pipelines-in-jupyterlab_ds-pipelines\">Working with pipelines in JupyterLab</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>From Open Data Hub version 2.10.0, data science pipelines are based on <a href=\"https://www.kubeflow.org/docs/components/pipelines/v2/\">KubeFlow Pipelines (KFP) version 2.0</a>. For more information, see <a href=\"https://opendatahub.io/docs/working-with-data-science-pipelines/#enabling-data-science-pipelines-2_ds-pipelines\">Enabling data science pipelines 2.0</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>A data science pipeline in Open Data Hub consists of the following components:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Pipeline server: A server that is attached to your data science project and hosts your data science pipeline.</p>\n</li>\n<li>\n<p>Pipeline: A pipeline defines the configuration of your machine learning workflow and the relationship between each component in the workflow.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Pipeline code: A definition of your pipeline in a YAML file.</p>\n</li>\n<li>\n<p>Pipeline graph: A graphical illustration of the steps executed in a pipeline run and the relationship between them.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Pipeline experiment: A workspace where you can try different configurations of your pipelines. You can use experiments to organize your runs into logical groups.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Archived pipeline experiment: An archived pipeline experiment.</p>\n</li>\n<li>\n<p>Pipeline artifact: An output artifact produced by a pipeline component.</p>\n</li>\n<li>\n<p>Pipeline execution: The execution of a task in a pipeline.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Pipeline run: An execution of your pipeline.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Active run: A pipeline run that is executing, or stopped.</p>\n</li>\n<li>\n<p>Scheduled run: A pipeline run that is scheduled to execute at least once.</p>\n</li>\n<li>\n<p>Archived run: An archived pipeline run.</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>This feature is based on Kubeflow Pipelines 2.0. Use the latest Kubeflow Pipelines 2.0 SDK to build your data science pipeline in Python code. After you have built your pipeline, use the SDK to compile it into an Intermediate Representation (IR) YAML file. The Open Data Hub user interface enables you to track and manage pipelines and pipeline runs.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can store your pipeline artifacts in an S3-compatible object storage bucket so that you do not consume local storage. To do this, you must first configure write access to your S3 bucket on your storage account.</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"enabling-data-science-pipelines-2_ds-pipelines\">Enabling data science pipelines 2.0</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>From Open Data Hub version 2.10.0, data science pipelines are based on <a href=\"https://www.kubeflow.org/docs/components/pipelines/v2/\">KubeFlow Pipelines (KFP) version 2.0</a>. Data science pipelines 2.0 is enabled and deployed by default in Open Data Hub.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>The <code>PipelineConf</code> class is deprecated, and there is no KFP 2.0 equivalent.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Data science pipelines 2.0 contains an installation of Argo Workflows. Open Data Hub does not support direct customer usage of this installation of Argo Workflows.</p>\n</div>\n<div class=\"paragraph\">\n<p>To install or upgrade to Open Data Hub 2.10.0 or later with data science pipelines, ensure that your cluster does not have an existing installation of Argo Workflows that is not installed by Open Data Hub.</p>\n</div>\n<div class=\"paragraph\">\n<p>Argo Workflows resources that are created by Open Data Hub have the following labels in the OpenShift Console under <strong>Administration &gt; CustomResourceDefinitions</strong>, in the <code>argoproj.io</code> group:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code> labels:\n    app.kubernetes.io/part-of: data-science-pipelines-operator\n    app.opendatahub.io/data-science-pipelines-operator: 'true'</code></pre>\n</div>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_installing_open_data_hub_with_data_science_pipelines_2_0\">Installing Open Data Hub with data science pipelines 2.0</h3>\n<div class=\"paragraph\">\n<p>To install Open Data Hub 2.10.0 or later with data science pipelines, ensure that there is no installation of Argo Workflows that is not installed by data science pipelines on your cluster, and follow the installation steps described in <a href=\"https://opendatahub.io/docs/installing-open-data-hub/\">Installing Open Data Hub</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>If you install Open Data Hub 2.10.0 or later with the <code>datasciencepipelines</code> component while there is an existing installation of Argo Workflows that is not installed by data science pipelines on your cluster, data science pipelines will be disabled after the installation completes.</p>\n</div>\n<div class=\"paragraph\">\n<p>To enable data science pipelines, remove the separate installation of Argo Workflows from your cluster. Data science pipelines will be enabled automatically.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_upgrading_to_data_science_pipelines_2_0\">Upgrading to data science pipelines 2.0</h3>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>After you upgrade to Open Data Hub 2.9 or later, pipelines created with data science pipelines 1.0 continue to run, but are inaccessible from the Open Data Hub dashboard. If you are a current data science pipelines user, do not upgrade to Open Data Hub with data science pipelines 2.0 until you are ready to migrate to the new pipelines solution.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>To upgrade to Open Data Hub 2 with data science pipelines 2.0, ensure that there is no installation of Argo Workflows that is not installed by data science pipelines on your cluster, and follow the upgrade steps described in <a href=\"https://opendatahub.io/docs/upgrading-open-data-hub/\">Upgrading Open Data Hub</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>If you upgrade to Open Data Hub 2 with data science pipelines enabled and an Argo Workflows installation that is not installed by data science pipelines exists on your cluster, Open Data Hub components will not be upgraded. To complete the component upgrade, disable data science pipelines or remove the separate installation of Argo Workflows. The component upgrade will complete automatically.</p>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"managing-data-science-pipelines_ds-pipelines\">Managing data science pipelines</h2>\n<div class=\"sectionbody\">\n<div class=\"sect2\">\n<h3 id=\"configuring-a-pipeline-server_ds-pipelines\">Configuring a pipeline server</h3>\n<div class=\"paragraph _abstract\">\n<p>Before you can successfully create a pipeline in Open Data Hub, you must configure a pipeline server. This task includes configuring where your pipeline artifacts and data are stored.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>You are not required to specify any storage directories when configuring a data connection for your pipeline server. When you import a pipeline, the <code>/pipelines</code> folder is created in the <code>root</code> folder of the bucket, containing a YAML file for the pipeline. If you upload a new version of the same pipeline, a new YAML file with a different ID is added to the <code>/pipelines</code> folder.</p>\n</div>\n<div class=\"paragraph\">\n<p>When you run a pipeline, the artifacts are stored in the <code>/pipeline-name</code> folder in the <code>root</code> folder of the bucket.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you use an external MySQL database and upgrade to Open Data Hub 2.10.0 or later, the database is migrated to data science pipelines 2.0 format, making it incompatible with earlier versions of Open Data Hub.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that you can add a pipeline server to.</p>\n</li>\n<li>\n<p>You have an existing S3-compatible object storage bucket and you have configured write access to your S3 bucket on your storage account.</p>\n</li>\n<li>\n<p>If you are configuring a pipeline server with an external MySQL database, your database must use at least MySQL version 5.x. However, Red&#160;Hat recommends that you use MySQL version 8.x.</p>\n</li>\n<li>\n<p>If you are configuring a pipeline server with a MariaDB database, your database must use MariaDB version 10.3 or later. However, Red&#160;Hat recommends that you use at least MariaDB version 10.5.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to configure a pipeline server for.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Pipelines</strong> tab.</p>\n</li>\n<li>\n<p>Click <strong>Configure pipeline server</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Configure pipeline server</strong> dialog appears.</p>\n</div>\n</li>\n<li>\n<p>In the <strong>Object storage connection</strong> section, provide values for the mandatory fields:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Access key</strong> field, enter the access key ID for the S3-compatible object storage provider.</p>\n</li>\n<li>\n<p>In the <strong>Secret key</strong> field, enter the secret access key for the S3-compatible object storage account that you specified.</p>\n</li>\n<li>\n<p>In the <strong>Endpoint</strong> field, enter the endpoint of your S3-compatible object storage bucket.</p>\n</li>\n<li>\n<p>In the <strong>Region</strong> field, enter the default region of your S3-compatible object storage account.</p>\n</li>\n<li>\n<p>In the <strong>Bucket</strong> field, enter the name of your S3-compatible object storage bucket.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you specify incorrect data connection settings, you cannot update these settings on the same pipeline server. Therefore, you must delete the pipeline server and configure another one.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>In the <strong>Database</strong> section, click <strong>Show advanced database options</strong> to specify the database to store your pipeline data and select one of the following sets of actions:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Select <strong>Use default database stored on your cluster</strong> to deploy a MariaDB database in your project.</p>\n</li>\n<li>\n<p>Select <strong>Connect to external MySQL database</strong> to add a new connection to an external database that your pipeline server can access.</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Host</strong> field, enter the database&#8217;s host name.</p>\n</li>\n<li>\n<p>In the <strong>Port</strong> field, enter the database&#8217;s port.</p>\n</li>\n<li>\n<p>In the <strong>Username</strong> field, enter the default user name that is connected to the database.</p>\n</li>\n<li>\n<p>In the <strong>Password</strong> field, enter the password for the default user account.</p>\n</li>\n<li>\n<p>In the <strong>Database</strong> field, enter the database name.</p>\n</li>\n</ol>\n</div>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Click <strong>Configure pipeline server</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>In the <strong>Pipelines</strong> tab for the project:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>The <strong>Import pipeline</strong> button is available.</p>\n</li>\n<li>\n<p>When you click the action menu (<strong>&#8942;</strong>) and then click <strong>View pipeline server configuration</strong>, the pipeline server details are displayed.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"defining-a-pipeline_ds-pipelines\">Defining a pipeline</h3>\n<div class=\"paragraph _abstract\">\n<p>The Kubeflow Pipelines SDK enables you to define end-to-end machine learning and data pipelines. Use the latest Kubeflow Pipelines 2.0 SDK to build your data science pipeline in Python code. After you have built your pipeline, use the SDK to compile it into an Intermediate Representation (IR) YAML file. After defining the pipeline, you can import the YAML file to the Open Data Hub dashboard to enable you to configure its execution settings.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can also use the Elyra JupyterLab extension to create and run data science pipelines within JupyterLab. For more information about the Elyra JupyterLab extension, see <a href=\"https://elyra.readthedocs.io/en/stable/getting_started/overview.html\">Elyra Documentation</a>.</p>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://www.kubeflow.org/docs/components/pipelines/v2/\">Kubeflow Pipelines 2.0 Documentation</a></p>\n</li>\n<li>\n<p><a href=\"https://elyra.readthedocs.io/en/stable/getting_started/overview.html\">Elyra Documentation</a></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"importing-a-data-science-pipeline_ds-pipelines\">Importing a data science pipeline</h3>\n<div class=\"paragraph _abstract\">\n<p>To help you begin working with data science pipelines in Open Data Hub, you can import a YAML file containing your pipeline&#8217;s code to an active pipeline server, or you can import the YAML file from a URL. This file contains a Kubeflow pipeline compiled by using the Kubeflow compiler. After you have imported the pipeline to a pipeline server, you can execute the pipeline by creating a pipeline run.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have compiled your pipeline with the Kubeflow compiler and you have access to the resulting YAML file.</p>\n</li>\n<li>\n<p>If you are uploading your pipeline from a URL, the URL is publicly accessible.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Pipelines</strong> page, select the <strong>project</strong> that you want to import a pipeline to.</p>\n</li>\n<li>\n<p>Click <strong>Import pipeline</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Import pipeline</strong> dialog, enter the details for the pipeline that you are importing.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Pipeline name</strong> field, enter a name for the pipeline that you are importing.</p>\n</li>\n<li>\n<p>In the <strong>Pipeline description</strong> field, enter a description for the pipeline that you are importing.</p>\n</li>\n<li>\n<p>Select where you want to import your pipeline from by performing one of the following actions:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Select <strong>Upload a file</strong> to upload your pipeline from your local machine&#8217;s file system. Import your pipeline by clicking <strong>upload</strong> or by dragging and dropping a file.</p>\n</li>\n<li>\n<p>Select <strong>Import by url</strong> to upload your pipeline from a URL and then enter the URL into the text box.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Click <strong>Import pipeline</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline that you imported appears on the <strong>Pipelines</strong> page and on the <strong>Pipelines</strong> tab on the project details page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"deleting-a-data-science-pipeline_ds-pipelines\">Deleting a data science pipeline</h3>\n<div class=\"paragraph _abstract\">\n<p>If you no longer require access to your data science pipeline on the dashboard, you can delete it so that it does not appear on the <strong>Data Science Pipelines</strong> page.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>There are active pipelines available on the <strong>Pipelines</strong> page.</p>\n</li>\n<li>\n<p>The pipeline that you want to delete does not contain any pipeline versions.</p>\n</li>\n<li>\n<p>The pipeline that you want to delete does not contain any pipeline versions. For more information, see <a href=\"https://opendatahub.io/docs/working-with-data-science-pipelines/#deleting-a-pipeline-version_ds-pipelines\">Deleting a pipeline version</a>.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Pipelines</strong> page, select the project that contains the pipeline that you want to delete from the <strong>Project</strong> list.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the pipeline that you want to delete and click <strong>Delete pipeline</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Delete pipeline</strong> dialog, enter the pipeline name in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete pipeline</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The data science pipeline that you deleted no longer appears on the <strong>Pipelines</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"deleting-a-pipeline-server_ds-pipelines\">Deleting a pipeline server</h3>\n<div class=\"paragraph _abstract\">\n<p>After you have finished running your data science pipelines, you can delete the pipeline server. Deleting a pipeline server automatically deletes all of its associated pipelines, pipeline versions, and runs. If your pipeline data is stored in a database, the database is also deleted along with its meta-data. In addition, after deleting a pipeline server, you cannot create new pipelines or pipeline runs until you create another pipeline server.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Pipelines</strong> page, select the <strong>project</strong> for the pipeline server that you want to delete.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline server actions</strong> list, select <strong>Delete pipeline server</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Delete pipeline server</strong> dialog, enter the pipeline server&#8217;s name in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Pipelines previously assigned to the deleted pipeline server no longer appears on the <strong>Pipelines</strong> page for the relevant data science project.</p>\n</li>\n<li>\n<p>Pipeline runs previously assigned to the deleted pipeline server no longer appears on the <strong>Runs</strong> page for the relevant data science project.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-the-details-of-a-pipeline-server_ds-pipelines\">Viewing the details of a pipeline server</h3>\n<div class=\"paragraph _abstract\">\n<p>You can view the details of pipeline servers configured in Open Data Hub, such as the pipeline&#8217;s data connection details and where its data is stored.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>You have previously created a data science project that contains an active and available pipeline server.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Pipelines</strong> page opens, select the <strong>project</strong> whose pipeline server you want to view.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline server actions</strong> list, select <strong>View pipeline server configuration</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>You can view the relevant pipeline server details in the <strong>View pipeline server</strong> dialog.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-existing-pipelines_ds-pipelines\">Viewing existing pipelines</h3>\n<div class=\"paragraph _abstract\">\n<p>You can view the details of pipelines that you have imported to Open Data Hub, such as the pipeline&#8217;s last run, when it was created, the pipeline&#8217;s executed runs, and details of any associated pipeline versions.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>Existing pipelines are available.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Pipelines</strong> page, select the relevant <strong>project</strong> for the pipelines you want to view.</p>\n</li>\n<li>\n<p>Study the pipelines on the list.</p>\n</li>\n<li>\n<p>Optional: Click <strong>Expand</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhoai-expand-icon.png\" alt=\"rhoai expand icon\"></span>) on the relevant row to view details of any pipeline versions associated with the pipeline.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>A list of previously created data science pipelines appears on the <strong>Pipelines</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"overview-of-pipeline-versions_ds-pipelines\">Overview of pipeline versions</h3>\n<div class=\"paragraph _abstract\">\n<p>You can manage incremental changes to pipelines in Open Data Hub by using versioning. This allows you to develop and deploy pipelines iteratively, preserving a record of your changes. You can track and manage your changes on the Open Data Hub dashboard, allowing you to schedule and execute runs against all available versions of your pipeline.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"uploading-a-pipeline-version_ds-pipelines\">Uploading a pipeline version</h3>\n<div class=\"paragraph _abstract\">\n<p>You can upload a YAML file to an active pipeline server that contains the latest version of your pipeline, or you can upload the YAML file from a URL. The YAML file must consist of a Kubeflow pipeline compiled by using the Kubeflow compiler. After you upload a pipeline version to a pipeline server, you can execute it by creating a pipeline run.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have a pipeline version available and ready to upload.</p>\n</li>\n<li>\n<p>If you are uploading your pipeline version from a URL, the URL is publicly accessible.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Pipelines</strong> page, select the <strong>project</strong> that you want to upload a pipeline version to.</p>\n</li>\n<li>\n<p>Click the <strong>Import pipeline</strong> dropdown list and select <strong>Upload new version</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Upload new version</strong> dialog, enter the details for the pipeline version that you are uploading.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>From the <strong>Pipeline</strong> list, select the pipeline that you want to upload your pipeline version to.</p>\n</li>\n<li>\n<p>In the <strong>Pipeline version name</strong> field, confirm the name for the pipeline version, and change it if necessary.</p>\n</li>\n<li>\n<p>In the <strong>Pipeline version description</strong> field, enter a description for the pipeline version.</p>\n</li>\n<li>\n<p>Select where you want to upload your pipeline version from by performing one of the following actions:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Select <strong>Upload a file</strong> to upload your pipeline version from your local machine&#8217;s file system. Import your pipeline version by clicking <strong>upload</strong> or by dragging and dropping a file.</p>\n</li>\n<li>\n<p>Select <strong>Import by url</strong> to upload your pipeline version from a URL and then enter the URL into the text box.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Click <strong>Upload</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline version that you uploaded is displayed on the <strong>Pipelines</strong> page. Click <strong>Expand</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhoai-expand-icon.png\" alt=\"rhoai expand icon\"></span>) on the row containing the pipeline to view its versions.</p>\n</li>\n<li>\n<p>The <strong>Version</strong> column on the row containing the pipeline version that you uploaded on the <strong>Pipelines</strong> page increments by one.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"deleting-a-pipeline-version_ds-pipelines\">Deleting a pipeline version</h3>\n<div class=\"paragraph _abstract\">\n<p>You can delete specific versions of a pipeline when you no longer require them. Deleting a default pipeline version automatically changes the default pipeline version to the next most recent version. If no pipeline versions exist, the pipeline persists without a default version.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Pipelines</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Delete the pipeline versions that you no longer require:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>To delete a single pipeline version:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>From the <strong>Project</strong> list, select the project that contains a version of a pipeline that you want to delete.</p>\n</li>\n<li>\n<p>On the row containing the pipeline, click <strong>Expand</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhoai-expand-icon.png\" alt=\"rhoai expand icon\"></span>).</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the project version that you want to delete and click <strong>Delete pipeline version</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Delete pipeline version</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter the name of the pipeline version in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>To delete multiple pipeline versions:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>On the row containing each pipeline version that you want to delete, select the checkbox.</p>\n</li>\n<li>\n<p>Click the action menu (&#8942;) next to the <strong>Import pipeline</strong> dropdown, and select <strong>Delete</strong> from the list.</p>\n</li>\n</ol>\n</div>\n</li>\n</ul>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline version that you deleted no longer appears on the <strong>Pipelines</strong> page or on the <strong>Pipelines</strong> tab for the data science project.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-the-details-of-a-pipeline-version_ds-pipelines\">Viewing the details of a pipeline version</h3>\n<div class=\"paragraph _abstract\">\n<p>You can view the details of a pipeline version that you have uploaded to Open Data Hub, such as its graph and YAML code.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n<li>\n<p>You have a pipeline available on an active and available pipeline server.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Pipelines</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project containing the pipeline versions that you want to view details for.</p>\n</li>\n<li>\n<p>Click <strong>Expand</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhoai-expand-icon.png\" alt=\"rhoai expand icon\"></span>) on the row containing the pipeline that you want to view versions for.</p>\n</li>\n<li>\n<p>Click the pipeline version that you want to view the details of.</p>\n<div class=\"paragraph\">\n<p>The <strong>Pipeline details</strong> page opens, displaying the <strong>Graph</strong> and <strong>YAML</strong> tabs.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>On the <strong>Pipeline details</strong> page, you can view the pipeline graph and YAML code.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"downloading-a-data-science-pipeline-version_ds-pipelines\">Downloading a data science pipeline version</h3>\n<div class=\"paragraph _abstract\">\n<p>To make further changes to a data science pipeline version that you previously uploaded to Open Data Hub, you can download pipeline version code from the user interface.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have created and imported a pipeline to an active pipeline server that is available to download.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Pipelines</strong> page, select the <strong>project</strong> that contains the version that you want to download.</p>\n</li>\n<li>\n<p>For a pipeline that contains the version that you want to download, click <strong>Expand</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhoai-expand-icon.png\" alt=\"rhoai expand icon\"></span>).</p>\n</li>\n<li>\n<p>Click the pipeline version that you want to download.</p>\n</li>\n<li>\n<p>On the <strong>Pipeline details</strong> page, click the <strong>YAML</strong> tab.</p>\n</li>\n<li>\n<p>Click the <strong>Download</strong> button (<span class=\"image\"><img src=\"/static/docs/images/rhoai-download-icon.png\" alt=\"rhoai download icon\"></span>) to download the YAML file containing your pipeline version code to your local machine.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline version code downloads to your browser&#8217;s default directory for downloaded files.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"managing-pipeline-experiments_ds-pipelines\">Managing pipeline experiments</h2>\n<div class=\"sectionbody\">\n<div class=\"sect2\">\n<h3 id=\"overview-of-pipeline-experiments_ds-pipelines\">Overview of pipeline experiments</h3>\n<div class=\"paragraph _abstract\">\n<p>A pipeline experiment is a workspace where you can try different configurations of your pipelines. You can use experiments to organize your runs into logical groups. As a data scientist, you can use Open Data Hub to define, manage, and track pipeline experiments. You can view a record of previously created and archived experiments from the <strong>Experiments</strong> page in the Open Data Hub user interface. Pipeline experiments contain pipeline runs, including recurring runs. This allows you to try different configurations of your pipelines.</p>\n</div>\n<div class=\"paragraph\">\n<p>When you work with data science pipelines, it is important to monitor and record your pipeline experiments to track the performance of your data science pipelines. You can compare the results of up to 10 pipeline runs at one time, and view available parameter, scalar metric, confusion matrix, and receiver operating characteristic (ROC) curve data for all selected runs.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can view artifacts for an executed pipeline run from the Open Data Hub dashboard. Pipeline artifacts can help you to evaluate the performance of your pipeline runs and make it easier to understand your pipeline components. Pipeline artifacts can range from plain text data to detailed, interactive data visualizations.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"creating-a-pipeline-experiment_ds-pipelines\">Creating a pipeline experiment</h3>\n<div class=\"paragraph _abstract\">\n<p>Pipeline experiments are workspaces where you can try different configurations of your pipelines. You can also use experiments to organize your pipeline runs into logical groups. Pipeline experiments contain pipeline runs, including recurring runs.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Experiments</strong> &#8594; <strong>Experiments and runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Experiments</strong> page, from the <strong>Project</strong> drop-down list, select the project to create the pipeline experiment in.</p>\n</li>\n<li>\n<p>Click <strong>Create experiment</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Create experiment</strong> dialog, configure the pipeline experiment:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Experiment name</strong> field, enter a name for the pipeline experiment.</p>\n</li>\n<li>\n<p>In the <strong>Description</strong> field, enter a description for the pipeline experiment.</p>\n</li>\n<li>\n<p>Click <strong>Create experiment</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline experiment that you created appears on the <strong>Experiments</strong> tab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"archiving-a-pipeline-experiment_ds-pipelines\">Archiving a pipeline experiment</h3>\n<div class=\"paragraph _abstract\">\n<p>You can retain records of your pipeline experiments by archiving them. If required, you can restore pipeline experiments from your archive to reuse, or delete pipeline experiments that are no longer required.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and has a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>A pipeline experiment is available to archive.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Experiments</strong> &#8594; <strong>Experiments and runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Experiments</strong> page, from the <strong>Project</strong> drop-down list, select the project that contains the pipeline experiment that you want to archive.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the pipeline experiment that you want to archive, and then click <strong>Archive</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Archiving experiment</strong> dialog, enter the pipeline experiment name in the text field to confirm that you intend to archive it.</p>\n</li>\n<li>\n<p>Click <strong>Archive</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The archived pipeline experiment does not appear in the <strong>Runs</strong> tab, and instead appears in the <strong>Archive</strong> tab on the <strong>Experiments</strong> page for the pipeline experiment.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"deleting-an-archived-pipeline-experiment_ds-pipelines\">Deleting an archived pipeline experiment</h3>\n<div class=\"paragraph _abstract\">\n<p>You can delete pipeline experiments from the Open Data Hub experiment archive.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>A pipeline experiment is available in the pipeline archive.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Experiments</strong> &#8594; <strong>Experiments and runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Experiments</strong> page, from the <strong>Project</strong> drop-down list, select the project that contains the archived pipeline experiment that you want to delete.</p>\n</li>\n<li>\n<p>Click the <strong>Archive</strong> tab.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the pipeline experiment that you want to delete, and then click <strong>Delete</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Delete experiment</strong> dialog, enter the pipeline experiment name in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline experiment that you deleted no longer appears on the <strong>Archive</strong> tab on the <strong>Experiments</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"restoring-an-archived-pipeline-experiment_ds-pipelines\">Restoring an archived pipeline experiment</h3>\n<div class=\"paragraph _abstract\">\n<p>You can restore an archived pipeline experiment to the active state.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and has a pipeline server.</p>\n</li>\n<li>\n<p>An archived pipeline experiment exists in your project.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Experiments</strong> &#8594; <strong>Experiments and runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Experiments</strong> page, from the <strong>Project</strong> drop-down list, select the project that contains the archived pipeline experiment that you want to restore.</p>\n</li>\n<li>\n<p>Click the <strong>Archive</strong> tab.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the pipeline experiment that you want to restore, and then click <strong>Restore</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Restore experiment</strong> dialog, click <strong>Restore</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The restored pipeline experiment appears in the <strong>Experiments</strong> tab on the <strong>Experiments</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-pipeline-task-executions_ds-pipelines\">Viewing pipeline task executions</h3>\n<div class=\"paragraph _abstract\">\n<p>When a pipeline run executes, you can view details of executed tasks in each step in a pipeline run from the Open Data Hub dashboard. A step forms part of a task in a pipeline.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have previously triggered a pipeline run.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Experiments</strong> &#8594; <strong>Executions</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Executions</strong> page, from the <strong>Project</strong> drop-down list, select the project that contains the experiment for the pipeline task executions that you want to view.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>On the <strong>Executions</strong> page, you can view the execution details of each pipeline task execution, such as its name, status, unique ID, and execution type. The execution status indicates whether the pipeline task has successfully executed. For further information about the details of the task execution, click the execution name.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-pipeline-artifacts_ds-pipelines\">Viewing pipeline artifacts</h3>\n<div class=\"paragraph _abstract\">\n<p>After a pipeline run executes, you can view its pipeline artifacts from the Open Data Hub dashboard. Pipeline artifacts can help you to evaluate the performance of your pipeline runs and make it easier to understand your pipeline components. Pipeline artifacts can range from plain text data to detailed, interactive data visualizations.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have previously triggered a pipeline run.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Experiments</strong> &#8594; <strong>Artifacts</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Artifacts</strong> page, from the <strong>Project</strong> drop-down list, select the project that contains the pipeline experiment for the pipeline artifacts that you want to view.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>On the <strong>Artifacts</strong> page, you can view the details of each pipeline artifact, such as its name, unique ID, type, and URI.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"comparing-runs_ds-pipelines\">Comparing runs</h3>\n<div class=\"paragraph _abstract\">\n<p>You can compare up to 10 pipeline runs at one time, and view available parameter, scalar metric, confusion matrix, and receiver operating characteristic (ROC) curve data for all selected runs.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and has a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have created at least 2 pipeline runs.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the Open Data Hub dashboard, select <strong>Experiments &gt; Experiments and runs</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Experiments</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> drop-down list, select the project that contains the runs that you want to compare.</p>\n</li>\n<li>\n<p>In the <strong>Experiments</strong> column, click the experiment that you want to compare runs for. To select runs that are not in an experiment, click <strong>Default</strong>. All runs that are created without specifying an experiment will appear in the <strong>Default</strong> group.</p>\n<div class=\"paragraph\">\n<p>The <strong>Runs</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Select the checkbox next to each run that you want to compare, and then click <strong>Compare runs</strong>. You can compare a maximum of 10 runs at one time.</p>\n<div class=\"paragraph\">\n<p>The <strong>Compare runs</strong> page opens and displays available parameter, scalar metric, confusion matrix, and receiver operating characteristic (ROC) curve data for the runs that you selected.</p>\n</div>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>The <strong>Run list</strong> section displays a list of selected runs. You can filter the list by run name, experiment, pipeline version start date, duration, and status.</p>\n</li>\n<li>\n<p>The <strong>Parameters</strong> section displays parameter information for each selected run. Set the <strong>Hide parameters with no differences</strong> switch to <strong>On</strong> to hide parameters that have the same values.</p>\n</li>\n<li>\n<p>The <strong>Metrics</strong> section displays scalar metric, confusion matrix, and ROC curve data for all selected runs.</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>On the <strong>Scalar metrics</strong> tab, set the <strong>Hide parameters with no differences</strong> switch to <strong>On</strong> to hide parameters that have the same values.</p>\n</li>\n<li>\n<p>On the <strong>ROC curve</strong> tab, in the artifacts list, adjust the ROC curve chart by deselecting the checkbox next to artifacts that you want to remove from the chart.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>To select different runs for comparison, click <strong>Manage runs</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Manage runs</strong> dialog opens.</p>\n</div>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>From the <strong>Search</strong> filter drop-down list, select <strong>Run</strong>, <strong>Experiment</strong>, <strong>Pipeline version</strong>, <strong>Start date</strong>, or <strong>Status</strong> to filter the run list by each value.</p>\n</li>\n<li>\n<p>Deselect the checkbox next to each run that you want to remove from your comparison.</p>\n</li>\n<li>\n<p>Select the checkbox next to each run that you want to add to your comparison.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Update</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The <strong>Compare runs</strong> page opens and displays data for the runs that you selected.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"managing-pipeline-runs_ds-pipelines\">Managing pipeline runs</h2>\n<div class=\"sectionbody\">\n<div class=\"sect2\">\n<h3 id=\"overview-of-pipeline-runs_ds-pipelines\">Overview of pipeline runs</h3>\n<div class=\"paragraph _abstract\">\n<p>A pipeline run is a single execution of a data science pipeline. As data scientist, you can use Open Data Hub to define, manage, and track executions of a data science pipeline. You can view a record of previously executed, scheduled, and archived runs from the <strong>Runs</strong> page in the Open Data Hub user interface.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can optimize your use of pipeline runs for portability and repeatability by using pipeline experiments. With experiments, you can logically group pipeline runs and try different configurations of your pipelines. You can also clone your pipeline runs to reproduce and scale them, or archive them when you want to retain a record of their execution, but no longer require them. You can delete archived runs that you no longer want to retain, or you can restore them to their former state.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can execute a run once, that is, immediately after its creation, or on a recurring basis. Recurring runs consist of a copy of a pipeline with all of its parameter values and a run trigger. A run trigger indicates when a recurring run executes. You can define the following run triggers:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Periodic: used for scheduling runs to execute in intervals.</p>\n</li>\n<li>\n<p>Cron: used for scheduling runs as a cron job.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>You can also configure multiple instances of the same run to execute concurrently, from a range of one to ten. When executed, you can track the run&#8217;s progress from the run <strong>Details</strong> page on the Open Data Hub user interface. From here, you can view the run&#8217;s graph, and output artifacts. A pipeline run can be in one of the following states:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Scheduled: A pipeline run that is scheduled to execute at least once</p>\n</li>\n<li>\n<p>Active: A pipeline run that is executing, or stopped.</p>\n</li>\n<li>\n<p>Archived: An archived pipeline run.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>You can use catch up runs to ensure your pipeline runs do not permanently fall behind schedule when paused. For example, if you re-enable a paused recurring run, the run scheduler backfills each missed run interval. If you disable catch up runs, and you have a scheduled run interval ready to execute, the run scheduler only schedules the run execution for the latest run interval. Catch up runs are enabled by default. However, if your pipeline handles backfill internally, Red Hat recommends that you disable catch up runs to avoid duplicate backfill.</p>\n</div>\n<div class=\"paragraph\">\n<p>After a pipeline run executes, you can view details of its executed tasks on the <strong>Executions</strong> page, along with its artifacts, on the <strong>Artifacts</strong> page. From the <strong>Executions</strong> page, you can view the execution status of each task, which indicates whether it completed successfully. You can also view further information about each executed task by clicking the execution name in the list. From the <strong>Artifacts</strong> page, you can view the the details of each pipeline artifact, such as its name, unique ID, type, and URI. Pipeline artifacts can help you to evaluate the performance of your pipeline runs and make it easier to understand your pipeline components. Pipeline artifacts can range from plain text data or detailed, interactive data visualizations.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can review and analyze logs for each step in an active pipeline run. With the log viewer, you can search for specific log messages, view the log for each step, and download the step logs to your local machine.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"storing-data-with-data-science-pipelines_ds-pipelines\">Storing data with data science pipelines</h3>\n<div class=\"paragraph _abstract\">\n<p>When you run a data science pipeline, Open Data Hub stores the pipeline YAML configuration file and resulting pipeline run artifacts in the <code>root</code> directory of your storage bucket. The directories that contain pipeline run artifacts can differ depending on where you executed the pipeline run from. See the following table for further information:</p>\n</div>\n<table class=\"tableblock frame-all grid-all stretch\">\n<caption class=\"title\">Table 1. Pipeline configuration file and artifacts storage locations</caption>\n<colgroup>\n<col style=\"width: 33.3333%;\">\n<col style=\"width: 33.3333%;\">\n<col style=\"width: 33.3334%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Pipeline run source</th>\n<th class=\"tableblock halign-left valign-top\">Pipeline storage directory</th>\n<th class=\"tableblock halign-left valign-top\">Run artifacts storage directory</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Open Data Hub dashboard</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>/pipelines/&lt;pipeline_version_id&gt;</code></p>\n<p class=\"tableblock\">Example: <code>/pipelines/1d01c4eb-d2ab-4916-9935-a73a5580f1fb</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>/&lt;pipeline_name&gt;/&lt;pipeline run_id&gt;</code></p>\n<p class=\"tableblock\">Example: <code>iris-training-pipeline/2g48k8pw-a8ib-4884-9145-h41j7599h3ds</code></p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">JupyterLab Elyra extension</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>/pipelines/&lt;pipeline_version_id&gt;</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>/&lt;pipeline_name_timestamp&gt;</code></p>\n<p class=\"tableblock\">Example: <code>/hello-generic-world-0523161704</code></p>\n<p class=\"tableblock\">With the JupyterLab Elyra extension, you can also set an <a href=\"https://elyra.readthedocs.io/en/latest/user_guide/pipelines.html#generic-node-properties\">object storage path prefix</a>.</p>\n<p class=\"tableblock\">Example: <code>/iris-project/hello-generic-world-0523161704</code></p></td>\n</tr>\n</tbody>\n</table>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-active-pipeline-runs_ds-pipelines\">Viewing active pipeline runs</h3>\n<div class=\"paragraph _abstract\">\n<p>You can view a list of pipeline runs that were previously executed in a pipeline experiment. From this list, you can view details relating to your pipeline runs, such as the pipeline version that the run belongs to, along with the run status, duration, and execution start time.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and has a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have previously executed a pipeline run that is available.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Experiments</strong> &#8594; <strong>Experiments and runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Experiments</strong> page, from the <strong>Project</strong> drop-down list, select the project that contains the pipeline experiment for the active pipeline runs that you want to view.</p>\n</li>\n<li>\n<p>From the list of experiments, click the experiment that contains the active pipeline runs that you want to view.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, click the <strong>Runs</strong> tab.</p>\n<div class=\"paragraph\">\n<p>After a run has completed its execution, the run&#8217;s status appears in the <strong>Status</strong> column in the table, indicating whether the run has succeeded or failed.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>A list of active runs appears in the <strong>Runs</strong> tab on the <strong>Runs</strong> page for the pipeline experiment.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"executing-a-pipeline-run_ds-pipelines\">Executing a pipeline run</h3>\n<div class=\"paragraph _abstract\">\n<p>By default, a pipeline run executes once immediately after it is created.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Experiments</strong> &#8594; <strong>Experiments and runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Experiments</strong> page, from the <strong>Project</strong> drop-down list, select the project that contains the pipeline experiment that you want to create a run for.</p>\n</li>\n<li>\n<p>From the list of pipeline experiments, click the experiment that you want to create a run for.</p>\n</li>\n<li>\n<p>Click <strong>Create run</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Create run</strong> page, configure the run:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>From the <strong>Experiment</strong> list, select the pipeline experiment that you want to create a run for. Alternatively, to create a new pipeline experiment, click <strong>Create new experiment</strong>, and then complete the relevant fields in the <strong>Create experiment</strong> dialog.</p>\n</li>\n<li>\n<p>In the <strong>Name</strong> field, enter a name for the run.</p>\n</li>\n<li>\n<p>In the <strong>Description</strong> field, enter a description for the run.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline</strong> list, select the pipeline that you want to create a run for. Alternatively, to create a new pipeline, click <strong>Create new pipeline</strong>, and then complete the relevant fields in the <strong>Import pipeline</strong> dialog.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline version</strong> list, select the pipeline version to create a run for. Alternatively, to upload a new version, click <strong>Upload new version</strong>, and then complete the relevant fields in the <strong>Upload new version</strong> dialog.</p>\n</li>\n<li>\n<p>Configure the input parameters for the run by selecting the parameters from the list.</p>\n</li>\n<li>\n<p>Click <strong>Create run</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline run that you created appears in the <strong>Runs</strong> tab on the <strong>Runs</strong> page for the pipeline experiment.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"stopping-an-active-pipeline-run_ds-pipelines\">Stopping an active pipeline run</h3>\n<div class=\"paragraph _abstract\">\n<p>If you no longer require an active pipeline run to continue executing in a pipeline experiment, you can stop the run before its defined end date.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>There is a previously created data science project available that contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>An active pipeline run is currently executing.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Experiments</strong> &#8594; <strong>Experiments and runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Experiments</strong> page, from the <strong>Project</strong> drop-down list, select the project that contains the pipeline experiment for the active run that you want to stop.</p>\n</li>\n<li>\n<p>From the list of pipeline experiments, click the pipeline experiment that contains the run that you want to stop.</p>\n</li>\n<li>\n<p>In the <strong>Runs</strong> tab, click the action menu (<strong>&#8942;</strong>) beside the active run that you want to stop, and then click <strong>Stop</strong>.</p>\n<div class=\"paragraph\">\n<p>There might be a short delay while the run stops.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>In the list of active runs, the status of the run is \"stopped\".</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"duplicating-an-active-pipeline-run_ds-pipelines\">Duplicating an active pipeline run</h3>\n<div class=\"paragraph _abstract\">\n<p>To make it easier to quickly execute pipeline runs with the same configuration in a pipeline experiment, you can duplicate them.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>An active run is available to duplicate in the <strong>Active</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Experiments</strong> &#8594; <strong>Experiments and runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Experiments</strong> page, from the <strong>Project</strong> drop-down list, select the project that contains the pipeline experiment for the pipeline run that you want to duplicate.</p>\n</li>\n<li>\n<p>From the list of pipeline experiments, click the experiment that contains the pipeline run that you want to duplicate.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the relevant active run and click <strong>Duplicate</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Duplicate run</strong> page, configure the duplicate run:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>From the <strong>Experiment</strong> list, select the pipeline experiment that contains the pipeline run that you want to duplicate. Alternatively, to create a new pipeline experiment, click <strong>Create new experiment</strong>, and then complete the relevant fields in the <strong>Create experiment</strong> dialog.</p>\n</li>\n<li>\n<p>In the <strong>Name</strong> field, enter a name for the duplicate run.</p>\n</li>\n<li>\n<p>In the <strong>Description</strong> field, enter a description for the duplicate run.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline</strong> list, select the pipeline to contain the duplicate run. Alternatively, to create a new pipeline, click <strong>Create new pipeline</strong>, and then complete the relevant fields in the <strong>Import pipeline</strong> dialog.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline version</strong> list, select the pipeline version to contain the duplicate run. Alternatively, to upload a new version, click <strong>Upload new version</strong>, and then complete the relevant fields in the <strong>Upload new version</strong> dialog.</p>\n</li>\n<li>\n<p>In the <strong>Parameters</strong> section, configure the input parameters for the run that you are duplicating by selecting the appropriate parameters from the list.</p>\n</li>\n<li>\n<p>Click <strong>Create run</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The duplicate pipeline run appears in the <strong>Runs</strong> tab on the <strong>Runs</strong> page for the pipeline experiment.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-scheduled-pipeline-runs_ds-pipelines\">Viewing scheduled pipeline runs</h3>\n<div class=\"paragraph _abstract\">\n<p>You can view a list of pipeline runs that are scheduled for execution in a pipeline experiment. From this list, you can view details relating to your pipeline runs, such as the pipeline version that the run belongs to. You can also view the run status, execution frequency, and schedule.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have scheduled a pipeline run that is available to view.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Experiments</strong> &#8594; <strong>Experiments and runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Experiments</strong> page, from the <strong>Project</strong> drop-down list, select the project that contains the pipeline experiment for the scheduled pipeline runs that you want to view.</p>\n</li>\n<li>\n<p>From the list of pipeline experiments, click the experiment that contains the pipeline runs that you want to view.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, click the <strong>Schedules</strong> tab.</p>\n</li>\n<li>\n<p>Study the table showing a list of scheduled runs.</p>\n<div class=\"paragraph\">\n<p>After a run has been scheduled, the run&#8217;s status indicates whether the run is ready for execution or unavailable for execution. To change its execution availability, click the run&#8217;s Status icon.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>A list of scheduled runs appears in the <strong>Schedules</strong> tab on the <strong>Runs</strong> page for the pipeline experiment.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"scheduling-a-pipeline-run-using-a-cron-job_ds-pipelines\">Scheduling a pipeline run using a cron job</h3>\n<div class=\"paragraph _abstract\">\n<p>You can use a cron job to schedule a pipeline run to execute at a specific time. Cron jobs are useful for creating periodic and recurring tasks, and can also schedule individual tasks for a specific time, such as if you want to schedule a run for a low activity period. To successfully execute runs in Open Data Hub, you must use the supported format. See <a href=\"https://pkg.go.dev/github.com/robfig/cron#hdr-CRON_Expression_Format\">Cron Expression Format</a> for more information.</p>\n</div>\n<div class=\"paragraph\">\n<p>The following examples show the correct format:</p>\n</div>\n<table class=\"tableblock frame-all grid-all stretch\">\n<colgroup>\n<col style=\"width: 50%;\">\n<col style=\"width: 50%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Run occurrence</th>\n<th class=\"tableblock halign-left valign-top\">Cron format</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Every five minutes</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">@every 5m</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Every 10 minutes</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">0 */10 * * * *</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Daily at 16:16 UTC</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">0 16 16 * * *</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Daily every quarter of the hour</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">0 0,15,30,45 * * * *</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">On Monday and Tuesday at 15:40 UTC</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">0 40 15 * * MON,TUE</p></td>\n</tr>\n</tbody>\n</table>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://pkg.go.dev/github.com/robfig/cron#hdr-CRON_Expression_Format\">Cron Expression Format</a></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"scheduling-a-pipeline-run_ds-pipelines\">Scheduling a pipeline run</h3>\n<div class=\"paragraph _abstract\">\n<p>To repeatedly run a pipeline, you can create a scheduled pipeline run.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Experiments</strong> &#8594; <strong>Experiments and runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Experiments</strong> page, from the <strong>Project</strong> drop-down list, select the project that contains the pipeline experiment for the run that you want to schedule.</p>\n</li>\n<li>\n<p>From the list of pipeline experiments, click the experiment that contains the run that you want to schedule.</p>\n</li>\n<li>\n<p>Click the <strong>Schedules</strong> tab.</p>\n</li>\n<li>\n<p>Click <strong>Create schedule</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Schedule run</strong> page, configure the run that you are scheduling:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>From the <strong>Experiment</strong> list, select the pipeline experiment that you want to contain the scheduled run. Alternatively, to create a new pipeline experiment, click <strong>Create new experiment</strong>, and then complete the relevant fields in the <strong>Create experiment</strong> dialog.</p>\n</li>\n<li>\n<p>In the <strong>Name</strong> field, enter a name for the run.</p>\n</li>\n<li>\n<p>In the <strong>Description</strong> field, enter a description for the run.</p>\n</li>\n<li>\n<p>From the <strong>Trigger type</strong> list, select one of the following options:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Select <strong>Periodic</strong> to specify an execution frequency. In the <strong>Run every</strong> field, enter a numerical value and select an execution frequency from the list.</p>\n</li>\n<li>\n<p>Select <strong>Cron</strong> to specify the execution schedule in <code>cron</code> format in the <strong>Cron string</strong> field. This creates a cron job to execute the run. Click the <strong>Copy</strong> button (<span class=\"image\"><img src=\"/static/docs/images/osd-copy.png\" alt=\"osd copy\"></span>) to copy the cron job schedule to the clipboard. The field furthest to the left represents seconds. For more information about scheduling tasks using the supported <code>cron</code> format, see <a href=\"https://pkg.go.dev/github.com/robfig/cron#hdr-CRON_Expression_Format\">Cron Expression Format</a>.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>In the <strong>Maximum concurrent runs</strong> field, specify the number of runs that can execute concurrently, from a range of one to ten.</p>\n</li>\n<li>\n<p>For <strong>Start date</strong>, specify a start date for the run. Select a start date using the <strong>Calendar</strong> and the start time from the list of times.</p>\n</li>\n<li>\n<p>For <strong>End date</strong>, specify an end date for the run. Select an end date using the <strong>Calendar</strong> and the end time from the list of times.</p>\n</li>\n<li>\n<p>For <strong>Catch up</strong>, enable or disable catch up runs. You can use catch up runs to ensure your pipeline runs do not permanently fall behind schedule when paused. For example, if you re-enable a paused recurring run, the run scheduler backfills each missed run interval.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline</strong> list, select the pipeline that you want to create a run for. Alternatively, to create a new pipeline, click <strong>Create new pipeline</strong>, and then complete the relevant fields in the <strong>Import pipeline</strong> dialog.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline version</strong> list, select the pipeline version to create a run for. Alternatively, to upload a new version, click <strong>Upload new version</strong>, and then complete the relevant fields in the <strong>Upload new version</strong> dialog.</p>\n</li>\n<li>\n<p>Configure the input parameters for the run by selecting the parameters from the list.</p>\n</li>\n<li>\n<p>Click <strong>Schedule run</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline run that you scheduled appears in the <strong>Schedules</strong> tab on the <strong>Runs</strong> page for the pipeline experiment.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"duplicating-a-scheduled-pipeline-run_ds-pipelines\">Duplicating a scheduled pipeline run</h3>\n<div class=\"paragraph _abstract\">\n<p>To make it easier to schedule runs to execute as part of your pipeline experiment, you can duplicate existing scheduled runs.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>A scheduled run is available to duplicate in the <strong>Schedules</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Experiments</strong> &#8594; <strong>Experiments and runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Experiments</strong> page, from the <strong>Project</strong> drop-down list, select the project that contains the pipeline experiment for the scheduled run that you want to duplicate.</p>\n</li>\n<li>\n<p>From the list of pipeline experiments, click the experiment that contains the pipeline run that you want to duplicate.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, click the <strong>Schedules</strong> tab.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the run that you want to duplicate and click <strong>Duplicate</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Duplicate schedule</strong> page, configure the duplicate run:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>From the <strong>Experiment</strong> list, select the pipeline experiment that contains the scheduled pipeline run that you want to duplicate. Alternatively, to create a new pipeline experiment, click <strong>Create new experiment</strong>, and then and complete the relevant fields in the <strong>Create experiment</strong> dialog.</p>\n</li>\n<li>\n<p>In the <strong>Name</strong> field, enter a name for the duplicate run.</p>\n</li>\n<li>\n<p>In the <strong>Description</strong> field, enter a description for the duplicate run.</p>\n</li>\n<li>\n<p>From the <strong>Trigger type</strong> list, select one of the following options:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Select <strong>Periodic</strong> to specify an execution frequency. In the <strong>Run every</strong> field, enter a numerical value and select an execution frequency from the list.</p>\n</li>\n<li>\n<p>Select <strong>Cron</strong> to specify the execution schedule in <code>cron</code> format in the <strong>Cron string</strong> field. This creates a cron job to execute the run. Click the <strong>Copy</strong> button (<span class=\"image\"><img src=\"/static/docs/images/osd-copy.png\" alt=\"osd copy\"></span>) to copy the cron job schedule to the clipboard. The field furthest to the left represents seconds. For more information about scheduling tasks using the supported <code>cron</code> format, see <a href=\"https://pkg.go.dev/github.com/robfig/cron#hdr-CRON_Expression_Format\">Cron Expression Format</a>.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>For <strong>Maximum concurrent runs</strong>, specify the number of runs that can execute concurrently, from a range of one to ten.</p>\n</li>\n<li>\n<p>For <strong>Start date</strong>, specify a start date for the duplicate run. Select a start date using the <strong>Calendar</strong> and the start time from the list of times.</p>\n</li>\n<li>\n<p>For <strong>End date</strong>, specify an end date for the duplicate run. Select an end date using the <strong>Calendar</strong> and the end time from the list of times.</p>\n</li>\n<li>\n<p>For <strong>Catch up</strong>, enable or disable catch up runs. You can use catch up runs to ensure your pipeline runs do not permanently fall behind schedule when paused. For example, if you re-enable a paused recurring run, the run scheduler backfills each missed run interval.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline</strong> list, select the pipeline that you want to create a duplicate run for. Alternatively, to create a new pipeline, click <strong>Create new pipeline</strong>, and then complete the relevant fields in the <strong>Import pipeline</strong> dialog.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline version</strong> list, select the pipeline version to create a duplicate run for. Alternatively, to upload a new version, click <strong>Upload new version</strong>, and then complete the relevant fields in the <strong>Upload new version</strong> dialog.</p>\n</li>\n<li>\n<p>Configure the input parameters for the run by selecting the parameters from the list.</p>\n</li>\n<li>\n<p>Click <strong>Schedule run</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline run that you duplicated appears in the <strong>Schedules</strong> tab on the <strong>Runs</strong> page for the pipeline experiment.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"deleting-a-scheduled-pipeline-run_ds-pipelines\">Deleting a scheduled pipeline run</h3>\n<div class=\"paragraph _abstract\">\n<p>To discard pipeline runs that you previously scheduled, but no longer require, you can delete them so that they do not appear on the <strong>Schedules</strong> page.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have previously scheduled a run that is available to delete.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Experiments</strong> &#8594; <strong>Experiments and runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Experiments</strong> page, from the <strong>Project</strong> drop-down list, select the project that contains the pipeline experiment for the scheduled pipeline run that you want to delete.</p>\n</li>\n<li>\n<p>From the list of pipeline experiments, click the experiment that contains the scheduled pipeline run that you want to delete.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, click the <strong>Schedules</strong> tab.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the scheduled pipeline run that you want to delete, and then click <strong>Delete</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Delete schedule</strong> dialog, enter the run name in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The run that you deleted no longer appears on the <strong>Schedules</strong> tab for the pipeline experiment.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-the-details-of-a-pipeline-run_ds-pipelines\">Viewing the details of a pipeline run</h3>\n<div class=\"paragraph _abstract\">\n<p>To gain a clearer understanding of your pipeline runs, you can view the details of a previously triggered pipeline run, such as its graph, execution details, and run output.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have previously triggered a pipeline run.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Pipelines</strong> page, select the <strong>project</strong> that you want to view run details for.</p>\n</li>\n<li>\n<p>For a pipeline that you want to view run details for, click <strong>Expand</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhoai-expand-icon.png\" alt=\"rhoai expand icon\"></span>).</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) for the pipeline version and then click <strong>View runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, click the name of the run that you want to view the details of.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>On the <strong>Run details</strong> page, you can view the run&#8217;s graph, execution details, input parameters, step logs, and run output.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-archived-pipeline-runs_ds-pipelines\">Viewing archived pipeline runs</h3>\n<div class=\"paragraph _abstract\">\n<p>You can view a list of pipeline runs that you have archived. You can view details for your archived pipeline runs, such as the pipeline version, run status, duration, and execution start date.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and has a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>An archived pipeline run exists.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Experiments</strong> &#8594; <strong>Experiments and runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Experiments</strong> page, from the <strong>Project</strong> drop-down list, select the project that contains the pipeline experiment for the archived pipeline runs that you want to view.</p>\n</li>\n<li>\n<p>From the list of pipeline experiments, click the experiment that contains the archived pipeline runs that you want to view.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, click the <strong>Archive</strong> tab.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>A list of archived runs appears in the <strong>Archive</strong> tab on the <strong>Runs</strong> page for the pipeline experiment.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"archiving-a-pipeline-run_ds-pipelines\">Archiving a pipeline run</h3>\n<div class=\"paragraph _abstract\">\n<p>You can retain records of your pipeline runs by archiving them. If required, you can restore runs from your archive to reuse, or delete runs that are no longer required.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and has a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have previously executed a pipeline run that is available.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Experiments</strong> &#8594; <strong>Experiments and runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Experiments</strong> page, from the <strong>Project</strong> drop-down list, select the project that contains the pipeline experiment for the run that you want to archive.</p>\n</li>\n<li>\n<p>From the list of pipeline experiments, click the experiment that contains the pipeline run that you want to archive.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, click the <strong>Runs</strong> tab.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the pipeline run that you want to archive, and then click <strong>Archive</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Archiving run</strong> dialog, enter the run name in the text field to confirm that you intend to archive it.</p>\n</li>\n<li>\n<p>Click <strong>Archive</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The archived run does not appear in the <strong>Runs</strong> tab, and instead appears in the <strong>Archive</strong> tab on the <strong>Runs</strong> page for the pipeline experiment.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"restoring-an-archived-pipeline-run_ds-pipelines\">Restoring an archived pipeline run</h3>\n<div class=\"paragraph _abstract\">\n<p>You can restore an archived run to the active state.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and has a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>An archived run exists in your project.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Experiments</strong> &#8594; <strong>Experiments and runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Experiments</strong> page, from the <strong>Project</strong> drop-down list, select the project that contains the pipeline experiment that you want to restore.</p>\n</li>\n<li>\n<p>From the list of pipeline experiments, click the experiment that contains the archived pipeline run that you want to restore.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, click the <strong>Archive</strong> tab.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the pipeline run that you want to restore, and then click <strong>Restore</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Restore run</strong> dialog, click <strong>Restore</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The restored run appears in the <strong>Runs</strong> tab on the <strong>Runs</strong> page for the pipeline experiment.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"deleting-an-archived-pipeline-run_ds-pipelines\">Deleting an archived pipeline run</h3>\n<div class=\"paragraph _abstract\">\n<p>You can delete pipeline runs from the Open Data Hub run archive.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and has a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have previously archived a pipeline run.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, select the <strong>project</strong> for the archived pipeline run you want to delete.</p>\n</li>\n<li>\n<p>In the <strong>Run details</strong> page, click <strong>Archived</strong>.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the run that you want to delete and click <strong>Delete</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Delete run</strong> dialog, enter the run name in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The archived run that you deleted no longer appears in the <strong>Archived</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"duplicating-an-archived-pipeline-run_ds-pipelines\">Duplicating an archived pipeline run</h3>\n<div class=\"paragraph _abstract\">\n<p>To make it easier to reproduce runs with the same configuration as runs in your archive, you can duplicate them.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>An archived run is available to duplicate in the <strong>Archived</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, select the <strong>project</strong> that has the pipeline run that you want to duplicate.</p>\n</li>\n<li>\n<p>Click the <strong>Archived</strong> tab.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the relevant archived run and click <strong>Duplicate</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Duplicate run</strong> page, configure the duplicate run:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Name</strong> field, enter a name for the duplicate run.</p>\n</li>\n<li>\n<p>In the <strong>Description</strong> field, enter a description for the duplicate run.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline</strong> list, select the pipeline to contain the duplicate run.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline version</strong> list, select the pipeline version to contain the duplicate run.</p>\n</li>\n<li>\n<p>In the <strong>Parameters</strong> section, configure the input parameters for the run that you are duplicating by selecting the appropriate parameters from the list.</p>\n</li>\n<li>\n<p>Click <strong>Create run</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The duplicate pipeline run appears in the <strong>Active</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"working-with-pipeline-logs_ds-pipelines\">Working with pipeline logs</h2>\n<div class=\"sectionbody\">\n<div class=\"sect2\">\n<h3 id=\"about-pipeline-logs_ds-pipelines\">About pipeline logs</h3>\n<div class=\"paragraph _abstract\">\n<p>You can review and analyze step logs for each step in a triggered pipeline run.</p>\n</div>\n<div class=\"paragraph\">\n<p>To help you troubleshoot and audit your pipelines, you can review and analyze these step logs by using the log viewer in the Open Data Hub dashboard. From here, you can search for specific log messages, view the log for each step, and download the step logs to your local machine.</p>\n</div>\n<div class=\"paragraph\">\n<p>If the step log file exceeds its capacity, a warning appears above the log viewer stating that the log window displays partial content. Expanding the warning displays further information, such as how the log viewer refreshes every three seconds, and that each step log displays the last 500 lines of log messages received. In addition, you can click <strong>download all step logs</strong> to download all step logs to your local machine.</p>\n</div>\n<div class=\"paragraph\">\n<p>Each step has a set of container logs. You can view these container logs by selecting a container from the <strong>Steps</strong> list in the log viewer. The <code>Step-main</code> container log consists of the log output for the step. The <code>step-copy-artifact</code> container log consists of output relating to artifact data sent to s3-compatible storage. If the data transferred between the steps in your pipeline is larger than 3 KB, five container logs are typically available. These logs contain output relating to data transferred between your persistent volume claims (PVCs).</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-pipeline-step-logs_ds-pipelines\">Viewing pipeline step logs</h3>\n<div class=\"paragraph _abstract\">\n<p>To help you troubleshoot and audit your pipelines, you can review and analyze the log of each pipeline step using the log viewer. From here, you can search for specific log messages and download the logs for each step in your pipeline. If the pipeline is running, you can also pause and resume the log from the log viewer.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Logs are no longer stored in S3-compatible storage for Python scripts which are running in Elyra pipelines. From Open Data Hub version 2.14, you can view these logs in the pipeline step log viewer.</p>\n</div>\n<div class=\"paragraph\">\n<p>For this change to take effect, you must use the latest runtime images for Elyra, which are provided in the 2024.1 workbench images.</p>\n</div>\n<div class=\"paragraph\">\n<p>If you have an older workbench image version, update the <strong>Version selection</strong> field to <code>2024.1</code>, as described in <a href=\"https://opendatahub.io/docs/working-on-data-science-projects/#updating-a-project-workbench_projects\">Updating a project workbench</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>Updating your workbench image version will clear any existing runtime image selections for your pipeline. After you update your workbench version, open your workbench IDE and update the properties of your pipeline to select a runtime image.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have previously triggered a pipeline run.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Pipelines</strong> page, select the <strong>project</strong> that you want to view logs for.</p>\n</li>\n<li>\n<p>For the pipeline that you want to view logs for, click <strong>Expand</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhoai-expand-icon.png\" alt=\"rhoai expand icon\"></span>).</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) on the row containing the project version that you want to view pipeline logs for and click <strong>View runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, click the name of the run that you want to view logs for.</p>\n</li>\n<li>\n<p>On the graph on the <strong>Run details</strong> page , click the pipeline step that you want to view logs for.</p>\n</li>\n<li>\n<p>Click the <strong>Logs</strong> tab.</p>\n</li>\n<li>\n<p>To view the logs of another pipeline step, from the <strong>Steps</strong> list, select the step that you want to view logs for.</p>\n</li>\n<li>\n<p>Analyze the log using the log viewer.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>To search for a specific log message, enter at least part of the message in the search bar.</p>\n</li>\n<li>\n<p>To view the full log in a separate browser window, click the action menu (&#8942;) and select <strong>View raw logs</strong>. Alternatively, to expand the size of the log viewer, click the action menu (&#8942;) and select <strong>Expand</strong>.</p>\n</li>\n</ul>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>You can view the logs for each step in your pipeline.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"downloading-pipeline-step-logs_ds-pipelines\">Downloading pipeline step logs</h3>\n<div class=\"paragraph _abstract\">\n<p>Instead of viewing the step logs of a pipeline run using the log viewer on the Open Data Hub dashboard, you can download them for further analysis. You can choose to download the logs belonging to all steps in your pipeline, or you can download the log only for the step log displayed in the log viewer.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have previously triggered a pipeline run.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Pipelines</strong> page, select the <strong>project</strong> that you want to download logs for.</p>\n</li>\n<li>\n<p>For the pipeline that you want to download logs for, click <strong>Expand</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhoai-expand-icon.png\" alt=\"rhoai expand icon\"></span>).</p>\n</li>\n<li>\n<p>Click <strong>View runs</strong> on the row containing the pipeline version that you want to download logs for.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, click the name of the run that you want to download logs for.</p>\n</li>\n<li>\n<p>On the graph on the <strong>Run details</strong> page, click the pipeline step that you want to download logs for.</p>\n</li>\n<li>\n<p>Click the <strong>Logs</strong> tab.</p>\n</li>\n<li>\n<p>In the log viewer, click the <strong>Download</strong> button (<span class=\"image\"><img src=\"/static/docs/images/rhoai-download-icon.png\" alt=\"rhoai download icon\"></span>).</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Select <strong>Download current stop log</strong> to download the log for the current pipeline step.</p>\n</li>\n<li>\n<p>Select <strong>Download all step logs</strong> to download the logs for all steps in your pipeline run.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The step logs download to your browser&#8217;s default directory for downloaded files.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"working-with-pipelines-in-jupyterlab_ds-pipelines\">Working with pipelines in JupyterLab</h2>\n<div class=\"sectionbody\">\n<div class=\"sect2\">\n<h3 id=\"overview-of-pipelines-in-jupyterlab_ds-pipelines\">Overview of pipelines in JupyterLab</h3>\n<div class=\"paragraph _abstract\">\n<p>You can use Elyra to create visual end-to-end pipeline workflows in JupyterLab. Elyra is an extension for JupyterLab that provides you with a Pipeline Editor to create pipeline workflows that can be executed in Open Data Hub.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can access the Elyra extension within JupyterLab when you create the most recent version of one of the following notebook images:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Standard Data Science</p>\n</li>\n<li>\n<p>PyTorch</p>\n</li>\n<li>\n<p>TensorFlow</p>\n</li>\n<li>\n<p>TrustyAI</p>\n</li>\n<li>\n<p>HabanaAI</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>When you use the Pipeline Editor to visually design your pipelines, minimal coding is required to create and run pipelines. For more information about Elyra, see <a href=\"https://elyra.readthedocs.io/en/stable/getting_started/overview.html\">Elyra Documentation</a>. For more information about the Pipeline Editor, see <a href=\"https://elyra.readthedocs.io/en/stable/user_guide/jupyterlab-interface.html#visual-pipeline-editor\">Visual Pipeline Editor</a>. After you have created your pipeline, you can run it locally in JupyterLab, or remotely using data science pipelines in Open Data Hub.</p>\n</div>\n<div class=\"paragraph\">\n<p>The pipeline creation process consists of the following tasks:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Create a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>Create a pipeline server.</p>\n</li>\n<li>\n<p>Create a new pipeline in the Pipeline Editor in JupyterLab.</p>\n</li>\n<li>\n<p>Develop your pipeline by adding Python notebooks or Python scripts and defining their runtime properties.</p>\n</li>\n<li>\n<p>Define execution dependencies.</p>\n</li>\n<li>\n<p>Run or export your pipeline.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>Before you can run a pipeline in JupyterLab, your pipeline instance must contain a runtime configuration. A runtime configuration defines connectivity information for your pipeline instance and S3-compatible cloud storage.</p>\n</div>\n<div class=\"paragraph\">\n<p>If you create a workbench as part of a data science project, a default runtime configuration is created automatically. However, if you create a notebook from the Jupyter tile in the Open Data Hub dashboard, you must create a runtime configuration before you can run your pipeline in JupyterLab. For more information about runtime configurations, see <a href=\"https://elyra.readthedocs.io/en/stable/user_guide/runtime-conf.html\">Runtime Configuration</a>. As a prerequisite, before you create a workbench, ensure that you have created and configured a pipeline server within the same data science project as your workbench.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can use S3-compatible cloud storage to make data available to your notebooks and scripts while they are executed. Your cloud storage must be accessible from the machine in your deployment that runs JupyterLab and from the cluster that hosts data science pipelines. Before you create and run pipelines in JupyterLab, ensure that you have your s3-compatible storage credentials readily available.</p>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://elyra.readthedocs.io/en/stable/getting_started/overview.html\">Elyra Documentation</a></p>\n</li>\n<li>\n<p><a href=\"https://elyra.readthedocs.io/en/stable/user_guide/jupyterlab-interface.html#visual-pipeline-editor\">Visual Pipeline Editor</a></p>\n</li>\n<li>\n<p><a href=\"https://elyra.readthedocs.io/en/stable/user_guide/runtime-conf.html\">Runtime Configuration</a>.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"accessing-the-pipeline-editor_ds-pipelines\">Accessing the pipeline editor</h3>\n<div class=\"paragraph _abstract\">\n<p>You can use Elyra to create visual end-to-end pipeline workflows in JupyterLab. Elyra is an extension for JupyterLab that provides you with a Pipeline Editor to create pipeline workflows that can execute in Open Data Hub.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project.</p>\n</li>\n<li>\n<p>You have created a workbench with the <strong>Standard Data Science</strong> notebook image.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, PyTorch, or HabanaAI).</p>\n</li>\n<li>\n<p>You have access to S3-compatible storage.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>After you open JupyterLab, confirm that the JupyterLab launcher is automatically displayed.</p>\n</li>\n<li>\n<p>In the <strong>Elyra</strong> section of the JupyterLab launcher, click the <strong>Pipeline Editor</strong> tile.</p>\n<div class=\"paragraph\">\n<p>The Pipeline Editor opens.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>You can view the Pipeline Editor in JupyterLab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"creating-a-runtime-configuration_ds-pipelines\">Creating a runtime configuration</h3>\n<div class=\"paragraph _abstract\">\n<p>If you create a workbench as part of a data science project, a default runtime configuration is created automatically. However, if you create a notebook from the Jupyter tile in the Open Data Hub dashboard, you must create a runtime configuration before you can run your pipeline in JupyterLab. This enables you to specify connectivity information for your pipeline instance and S3-compatible cloud storage.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have access to S3-compatible cloud storage.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, PyTorch, or HabanaAI).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left sidebar of JupyterLab, click <strong>Runtimes</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>).</p>\n</li>\n<li>\n<p>Click the <strong>Create new runtime configuration</strong> button (<span class=\"image\"><img src=\"/static/docs/images/jupyter-create-runtime.png\" alt=\"Create new runtime configuration\"></span>).</p>\n<div class=\"paragraph\">\n<p>The <strong>Add new Data Science Pipelines runtime configuration</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Complete the relevant fields to define your runtime configuration.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Display Name</strong> field, enter a name for your runtime configuration.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Description</strong> field, enter a description to define your runtime configuration.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Tags</strong> field, click <strong>Add Tag</strong> to define a category for your pipeline instance. Enter a name for the tag and press Enter.</p>\n</li>\n<li>\n<p>Define the credentials of your data science pipeline:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint</strong> field, enter the API endpoint of your data science pipeline. Do not specify the pipelines namespace in this field.</p>\n</li>\n<li>\n<p>In the <strong>Public Data Science Pipelines API Endpoint</strong> field, enter the public API endpoint of your data science pipeline.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>You can obtain the data science pipelines API endpoint from the <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong> page in the dashboard. Copy the relevant end point and enter it in the <strong>Public Data Science Pipelines API Endpoint</strong> field.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Optional: In the <strong>Data Science Pipelines User Namespace</strong> field, enter the relevant user namespace to run pipelines.</p>\n</li>\n<li>\n<p>From the <strong>Authentication Type</strong> list, select the authentication type required to authenticate your pipeline.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you created a notebook directly from the Jupyter tile on the dashboard, select <code>EXISTING_BEARER_TOKEN</code> from the <strong>Authentication Type</strong> list.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint Username</strong> field, enter the user name required for the authentication type.</p>\n</li>\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint Password Or Token</strong>, enter the password or token required for the authentication type.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>To obtain the data science pipelines API endpoint token, in the upper-right corner of the OpenShift web console, click your user name and select <strong>Copy login command</strong>. After you have logged in, click <strong>Display token</strong> and copy the value of <code>--token=</code> from the <strong>Log in with this token</strong> command.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Define the connectivity information of your S3-compatible storage:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Cloud Object Storage Endpoint</strong> field, enter the endpoint of your S3-compatible storage. For more information about Amazon s3 endpoints, see <a href=\"https://docs.aws.amazon.com/general/latest/gr/s3.html\">Amazon Simple Storage Service endpoints and quotas</a>.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Public Cloud Object Storage Endpoint</strong> field, enter the URL of your S3-compatible storage.</p>\n</li>\n<li>\n<p>In the <strong>Cloud Object Storage Bucket Name</strong> field, enter the name of the bucket where your pipeline artifacts are stored. If the bucket name does not exist, it is created automatically.</p>\n</li>\n<li>\n<p>From the <strong>Cloud Object Storage Authentication Type</strong> list, select the authentication type required to access to your S3-compatible cloud storage. If you use AWS S3 buckets, select <code>KUBERNETES_SECRET</code> from the list.</p>\n</li>\n<li>\n<p>In the <strong>Cloud Object Storage Credentials Secret</strong> field, enter the secret that contains the storage user name and password. This secret is defined in the relevant user namespace, if applicable. In addition, it must be stored on the cluster that hosts your pipeline runtime.</p>\n</li>\n<li>\n<p>In the <strong>Cloud Object Storage Username</strong> field, enter the user name to connect to your S3-compatible cloud storage, if applicable. If you use AWS S3 buckets, enter your AWS Secret Access Key ID.</p>\n</li>\n<li>\n<p>In the <strong>Cloud Object Storage Password</strong> field, enter the password to connect to your S3-compatible cloud storage, if applicable. If you use AWS S3 buckets, enter your AWS Secret Access Key.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Save &amp; Close</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The runtime configuration that you created is shown in the <strong>Runtimes</strong> tab (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>) in the left sidebar of JupyterLab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"updating-a-runtime-configuration_ds-pipelines\">Updating a runtime configuration</h3>\n<div class=\"paragraph _abstract\">\n<p>To ensure that your runtime configuration is accurate and updated, you can change the settings of an existing runtime configuration.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have access to S3-compatible storage.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>A previously created runtime configuration is available in the JupyterLab interface.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, PyTorch, or HabanaAI).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left sidebar of JupyterLab, click <strong>Runtimes</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>).</p>\n</li>\n<li>\n<p>Hover the cursor over the runtime configuration that you want to update and click the <strong>Edit</strong> button (<span class=\"image\"><img src=\"/static/docs/images/rhoai-edit-icon.png\" alt=\"Edit runtime configuration\"></span>).</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Pipelines runtime configuration</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Fill in the relevant fields to update your runtime configuration.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Display Name</strong> field, update name for your runtime configuration, if applicable.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Description</strong> field, update the description of your runtime configuration, if applicable.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Tags</strong> field, click <strong>Add Tag</strong> to define a category for your pipeline instance. Enter a name for the tag and press Enter.</p>\n</li>\n<li>\n<p>Define the credentials of your data science pipeline:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint</strong> field, update the API endpoint of your data science pipeline, if applicable. Do not specify the pipelines namespace in this field.</p>\n</li>\n<li>\n<p>In the <strong>Public Data Science Pipelines API Endpoint</strong> field, update the API endpoint of your data science pipeline, if applicable.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Data Science Pipelines User Namespace</strong> field, update the relevant user namespace to run pipelines, if applicable.</p>\n</li>\n<li>\n<p>From the <strong>Authentication Type</strong> list, select a new authentication type required to authenticate your pipeline, if applicable.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you created a notebook directly from the Jupyter tile on the dashboard, select <code>EXISTING_BEARER_TOKEN</code> from the <strong>Authentication Type</strong> list.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint Username</strong> field, update the user name required for the authentication type, if applicable.</p>\n</li>\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint Password Or Token</strong>, update the password or token required for the authentication type, if applicable.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>To obtain the data science pipelines API endpoint token, in the upper-right corner of the OpenShift web console, click your user name and select <strong>Copy login command</strong>. After you have logged in, click <strong>Display token</strong> and copy the value of <code>--token=</code> from the <strong>Log in with this token</strong> command.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Define the connectivity information of your S3-compatible storage:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Cloud Object Storage Endpoint</strong> field, update the endpoint of your S3-compatible storage, if applicable. For more information about Amazon s3 endpoints, see <a href=\"https://docs.aws.amazon.com/general/latest/gr/s3.html\">Amazon Simple Storage Service endpoints and quotas</a>.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Public Cloud Object Storage Endpoint</strong> field, update the URL of your S3-compatible storage, if applicable.</p>\n</li>\n<li>\n<p>In the <strong>Cloud Object Storage Bucket Name</strong> field, update the name of the bucket where your pipeline artifacts are stored, if applicable. If the bucket name does not exist, it is created automatically.</p>\n</li>\n<li>\n<p>From the <strong>Cloud Object Storage Authentication Type</strong> list, update the authentication type required to access to your S3-compatible cloud storage, if applicable. If you use AWS S3 buckets, you must select <code>USER_CREDENTIALS</code> from the list.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Cloud Object Storage Credentials Secret</strong> field, update the secret that contains the storage user name and password, if applicable. This secret is defined in the relevant user namespace. You must save the secret on the cluster that hosts your pipeline runtime.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Cloud Object Storage Username</strong> field, update the user name to connect to your S3-compatible cloud storage, if applicable. If you use AWS S3 buckets, update your AWS Secret Access Key ID.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Cloud Object Storage Password</strong> field, update the password to connect to your S3-compatible cloud storage, if applicable. If you use AWS S3 buckets, update your AWS Secret Access Key.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Save &amp; Close</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The runtime configuration that you updated is shown in the <strong>Runtimes</strong> tab (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>) in the left sidebar of JupyterLab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"deleting-a-runtime-configuration_ds-pipelines\">Deleting a runtime configuration</h3>\n<div class=\"paragraph _abstract\">\n<p>After you have finished using your runtime configuration, you can delete it from the JupyterLab interface. After deleting a runtime configuration, you cannot run pipelines in JupyterLab until you create another runtime configuration.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>A previously created runtime configuration is visible in the JupyterLab interface.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, PyTorch, or HabanaAI).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left sidebar of JupyterLab, click <strong>Runtimes</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>).</p>\n</li>\n<li>\n<p>Hover the cursor over the runtime configuration that you want to delete and click the <strong>Delete Item</strong> button (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-trash-button.png\" alt=\"Delete item\"></span>).</p>\n<div class=\"paragraph\">\n<p>A dialog box appears prompting you to confirm the deletion of your runtime configuration.</p>\n</div>\n</li>\n<li>\n<p>Click <strong>OK</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The runtime configuration that you deleted is no longer shown in the <strong>Runtimes</strong> tab (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>) in the left sidebar of JupyterLab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"duplicating-a-runtime-configuration_ds-pipelines\">Duplicating a runtime configuration</h3>\n<div class=\"paragraph _abstract\">\n<p>To prevent you from re-creating runtime configurations with similar values in their entirety, you can duplicate an existing runtime configuration in the JupyterLab interface.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>A previously created runtime configuration is visible in the JupyterLab interface.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, PyTorch, or HabanaAI).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left sidebar of JupyterLab, click <strong>Runtimes</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>).</p>\n</li>\n<li>\n<p>Hover the cursor over the runtime configuration that you want to duplicate and click the <strong>Duplicate</strong> button (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-duplicate.png\" alt=\"Duplicate\"></span>).</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The runtime configuration that you duplicated is shown in the <strong>Runtimes</strong> tab (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>) in the left sidebar of JupyterLab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"running-a-pipeline-in-jupyterlab_ds-pipelines\">Running a pipeline in JupyterLab</h3>\n<div class=\"paragraph _abstract\">\n<p>You can run pipelines that you have created in JupyterLab from the Pipeline Editor user interface. Before you can run a pipeline, you must create a data science project and a pipeline server. After you create a pipeline server, you must create a workbench within the same project as your pipeline server.\nYour pipeline instance in JupyterLab must contain a runtime configuration. If you create a workbench as part of a data science project, a default runtime configuration is created automatically. However, if you create a notebook from the Jupyter tile in the Open Data Hub dashboard, you must create a runtime configuration before you can run your pipeline in JupyterLab. A runtime configuration defines connectivity information for your pipeline instance and S3-compatible cloud storage.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have access to S3-compatible storage.</p>\n</li>\n<li>\n<p>You have created a pipeline in JupyterLab.</p>\n</li>\n<li>\n<p>You have opened your pipeline in the Pipeline Editor in JupyterLab.</p>\n</li>\n<li>\n<p>Your pipeline instance contains a runtime configuration.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, PyTorch, or HabanaAI).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the Pipeline Editor user interface, click <strong>Run Pipeline</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-run-pipeline-button.png\" alt=\"The Runtimes icon\"></span>).</p>\n<div class=\"paragraph\">\n<p>The <strong>Run Pipeline</strong> dialog appears. The <strong>Pipeline Name</strong> field is automatically populated with the pipeline file name.</p>\n</div>\n</li>\n<li>\n<p>Define the settings for your pipeline run.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>From the <strong>Runtime Configuration</strong> list, select the relevant runtime configuration to run your pipeline.</p>\n</li>\n<li>\n<p>Optional: Configure your pipeline parameters, if applicable. If your pipeline contains nodes that reference pipeline parameters, you can change the default parameter values. If a parameter is required and has no default value, you must enter a value.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>OK</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>You can view the output artifacts of your pipeline run. The artifacts are stored in your designated object storage bucket.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"exporting-a-pipeline-in-jupyterlab_ds-pipelines\">Exporting a pipeline in JupyterLab</h3>\n<div class=\"paragraph _abstract\">\n<p>You can export pipelines that you have created in JupyterLab. When you export a pipeline, the pipeline is prepared for later execution, but is not uploaded or executed immediately. During the export process, any package dependencies are uploaded to S3-compatible storage. Also, pipeline code is generated for the target runtime.</p>\n</div>\n<div class=\"paragraph\">\n<p>Before you can export a pipeline, you must create a data science project and a pipeline server. After you create a pipeline server, you must create a workbench within the same project as your pipeline server. In addition, your pipeline instance in JupyterLab must contain a runtime configuration. If you create a workbench as part of a data science project, a default runtime configuration is created automatically. However, if you create a notebook from the Jupyter tile in the Open Data Hub dashboard, you must create a runtime configuration before you can export your pipeline in JupyterLab. A runtime configuration defines connectivity information for your pipeline instance and S3-compatible cloud storage.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>You have access to S3-compatible storage.</p>\n</li>\n<li>\n<p>You have a created a pipeline in JupyterLab.</p>\n</li>\n<li>\n<p>You have opened your pipeline in the Pipeline Editor in JupyterLab.</p>\n</li>\n<li>\n<p>Your pipeline instance contains a runtime configuration.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, PyTorch, or HabanaAI).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the Pipeline Editor user interface, click <strong>Export Pipeline</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-export-pipeline-button.png\" alt=\"Export pipeline\"></span>).</p>\n<div class=\"paragraph\">\n<p>The <strong>Export Pipeline</strong> dialog appears. The <strong>Pipeline Name</strong> field is automatically populated with the pipeline file name.</p>\n</div>\n</li>\n<li>\n<p>Define the settings to export your pipeline.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>From the <strong>Runtime Configuration</strong> list, select the relevant runtime configuration to export your pipeline.</p>\n</li>\n<li>\n<p>From the <strong>Export Pipeline as</strong> select an appropriate file format</p>\n</li>\n<li>\n<p>In the <strong>Export Filename</strong> field, enter a file name for the exported pipeline.</p>\n</li>\n<li>\n<p>Select the <strong>Replace if file already exists</strong> check box to replace an existing file of the same name as the pipeline you are exporting.</p>\n</li>\n<li>\n<p>Optional: Configure your pipeline parameters, if applicable. If your pipeline contains nodes that reference pipeline parameters, you can change the default parameter values. If a parameter is required and has no default value, you must enter a value.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>OK</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>You can view the file containing the pipeline that you exported in your designated object storage bucket.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1 _additional-resources\">\n<h2 id=\"_additional_resources\">Additional resources</h2>\n<div class=\"sectionbody\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p><a href=\"https://www.kubeflow.org/docs/components/pipelines/v2/\">Kubeflow Pipelines 2.0 Documentation</a></p>\n</li>\n</ul>\n</div>\n</div>\n</div>","id":"56d9f021-4335-5970-af6d-fbdcc318042b","document":{"title":"Working with data science pipelines"}},"markdownRemark":null},"pageContext":{"id":"56d9f021-4335-5970-af6d-fbdcc318042b"}},"staticQueryHashes":["2604506565"],"slicesMap":{}}