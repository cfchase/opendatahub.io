{"componentChunkName":"component---src-templates-docs-page-tsx","path":"/docs/working-on-data-science-projects/","result":{"data":{"allFile":{"edges":[{"node":{"childAsciidoc":{"fields":{"slug":"/docs/getting-started-with-open-data-hub/"},"sections":[{"parentId":null,"name":"Logging in to Open Data Hub","level":1,"index":0,"id":"logging-in_get-started"},{"parentId":null,"name":"The Open Data Hub user interface","level":1,"index":1,"id":"user-interface_get-started"},{"parentId":"user-interface_get-started","name":"Global navigation","level":2,"index":0,"id":"_global_navigation"},{"parentId":"user-interface_get-started","name":"Side navigation","level":2,"index":1,"id":"_side_navigation"},{"parentId":null,"name":"Notifications in Open Data Hub","level":1,"index":2,"id":"notifications_get-started"},{"parentId":null,"name":"Creating a data science project","level":1,"index":3,"id":"creating-a-data-science-project_get-started"},{"parentId":null,"name":"Creating a project workbench","level":1,"index":4,"id":"creating-a-project-workbench_get-started"},{"parentId":"creating-a-project-workbench_get-started","name":"Launching Jupyter and starting a notebook server","level":2,"index":0,"id":"launching-jupyter-and-starting-a-notebook-server_get-started"},{"parentId":"creating-a-project-workbench_get-started","name":"Options for notebook server environments","level":2,"index":1,"id":"options-for-notebook-server-environments_get-started"},{"parentId":null,"name":"Tutorials for data scientists","level":1,"index":5,"id":"tutorials-for-data-scientists_get-started"},{"parentId":"tutorials-for-data-scientists_get-started","name":"Accessing tutorials","level":2,"index":0,"id":"accessing-tutorials_get-started"},{"parentId":null,"name":"Enabling services connected to Open Data Hub","level":1,"index":6,"id":"enabling-services_get-started"},{"parentId":null,"name":"Disabling applications connected to Open Data Hub","level":1,"index":7,"id":"disabling-applications_get-started"},{"parentId":"disabling-applications_get-started","name":"Removing disabled applications from Open Data Hub","level":2,"index":0,"id":"removing-disabled-applications_get-started"},{"parentId":null,"name":"Support requirements and limitations","level":1,"index":8,"id":"support-requirements-and-limitations_requirements"},{"parentId":"support-requirements-and-limitations_requirements","name":"Supported browsers","level":2,"index":0,"id":"supported-browsers_requirements"},{"parentId":"support-requirements-and-limitations_requirements","name":"Supported services","level":2,"index":1,"id":"supported-services_requirements"},{"parentId":"support-requirements-and-limitations_requirements","name":"Supported packages","level":2,"index":2,"id":"supported-packages_requirements"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-on-data-science-projects/"},"sections":[{"parentId":null,"name":"Creating and importing notebooks","level":1,"index":0,"id":"creating-and-importing-notebooks_notebooks"},{"parentId":"creating-and-importing-notebooks_notebooks","name":"Creating a new notebook","level":2,"index":0,"id":"creating-a-new-notebook_notebooks"},{"parentId":"creating-a-new-notebook_notebooks","name":"Notebook images for data scientists","level":3,"index":0,"id":"notebook-images-for-data-scientists_notebooks"},{"parentId":"creating-and-importing-notebooks_notebooks","name":"Uploading an existing notebook file from local storage","level":2,"index":1,"id":"uploading-an-existing-notebook-file-from-local-storage_notebooks"},{"parentId":"creating-and-importing-notebooks_notebooks","name":"Uploading an existing notebook file from a Git repository using JupyterLab","level":2,"index":2,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_notebooks"},{"parentId":"creating-and-importing-notebooks_notebooks","name":"Uploading an existing notebook file from a Git repository using the command line interface","level":2,"index":3,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_notebooks"},{"parentId":null,"name":"Collaborating on notebooks using Git","level":1,"index":1,"id":"collaborating-on-notebooks-using-git_git-collab"},{"parentId":"collaborating-on-notebooks-using-git_git-collab","name":"Uploading an existing notebook file from a Git repository using JupyterLab","level":2,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_git-collab"},{"parentId":"collaborating-on-notebooks-using-git_git-collab","name":"Uploading an existing notebook file from a Git repository using the command line interface","level":2,"index":1,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_git-collab"},{"parentId":"collaborating-on-notebooks-using-git_git-collab","name":"Updating your project with changes from a remote Git repository","level":2,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_git-collab"},{"parentId":"collaborating-on-notebooks-using-git_git-collab","name":"Pushing project changes to a Git repository","level":2,"index":3,"id":"pushing-project-changes-to-a-git-repository_git-collab"},{"parentId":null,"name":"Working on data science projects","level":1,"index":2,"id":"working-on-data-science-projects_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Using data science projects","level":2,"index":0,"id":"_using_data_science_projects"},{"parentId":"_using_data_science_projects","name":"Creating a data science project","level":3,"index":0,"id":"creating-a-data-science-project_nb-server"},{"parentId":"_using_data_science_projects","name":"Updating a data science project","level":3,"index":1,"id":"updating-a-data-science-project_nb-server"},{"parentId":"_using_data_science_projects","name":"Deleting a data science project","level":3,"index":2,"id":"deleting-a-data-science-project_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Using project workbenches","level":2,"index":1,"id":"_using_project_workbenches"},{"parentId":"_using_project_workbenches","name":"Creating a project workbench","level":3,"index":0,"id":"creating-a-project-workbench_nb-server"},{"parentId":"_using_project_workbenches","name":"Starting a workbench","level":3,"index":1,"id":"starting-a-workbench_nb-server"},{"parentId":"_using_project_workbenches","name":"Updating a project workbench","level":3,"index":2,"id":"updating-a-project-workbench_nb-server"},{"parentId":"_using_project_workbenches","name":"Deleting a workbench from a data science project","level":3,"index":3,"id":"deleting-a-workbench-from-a-data-science-project_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Using data connections","level":2,"index":2,"id":"_using_data_connections"},{"parentId":"_using_data_connections","name":"Adding a data connection to your data science project","level":3,"index":0,"id":"adding-a-data-connection-to-your-data-science-project_nb-server"},{"parentId":"_using_data_connections","name":"Deleting a data connection","level":3,"index":1,"id":"deleting-a-data-connection_nb-server"},{"parentId":"_using_data_connections","name":"Updating a connected data source","level":3,"index":2,"id":"updating-a-connected-data-source_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Configuring cluster storage","level":2,"index":3,"id":"_configuring_cluster_storage"},{"parentId":"_configuring_cluster_storage","name":"Adding cluster storage to your data science project","level":3,"index":0,"id":"adding-cluster-storage-to-your-data-science-project_nb-server"},{"parentId":"_configuring_cluster_storage","name":"Updating cluster storage","level":3,"index":1,"id":"updating-cluster-storage_nb-server"},{"parentId":"_configuring_cluster_storage","name":"Deleting cluster storage from a data science project","level":3,"index":2,"id":"deleting-cluster-storage-from-a-data-science-project_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Configuring model servers","level":2,"index":4,"id":"_configuring_model_servers"},{"parentId":"_configuring_model_servers","name":"Configuring a model server for your data science project","level":3,"index":0,"id":"configuring-a-model-server-for-your-data-science-project_nb-server"},{"parentId":"_configuring_model_servers","name":"Adding a custom model-serving runtime","level":3,"index":1,"id":"adding-a-custom-model-serving-runtime_nb-server"},{"parentId":"_configuring_model_servers","name":"Updating a model server","level":3,"index":2,"id":"updating-a-model-server_nb-server"},{"parentId":"_configuring_model_servers","name":"Deleting a model server","level":3,"index":3,"id":"deleting-a-model-server_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Configuring access to data science projects","level":2,"index":5,"id":"_configuring_access_to_data_science_projects"},{"parentId":"_configuring_access_to_data_science_projects","name":"Configuring access to data science projects","level":3,"index":0,"id":"configuring-access-to-data-science-projects_nb-server"},{"parentId":"_configuring_access_to_data_science_projects","name":"Sharing access to a data science project","level":3,"index":1,"id":"sharing-access-to-a-data-science-project_nb-server"},{"parentId":"_configuring_access_to_data_science_projects","name":"Updating access to a data science project","level":3,"index":2,"id":"updating-access-to-a-data-science-project_nb-server"},{"parentId":"_configuring_access_to_data_science_projects","name":"Removing access to a data science project","level":3,"index":3,"id":"removing-access-to-a-data-science-project_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Viewing Python packages installed on your notebook server","level":2,"index":6,"id":"viewing-python-packages-installed-on-your-notebook-server_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Installing Python packages on your notebook server","level":2,"index":7,"id":"installing-python-packages-on-your-notebook-server_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Updating notebook server settings by restarting your server","level":2,"index":8,"id":"updating-notebook-server-settings-by-restarting-your-server_nb-server"},{"parentId":null,"name":"Working with data science pipelines","level":1,"index":3,"id":"working-with-data-science-pipelines_ds-pipelines"},{"parentId":"working-with-data-science-pipelines_ds-pipelines","name":"Managing data science pipelines","level":2,"index":0,"id":"_managing_data_science_pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Configuring a pipeline server","level":3,"index":0,"id":"configuring-a-pipeline-server_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Defining a pipeline","level":3,"index":1,"id":"defining-a-pipeline_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Importing a data science pipeline","level":3,"index":2,"id":"importing-a-data-science-pipeline_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Downloading a data science pipeline","level":3,"index":3,"id":"downloading-a-data-science-pipeline_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Deleting a data science pipeline","level":3,"index":4,"id":"deleting-a-data-science-pipeline_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Deleting a pipeline server","level":3,"index":5,"id":"deleting-a-pipeline-server_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Viewing the details of a pipeline server","level":3,"index":6,"id":"viewing-the-details-of-a-pipeline-server_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Viewing existing pipelines","level":3,"index":7,"id":"viewing-existing-pipelines_ds-pipelines"},{"parentId":"working-with-data-science-pipelines_ds-pipelines","name":"Managing pipeline runs","level":2,"index":1,"id":"_managing_pipeline_runs"},{"parentId":"_managing_pipeline_runs","name":"Overview of pipeline runs","level":3,"index":0,"id":"overview-of-pipeline-runs_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Scheduling a pipeline run","level":3,"index":1,"id":"scheduling-a-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Cloning a scheduled pipeline run","level":3,"index":2,"id":"cloning-a-scheduled-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Stopping a triggered pipeline run","level":3,"index":3,"id":"stopping-a-triggered-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Deleting a scheduled pipeline run","level":3,"index":4,"id":"deleting-a-scheduled-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Deleting a triggered pipeline run","level":3,"index":5,"id":"deleting-a-triggered-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Viewing scheduled pipeline runs","level":3,"index":6,"id":"viewing-scheduled-pipeline-runs_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Viewing triggered pipeline runs","level":3,"index":7,"id":"viewing-triggered-pipeline-runs_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Viewing the details of a pipeline run","level":3,"index":8,"id":"viewing-the-details-of-a-pipeline-run_ds-pipelines"},{"parentId":"working-with-data-science-pipelines_ds-pipelines","name":"Working with pipelines in JupyterLab","level":2,"index":2,"id":"_working_with_pipelines_in_jupyterlab"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Overview of pipelines in JupyterLab","level":3,"index":0,"id":"overview-of-pipelines-in-jupyterlab_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Accessing the pipeline editor","level":3,"index":1,"id":"accessing-the-pipeline-editor_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Creating a runtime configuration","level":3,"index":2,"id":"creating-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Updating a runtime configuration","level":3,"index":3,"id":"updating-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Deleting a runtime configuration","level":3,"index":4,"id":"deleting-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Duplicating a runtime configuration","level":3,"index":5,"id":"duplicating-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Running a pipeline in JupyterLab","level":3,"index":6,"id":"running-a-pipeline-in-jupyterlab_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Exporting a pipeline in JupyterLab","level":3,"index":7,"id":"exporting-a-pipeline-in-jupyterlab_ds-pipelines"},{"parentId":"working-with-data-science-pipelines_ds-pipelines","name":"Additional resources","level":2,"index":3,"id":"_additional_resources"},{"parentId":null,"name":"Model serving on Open Data Hub","level":1,"index":4,"id":"model-serving-on-openshift-data-science_model-serving"},{"parentId":"model-serving-on-openshift-data-science_model-serving","name":"Configuring model servers","level":2,"index":0,"id":"_configuring_model_servers_2"},{"parentId":"_configuring_model_servers_2","name":"Configuring a model server for your data science project","level":3,"index":0,"id":"configuring-a-model-server-for-your-data-science-project_model-serving"},{"parentId":"_configuring_model_servers_2","name":"Adding a custom model-serving runtime","level":3,"index":1,"id":"adding-a-custom-model-serving-runtime_model-serving"},{"parentId":"_configuring_model_servers_2","name":"Updating a model server","level":3,"index":2,"id":"updating-a-model-server_model-serving"},{"parentId":"_configuring_model_servers_2","name":"Deleting a model server","level":3,"index":3,"id":"deleting-a-model-server_model-serving"},{"parentId":"model-serving-on-openshift-data-science_model-serving","name":"Working with deployed models","level":2,"index":1,"id":"_working_with_deployed_models"},{"parentId":"_working_with_deployed_models","name":"Deploying a model in Open Data Hub","level":3,"index":0,"id":"deploying-a-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Viewing a deployed model","level":3,"index":1,"id":"viewing-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Updating the deployment properties of a deployed model","level":3,"index":2,"id":"updating-the-deployment-properties-of-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Deleting a deployed model","level":3,"index":3,"id":"deleting-a-deployed-model_model-serving"},{"parentId":null,"name":"Troubleshooting common problems in Jupyter for administrators","level":1,"index":5,"id":"troubleshooting-common-problems-in-jupyter-for-administrators_model-serving"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_model-serving","name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":2,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_model-serving","name":"A user&#8217;s notebook server does not start","level":2,"index":1,"id":"_a_users_notebook_server_does_not_start"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_model-serving","name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":2,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"},{"parentId":null,"name":"Troubleshooting common problems in Jupyter for users","level":1,"index":6,"id":"troubleshooting-common-problems-in-jupyter-for-users_model-serving"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-users_model-serving","name":"I see a <strong>403: Forbidden</strong> error when I log in to Jupyter","level":2,"index":0,"id":"_i_see_a_403_forbidden_error_when_i_log_in_to_jupyter"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-users_model-serving","name":"My notebook server does not start","level":2,"index":1,"id":"_my_notebook_server_does_not_start"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-users_model-serving","name":"I see a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when I run my notebook cells","level":2,"index":2,"id":"_i_see_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_i_run_my_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/_artifacts/document-attributes-global/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/collaborating-on-notebooks-using-git/"},"sections":[{"parentId":null,"name":"Uploading an existing notebook file from a Git repository using JupyterLab","level":1,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_git-collab"},{"parentId":null,"name":"Uploading an existing notebook file from a Git repository using the command line interface","level":1,"index":1,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_git-collab"},{"parentId":null,"name":"Updating your project with changes from a remote Git repository","level":1,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_git-collab"},{"parentId":null,"name":"Pushing project changes to a Git repository","level":1,"index":3,"id":"pushing-project-changes-to-a-git-repository_git-collab"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/creating-and-importing-notebooks/"},"sections":[{"parentId":null,"name":"Creating a new notebook","level":1,"index":0,"id":"creating-a-new-notebook_notebooks"},{"parentId":"creating-a-new-notebook_notebooks","name":"Notebook images for data scientists","level":2,"index":0,"id":"notebook-images-for-data-scientists_notebooks"},{"parentId":null,"name":"Uploading an existing notebook file from local storage","level":1,"index":1,"id":"uploading-an-existing-notebook-file-from-local-storage_notebooks"},{"parentId":null,"name":"Uploading an existing notebook file from a Git repository using JupyterLab","level":1,"index":2,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_notebooks"},{"parentId":null,"name":"Uploading an existing notebook file from a Git repository using the command line interface","level":1,"index":3,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_notebooks"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/model-serving/"},"sections":[{"parentId":null,"name":"Configuring model servers","level":1,"index":0,"id":"_configuring_model_servers"},{"parentId":"_configuring_model_servers","name":"Configuring a model server for your data science project","level":2,"index":0,"id":"configuring-a-model-server-for-your-data-science-project_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a custom model-serving runtime","level":2,"index":1,"id":"adding-a-custom-model-serving-runtime_model-serving"},{"parentId":"_configuring_model_servers","name":"Updating a model server","level":2,"index":2,"id":"updating-a-model-server_model-serving"},{"parentId":"_configuring_model_servers","name":"Deleting a model server","level":2,"index":3,"id":"deleting-a-model-server_model-serving"},{"parentId":null,"name":"Working with deployed models","level":1,"index":1,"id":"_working_with_deployed_models"},{"parentId":"_working_with_deployed_models","name":"Deploying a model in {productname-short}","level":2,"index":0,"id":"deploying-a-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Viewing a deployed model","level":2,"index":1,"id":"viewing-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Updating the deployment properties of a deployed model","level":2,"index":2,"id":"updating-the-deployment-properties-of-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Deleting a deployed model","level":2,"index":3,"id":"deleting-a-deployed-model_model-serving"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/support-requirements-and-limitations/"},"sections":[{"parentId":null,"name":"Supported browsers","level":1,"index":0,"id":"supported-browsers_requirements"},{"parentId":null,"name":"Supported services","level":1,"index":1,"id":"supported-services_requirements"},{"parentId":null,"name":"Supported packages","level":1,"index":2,"id":"supported-packages_requirements"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-on-data-science-projects/"},"sections":[{"parentId":null,"name":"Using data science projects","level":1,"index":0,"id":"_using_data_science_projects"},{"parentId":"_using_data_science_projects","name":"Creating a data science project","level":2,"index":0,"id":"creating-a-data-science-project_nb-server"},{"parentId":"_using_data_science_projects","name":"Updating a data science project","level":2,"index":1,"id":"updating-a-data-science-project_nb-server"},{"parentId":"_using_data_science_projects","name":"Deleting a data science project","level":2,"index":2,"id":"deleting-a-data-science-project_nb-server"},{"parentId":null,"name":"Using project workbenches","level":1,"index":1,"id":"_using_project_workbenches"},{"parentId":"_using_project_workbenches","name":"Creating a project workbench","level":2,"index":0,"id":"creating-a-project-workbench_nb-server"},{"parentId":"_using_project_workbenches","name":"Starting a workbench","level":2,"index":1,"id":"starting-a-workbench_nb-server"},{"parentId":"_using_project_workbenches","name":"Updating a project workbench","level":2,"index":2,"id":"updating-a-project-workbench_nb-server"},{"parentId":"_using_project_workbenches","name":"Deleting a workbench from a data science project","level":2,"index":3,"id":"deleting-a-workbench-from-a-data-science-project_nb-server"},{"parentId":null,"name":"Using data connections","level":1,"index":2,"id":"_using_data_connections"},{"parentId":"_using_data_connections","name":"Adding a data connection to your data science project","level":2,"index":0,"id":"adding-a-data-connection-to-your-data-science-project_nb-server"},{"parentId":"_using_data_connections","name":"Deleting a data connection","level":2,"index":1,"id":"deleting-a-data-connection_nb-server"},{"parentId":"_using_data_connections","name":"Updating a connected data source","level":2,"index":2,"id":"updating-a-connected-data-source_nb-server"},{"parentId":null,"name":"Configuring cluster storage","level":1,"index":3,"id":"_configuring_cluster_storage"},{"parentId":"_configuring_cluster_storage","name":"Adding cluster storage to your data science project","level":2,"index":0,"id":"adding-cluster-storage-to-your-data-science-project_nb-server"},{"parentId":"_configuring_cluster_storage","name":"Updating cluster storage","level":2,"index":1,"id":"updating-cluster-storage_nb-server"},{"parentId":"_configuring_cluster_storage","name":"Deleting cluster storage from a data science project","level":2,"index":2,"id":"deleting-cluster-storage-from-a-data-science-project_nb-server"},{"parentId":null,"name":"Configuring model servers","level":1,"index":4,"id":"_configuring_model_servers"},{"parentId":"_configuring_model_servers","name":"Configuring a model server for your data science project","level":2,"index":0,"id":"configuring-a-model-server-for-your-data-science-project_nb-server"},{"parentId":"_configuring_model_servers","name":"Adding a custom model-serving runtime","level":2,"index":1,"id":"adding-a-custom-model-serving-runtime_nb-server"},{"parentId":"_configuring_model_servers","name":"Updating a model server","level":2,"index":2,"id":"updating-a-model-server_nb-server"},{"parentId":"_configuring_model_servers","name":"Deleting a model server","level":2,"index":3,"id":"deleting-a-model-server_nb-server"},{"parentId":null,"name":"Configuring access to data science projects","level":1,"index":5,"id":"_configuring_access_to_data_science_projects"},{"parentId":"_configuring_access_to_data_science_projects","name":"Configuring access to data science projects","level":2,"index":0,"id":"configuring-access-to-data-science-projects_nb-server"},{"parentId":"_configuring_access_to_data_science_projects","name":"Sharing access to a data science project","level":2,"index":1,"id":"sharing-access-to-a-data-science-project_nb-server"},{"parentId":"_configuring_access_to_data_science_projects","name":"Updating access to a data science project","level":2,"index":2,"id":"updating-access-to-a-data-science-project_nb-server"},{"parentId":"_configuring_access_to_data_science_projects","name":"Removing access to a data science project","level":2,"index":3,"id":"removing-access-to-a-data-science-project_nb-server"},{"parentId":null,"name":"Viewing Python packages installed on your notebook server","level":1,"index":6,"id":"viewing-python-packages-installed-on-your-notebook-server_nb-server"},{"parentId":null,"name":"Installing Python packages on your notebook server","level":1,"index":7,"id":"installing-python-packages-on-your-notebook-server_nb-server"},{"parentId":null,"name":"Updating notebook server settings by restarting your server","level":1,"index":8,"id":"updating-notebook-server-settings-by-restarting-your-server_nb-server"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Managing data science pipelines","level":1,"index":0,"id":"_managing_data_science_pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Configuring a pipeline server","level":2,"index":0,"id":"configuring-a-pipeline-server_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Defining a pipeline","level":2,"index":1,"id":"defining-a-pipeline_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Importing a data science pipeline","level":2,"index":2,"id":"importing-a-data-science-pipeline_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Downloading a data science pipeline","level":2,"index":3,"id":"downloading-a-data-science-pipeline_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Deleting a data science pipeline","level":2,"index":4,"id":"deleting-a-data-science-pipeline_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Deleting a pipeline server","level":2,"index":5,"id":"deleting-a-pipeline-server_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Viewing the details of a pipeline server","level":2,"index":6,"id":"viewing-the-details-of-a-pipeline-server_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Viewing existing pipelines","level":2,"index":7,"id":"viewing-existing-pipelines_ds-pipelines"},{"parentId":null,"name":"Managing pipeline runs","level":1,"index":1,"id":"_managing_pipeline_runs"},{"parentId":"_managing_pipeline_runs","name":"Overview of pipeline runs","level":2,"index":0,"id":"overview-of-pipeline-runs_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Scheduling a pipeline run","level":2,"index":1,"id":"scheduling-a-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Cloning a scheduled pipeline run","level":2,"index":2,"id":"cloning-a-scheduled-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Stopping a triggered pipeline run","level":2,"index":3,"id":"stopping-a-triggered-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Deleting a scheduled pipeline run","level":2,"index":4,"id":"deleting-a-scheduled-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Deleting a triggered pipeline run","level":2,"index":5,"id":"deleting-a-triggered-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Viewing scheduled pipeline runs","level":2,"index":6,"id":"viewing-scheduled-pipeline-runs_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Viewing triggered pipeline runs","level":2,"index":7,"id":"viewing-triggered-pipeline-runs_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Viewing the details of a pipeline run","level":2,"index":8,"id":"viewing-the-details-of-a-pipeline-run_ds-pipelines"},{"parentId":null,"name":"Working with pipelines in JupyterLab","level":1,"index":2,"id":"_working_with_pipelines_in_jupyterlab"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Overview of pipelines in JupyterLab","level":2,"index":0,"id":"overview-of-pipelines-in-jupyterlab_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Accessing the pipeline editor","level":2,"index":1,"id":"accessing-the-pipeline-editor_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Creating a runtime configuration","level":2,"index":2,"id":"creating-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Updating a runtime configuration","level":2,"index":3,"id":"updating-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Deleting a runtime configuration","level":2,"index":4,"id":"deleting-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Duplicating a runtime configuration","level":2,"index":5,"id":"duplicating-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Running a pipeline in JupyterLab","level":2,"index":6,"id":"running-a-pipeline-in-jupyterlab_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Exporting a pipeline in JupyterLab","level":2,"index":7,"id":"exporting-a-pipeline-in-jupyterlab_ds-pipelines"},{"parentId":null,"name":"Additional resources","level":1,"index":3,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-pipeline-editor/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-tutorials/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-custom-model-serving-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-data-connection-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-cluster-storage-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/cloning-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-model-server-for-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-access-to-data-science-projects/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-new-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/defining-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-data-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-triggered-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-workbench-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-cluster-storage-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/disabling-applications-connected-to-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-services-connected-to-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/exporting-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/importing-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-python-packages-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/launching-jupyter-and-starting-a-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/logging-in-to-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/notebook-images-for-data-scientists/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/notifications-in-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/options-for-notebook-server-environments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipelines-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/pushing-project-changes-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-disabled-applications-from-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/scheduling-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/sharing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-a-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-a-triggered-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/supported-browsers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/supported-packages/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/supported-services/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/the-open-data-hub-user-interface/"},"sections":[{"parentId":null,"name":"Global navigation","level":1,"index":0,"id":"_global_navigation"},{"parentId":null,"name":"Side navigation","level":1,"index":1,"id":"_side_navigation"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-in-jupyter-for-administrators/"},"sections":[{"parentId":null,"name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":1,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":null,"name":"A user&#8217;s notebook server does not start","level":1,"index":1,"id":"_a_users_notebook_server_does_not_start"},{"parentId":null,"name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":1,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-in-jupyter-for-users/"},"sections":[{"parentId":null,"name":"I see a <strong>403: Forbidden</strong> error when I log in to Jupyter","level":1,"index":0,"id":"_i_see_a_403_forbidden_error_when_i_log_in_to_jupyter"},{"parentId":null,"name":"My notebook server does not start","level":1,"index":1,"id":"_my_notebook_server_does_not_start"},{"parentId":null,"name":"I see a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when I run my notebook cells","level":1,"index":2,"id":"_i_see_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_i_run_my_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/tutorials-for-data-scientists/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-connected-data-source/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-cluster-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-notebook-server-settings-by-restarting-your-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-the-deployment-properties-of-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-your-project-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-existing-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-python-packages-installed-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-scheduled-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-performance-metrics-of-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-triggered-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-pipeline-editor/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-tutorials/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-custom-model-serving-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-data-connection-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-cluster-storage-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/cloning-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-model-server-for-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-access-to-data-science-projects/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-new-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/defining-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-data-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-triggered-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-workbench-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-cluster-storage-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/disabling-applications-connected-to-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-services-connected-to-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/exporting-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/importing-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-python-packages-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/launching-jupyter-and-starting-a-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/logging-in-to-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/notebook-images-for-data-scientists/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/notifications-in-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/options-for-notebook-server-environments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipelines-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/pushing-project-changes-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-disabled-applications-from-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/scheduling-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/sharing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-a-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-a-triggered-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/supported-browsers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/supported-packages/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/supported-services/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/the-open-data-hub-user-interface/"},"sections":[{"parentId":null,"name":"Global navigation","level":1,"index":0,"id":"_global_navigation"},{"parentId":null,"name":"Side navigation","level":1,"index":1,"id":"_side_navigation"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-in-jupyter-for-administrators/"},"sections":[{"parentId":null,"name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":1,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":null,"name":"A user&#8217;s notebook server does not start","level":1,"index":1,"id":"_a_users_notebook_server_does_not_start"},{"parentId":null,"name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":1,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-in-jupyter-for-users/"},"sections":[{"parentId":null,"name":"I see a <strong>403: Forbidden</strong> error when I log in to Jupyter","level":1,"index":0,"id":"_i_see_a_403_forbidden_error_when_i_log_in_to_jupyter"},{"parentId":null,"name":"My notebook server does not start","level":1,"index":1,"id":"_my_notebook_server_does_not_start"},{"parentId":null,"name":"I see a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when I run my notebook cells","level":1,"index":2,"id":"_i_see_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_i_run_my_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/tutorials-for-data-scientists/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-connected-data-source/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-cluster-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-notebook-server-settings-by-restarting-your-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-the-deployment-properties-of-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-your-project-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-existing-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-python-packages-installed-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-scheduled-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-performance-metrics-of-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-triggered-pipeline-runs/"},"sections":null}}}]},"asciidoc":{"html":"<div id=\"toc\" class=\"toc\">\n<div id=\"toctitle\">Table of Contents</div>\n<ul class=\"sectlevel1\">\n<li><a href=\"#creating-and-importing-notebooks_notebooks\">Creating and importing notebooks</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#creating-a-new-notebook_notebooks\">Creating a new notebook</a></li>\n<li><a href=\"#uploading-an-existing-notebook-file-from-local-storage_notebooks\">Uploading an existing notebook file from local storage</a></li>\n<li><a href=\"#uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_notebooks\">Uploading an existing notebook file from a Git repository using JupyterLab</a></li>\n<li><a href=\"#uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_notebooks\">Uploading an existing notebook file from a Git repository using the command line interface</a></li>\n</ul>\n</li>\n<li><a href=\"#collaborating-on-notebooks-using-git_git-collab\">Collaborating on notebooks using Git</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_git-collab\">Uploading an existing notebook file from a Git repository using JupyterLab</a></li>\n<li><a href=\"#uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_git-collab\">Uploading an existing notebook file from a Git repository using the command line interface</a></li>\n<li><a href=\"#updating-your-project-with-changes-from-a-remote-git-repository_git-collab\">Updating your project with changes from a remote Git repository</a></li>\n<li><a href=\"#pushing-project-changes-to-a-git-repository_git-collab\">Pushing project changes to a Git repository</a></li>\n</ul>\n</li>\n<li><a href=\"#working-on-data-science-projects_nb-server\">Working on data science projects</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#_using_data_science_projects\">Using data science projects</a></li>\n<li><a href=\"#_using_project_workbenches\">Using project workbenches</a></li>\n<li><a href=\"#_using_data_connections\">Using data connections</a></li>\n<li><a href=\"#_configuring_cluster_storage\">Configuring cluster storage</a></li>\n<li><a href=\"#_configuring_model_servers\">Configuring model servers</a></li>\n<li><a href=\"#_configuring_access_to_data_science_projects\">Configuring access to data science projects</a></li>\n<li><a href=\"#viewing-python-packages-installed-on-your-notebook-server_nb-server\">Viewing Python packages installed on your notebook server</a></li>\n<li><a href=\"#installing-python-packages-on-your-notebook-server_nb-server\">Installing Python packages on your notebook server</a></li>\n<li><a href=\"#updating-notebook-server-settings-by-restarting-your-server_nb-server\">Updating notebook server settings by restarting your server</a></li>\n</ul>\n</li>\n<li><a href=\"#working-with-data-science-pipelines_ds-pipelines\">Working with data science pipelines</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#_managing_data_science_pipelines\">Managing data science pipelines</a></li>\n<li><a href=\"#_managing_pipeline_runs\">Managing pipeline runs</a></li>\n<li><a href=\"#_working_with_pipelines_in_jupyterlab\">Working with pipelines in JupyterLab</a></li>\n<li><a href=\"#_additional_resources\">Additional resources</a></li>\n</ul>\n</li>\n<li><a href=\"#model-serving-on-openshift-data-science_model-serving\">Model serving on Open Data Hub</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#_configuring_model_servers_2\">Configuring model servers</a></li>\n<li><a href=\"#_working_with_deployed_models\">Working with deployed models</a></li>\n</ul>\n</li>\n<li><a href=\"#troubleshooting-common-problems-in-jupyter-for-administrators_model-serving\">Troubleshooting common problems in Jupyter for administrators</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter\">A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter</a></li>\n<li><a href=\"#_a_users_notebook_server_does_not_start\">A user&#8217;s notebook server does not start</a></li>\n<li><a href=\"#_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells\">The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells</a></li>\n</ul>\n</li>\n<li><a href=\"#troubleshooting-common-problems-in-jupyter-for-users_model-serving\">Troubleshooting common problems in Jupyter for users</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#_i_see_a_403_forbidden_error_when_i_log_in_to_jupyter\">I see a <strong>403: Forbidden</strong> error when I log in to Jupyter</a></li>\n<li><a href=\"#_my_notebook_server_does_not_start\">My notebook server does not start</a></li>\n<li><a href=\"#_i_see_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_i_run_my_notebook_cells\">I see a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when I run my notebook cells</a></li>\n</ul>\n</li>\n</ul>\n</div>\n<div class=\"sect1\">\n<h2 id=\"creating-and-importing-notebooks_notebooks\">Creating and importing notebooks</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>You can create a blank notebook or import a notebook from a number of different sources.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"creating-a-new-notebook_notebooks\">Creating a new notebook</h3>\n<div class=\"paragraph _abstract\">\n<p>You can create a new Jupyter notebook from an existing notebook container image to access its resources and properties. The <strong>Notebook server control panel</strong> contains a list of available container images that you can run as a single-user notebook server.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>Ensure that you have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>Ensure that you have launched your notebook server and logged in to Jupyter.</p>\n</li>\n<li>\n<p>The notebook image exists in a registry, image stream, and is accessible.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Click <strong>File</strong> &#8594; <strong>New</strong> &#8594; <strong>Notebook</strong>.</p>\n</li>\n<li>\n<p>If prompted, select a kernel for your notebook from the list.</p>\n<div class=\"paragraph\">\n<p>If you want to use a kernel, click <strong>Select</strong>. If you do not want to use a kernel, click <strong>No Kernel</strong>.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Check that the notebook file is visible in the JupyterLab interface.</p>\n</li>\n</ul>\n</div>\n<div class=\"sect3\">\n<h4 id=\"notebook-images-for-data-scientists_notebooks\">Notebook images for data scientists</h4>\n<div class=\"paragraph _abstract\">\n<p>Open Data Hub contains Jupyter notebook images optimized with industry-leading tools and libraries required for your data science work. To provide a consistent, stable platform for your model development, all notebook images contain the same version of Python. Notebook images available on Open Data Hub are pre-built and ready for you to use immediately after Open Data Hub is installed or upgraded.</p>\n</div>\n<div class=\"paragraph\">\n<p>Notebook images are upgraded quarterly to ensure that you are working with the latest supported version.</p>\n</div>\n<div class=\"paragraph\">\n<p>Open Data Hub contains the following notebook images that are installed by default:</p>\n</div>\n<table class=\"tableblock frame-all grid-all stretch\">\n<caption class=\"title\">Table 1. Default notebook images</caption>\n<colgroup>\n<col style=\"width: 16.6666%;\">\n<col style=\"width: 83.3334%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Image name</th>\n<th class=\"tableblock halign-left valign-top\">Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">CUDA</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">If you are working with compute-intensive data science models that require GPU support, use the Compute Unified Device Architecture (CUDA) notebook image to gain access to the NVIDIA CUDA Toolkit. Using this toolkit, you can optimize your work using GPU-accelerated libraries and optimization tools.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Standard Data Science</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Use the Standard Data Science notebook image for models that do not require TensorFlow or PyTorch. This image contains commonly used libraries to assist you in developing your machine learning models.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">TensorFlow</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">TensorFlow is an open source platform for machine learning. With TensorFlow, you can build, train and deploy your machine learning models. TensorFlow contains advanced data visualization features, such as computational graph visualizations. It also allows you to easily monitor and track the progress of your models.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">PyTorch</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">PyTorch is an open source machine learning library optimized for deep learning. If you are working with computer vision or natural language processing models, use the Pytorch notebook image.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Minimal Python</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">If you do not require advanced machine learning features, or additional resources for compute-intensive data science work, you can use the Minimal Python image to develop your models.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">TrustyAI</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Use the TrustyAI notebook image to leverage your data science work with model explainability, tracing and accountability, and runtime monitoring.</p></td>\n</tr>\n</tbody>\n</table>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"uploading-an-existing-notebook-file-from-local-storage_notebooks\">Uploading an existing notebook file from local storage</h3>\n<div class=\"paragraph _abstract\">\n<p>You can load an existing notebook from local storage into JupyterLab to continue work, or adapt a project for a new use case.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>Credentials for logging in to Jupyter.</p>\n</li>\n<li>\n<p>A launched and running notebook server.</p>\n</li>\n<li>\n<p>A notebook file exists in your local storage.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the <strong>File Browser</strong> in the left sidebar of the JupyterLab interface, click <strong>Upload Files</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyter-upload-file.png\" alt=\"Upload Files\"></span>).</p>\n</li>\n<li>\n<p>Locate and select the notebook file and click <strong>Open</strong>.</p>\n<div class=\"paragraph\">\n<p>The file is displayed in the <strong>File Browser</strong>.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The notebook file displays in the <strong>File Browser</strong> in the left sidebar of the JupyterLab interface.</p>\n</li>\n<li>\n<p>You can open the notebook file in JupyterLab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_notebooks\">Uploading an existing notebook file from a Git repository using JupyterLab</h3>\n<div class=\"paragraph _abstract\">\n<p>You can use the JupyterLab user interface to clone a Git repository into your workspace to continue your work or integrate files from an external project.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>A launched and running Jupyter server.</p>\n</li>\n<li>\n<p>Read access for the Git repository you want to clone.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Copy the HTTPS URL for the Git repository.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>On GitHub, click <strong>&#10515; Code</strong> &#8594; <strong>HTTPS</strong> and click the Clipboard button.</p>\n</li>\n<li>\n<p>On GitLab, click <strong>Clone</strong> and click the Clipboard button under <strong>Clone with HTTPS</strong>.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>In the JupyterLab interface, click the <strong>Git Clone</strong> button (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-git-clone-button.png\" alt=\"Git Clone button\"></span>).</p>\n<div class=\"paragraph\">\n<p>You can also click <strong>Git</strong> &#8594; <strong>Clone a repository</strong> in the menu, or click the Git icon (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-git-button.png\" alt=\"Git button\"></span>) and click the <strong>Clone a repository</strong> button.</p>\n</div>\n<div class=\"paragraph\">\n<p>The <em>Clone a repo</em> dialog appears.</p>\n</div>\n</li>\n<li>\n<p>Enter the HTTPS URL of the repository that contains your notebook.</p>\n</li>\n<li>\n<p>Click <strong>CLONE</strong>.</p>\n</li>\n<li>\n<p>If prompted, enter your username and password for the Git repository.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Check that the contents of the repository are visible in the file browser in JupyterLab, or run the <strong>ls</strong> command in the terminal to verify that the repository is shown as a directory.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_notebooks\">Uploading an existing notebook file from a Git repository using the command line interface</h3>\n<div class=\"paragraph _abstract\">\n<p>You can use the command line interface to clone a Git repository into your workspace to continue your work or integrate files from an external project.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>A launched and running Jupyter server.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Copy the HTTPS URL for the Git repository.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>On GitHub, click <strong>&#10515; Code</strong> &#8594; <strong>HTTPS</strong> and click the Clipboard button.</p>\n</li>\n<li>\n<p>On GitLab, click <strong>Clone</strong> and click the Clipboard button under <strong>Clone with HTTPS</strong>.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>In JupyterLab, click <strong>File</strong> &#8594; <strong>New</strong> &#8594; <strong>Terminal</strong> to open a terminal window.</p>\n</li>\n<li>\n<p>Enter the <code>git clone</code> command.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>git clone <em>&lt;git-clone-URL&gt;</em></code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Replace <em>`&lt;git-clone-URL&gt;`</em> with the HTTPS URL, for example:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>[1234567890@jupyter-nb-jdoe ~]$ <strong>git clone https://github.com/example/myrepo.git</strong>\nCloning into <em>myrepo</em>...\nremote: Enumerating objects: 11, done.\nremote: Counting objects: 100% (11/11), done.\nremote: Compressing objects: 100% (10/10), done.\nremote: Total 2821 (delta 1), reused 5 (delta 1), pack-reused 2810\nReceiving objects: 100% (2821/2821), 39.17 MiB | 23.89 MiB/s, done.\nResolving deltas: 100% (1416/1416), done.</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Check that the contents of the repository are visible in the file browser in JupyterLab, or run the <strong>ls</strong> command in the terminal to verify that the repository is shown as a directory.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"collaborating-on-notebooks-using-git_git-collab\">Collaborating on notebooks using Git</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>If your notebooks or other files are stored in Git version control, you can import them from a Git repository onto your notebook server to work with them in JupyterLab. When you are ready, you can push your changes back to the Git repository so that others can review or use your models.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_git-collab\">Uploading an existing notebook file from a Git repository using JupyterLab</h3>\n<div class=\"paragraph _abstract\">\n<p>You can use the JupyterLab user interface to clone a Git repository into your workspace to continue your work or integrate files from an external project.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>A launched and running Jupyter server.</p>\n</li>\n<li>\n<p>Read access for the Git repository you want to clone.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Copy the HTTPS URL for the Git repository.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>On GitHub, click <strong>&#10515; Code</strong> &#8594; <strong>HTTPS</strong> and click the Clipboard button.</p>\n</li>\n<li>\n<p>On GitLab, click <strong>Clone</strong> and click the Clipboard button under <strong>Clone with HTTPS</strong>.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>In the JupyterLab interface, click the <strong>Git Clone</strong> button (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-git-clone-button.png\" alt=\"Git Clone button\"></span>).</p>\n<div class=\"paragraph\">\n<p>You can also click <strong>Git</strong> &#8594; <strong>Clone a repository</strong> in the menu, or click the Git icon (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-git-button.png\" alt=\"Git button\"></span>) and click the <strong>Clone a repository</strong> button.</p>\n</div>\n<div class=\"paragraph\">\n<p>The <em>Clone a repo</em> dialog appears.</p>\n</div>\n</li>\n<li>\n<p>Enter the HTTPS URL of the repository that contains your notebook.</p>\n</li>\n<li>\n<p>Click <strong>CLONE</strong>.</p>\n</li>\n<li>\n<p>If prompted, enter your username and password for the Git repository.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Check that the contents of the repository are visible in the file browser in JupyterLab, or run the <strong>ls</strong> command in the terminal to verify that the repository is shown as a directory.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_git-collab\">Uploading an existing notebook file from a Git repository using the command line interface</h3>\n<div class=\"paragraph _abstract\">\n<p>You can use the command line interface to clone a Git repository into your workspace to continue your work or integrate files from an external project.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>A launched and running Jupyter server.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Copy the HTTPS URL for the Git repository.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>On GitHub, click <strong>&#10515; Code</strong> &#8594; <strong>HTTPS</strong> and click the Clipboard button.</p>\n</li>\n<li>\n<p>On GitLab, click <strong>Clone</strong> and click the Clipboard button under <strong>Clone with HTTPS</strong>.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>In JupyterLab, click <strong>File</strong> &#8594; <strong>New</strong> &#8594; <strong>Terminal</strong> to open a terminal window.</p>\n</li>\n<li>\n<p>Enter the <code>git clone</code> command.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>git clone <em>&lt;git-clone-URL&gt;</em></code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Replace <em>`&lt;git-clone-URL&gt;`</em> with the HTTPS URL, for example:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>[1234567890@jupyter-nb-jdoe ~]$ <strong>git clone https://github.com/example/myrepo.git</strong>\nCloning into <em>myrepo</em>...\nremote: Enumerating objects: 11, done.\nremote: Counting objects: 100% (11/11), done.\nremote: Compressing objects: 100% (10/10), done.\nremote: Total 2821 (delta 1), reused 5 (delta 1), pack-reused 2810\nReceiving objects: 100% (2821/2821), 39.17 MiB | 23.89 MiB/s, done.\nResolving deltas: 100% (1416/1416), done.</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Check that the contents of the repository are visible in the file browser in JupyterLab, or run the <strong>ls</strong> command in the terminal to verify that the repository is shown as a directory.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"updating-your-project-with-changes-from-a-remote-git-repository_git-collab\">Updating your project with changes from a remote Git repository</h3>\n<div class=\"paragraph _abstract\">\n<p>You can pull changes made by other users into your data science project from a remote Git repository.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have configured the remote Git repository.</p>\n</li>\n<li>\n<p>You have already imported the Git repository into JupyterLab, and the contents of the repository are visible in the file browser in JupyterLab.</p>\n</li>\n<li>\n<p>You have permissions to pull files from the remote Git repository to your local repository.</p>\n</li>\n<li>\n<p>You have credentials for logging in to Jupyter.</p>\n</li>\n<li>\n<p>You have a launched and running Jupyter server.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the JupyterLab interface, click the<strong>Git</strong> button (<span class=\"image\"><img src=\"/static/docs/images/jupyter-git-sidebar.png\" alt=\"Git button\"></span>).</p>\n</li>\n<li>\n<p>Click the <strong>Pull latest changes</strong> button (<span class=\"image\"><img src=\"/static/docs/images/jupyter-git-pull-button.png\" alt=\"Pull latest changes button\"></span>).</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>You can view the changes pulled from the remote repository in the <strong>History</strong> tab of the Git pane.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"pushing-project-changes-to-a-git-repository_git-collab\">Pushing project changes to a Git repository</h3>\n<div class=\"paragraph _abstract\">\n<p>To build and deploy your application in a production environment, upload your work to a remote Git repository.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have opened a notebook in the JupyterLab interface.</p>\n</li>\n<li>\n<p>You have already added the relevant Git repository to your notebook server.</p>\n</li>\n<li>\n<p>You have permission to push changes to the relevant Git repository.</p>\n</li>\n<li>\n<p>You have installed the Git version control extension.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Click <strong>File</strong> &#8594; <strong>Save All</strong> to save any unsaved changes.</p>\n</li>\n<li>\n<p>Click the Git icon (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-git-button.png\" alt=\"Git button\"></span>) to open the Git pane in the JupyterLab interface.</p>\n</li>\n<li>\n<p>Confirm that your changed files appear under <strong>Changed</strong>.</p>\n<div class=\"paragraph\">\n<p>If your changed files appear under <strong>Untracked</strong>, click <strong>Git</strong> &#8594; <strong>Simple Staging</strong> to enable a simplified Git process.</p>\n</div>\n</li>\n<li>\n<p>Commit your changes.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Ensure that all files under <strong>Changed</strong> have a blue checkmark beside them.</p>\n</li>\n<li>\n<p>In the <strong>Summary</strong> field, enter a brief description of the changes you made.</p>\n</li>\n<li>\n<p>Click <strong>Commit</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Git</strong> &#8594; <strong>Push to Remote</strong> to push your changes to the remote repository.</p>\n</li>\n<li>\n<p>When prompted, enter your Git credentials and click <strong>OK</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Your most recently pushed changes are visible in the remote Git repository.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"working-on-data-science-projects_nb-server\">Working on data science projects</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>As a data scientist, you can organize your data science work into a single project. A data science project in Open Data Hub can consist of the following components:</p>\n</div>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">Workbenches</dt>\n<dd>\n<p>Creating a workbench allows you to add a Jupyter notebook to your project.</p>\n</dd>\n<dt class=\"hdlist1\">Cluster storage</dt>\n<dd>\n<p>For data science projects that require data to be retained, you can add cluster storage to the project.</p>\n</dd>\n<dt class=\"hdlist1\">Data connections</dt>\n<dd>\n<p>Adding a data connection to your project allows you to connect data inputs to your workbenches.</p>\n</dd>\n<dt class=\"hdlist1\">Models and model servers</dt>\n<dd>\n<p>Deploy a trained data science model to serve intelligent applications. Your model is deployed with an endpoint that allows applications to send requests to the model.</p>\n</dd>\n</dl>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_using_data_science_projects\">Using data science projects</h3>\n<div class=\"sect3\">\n<h4 id=\"creating-a-data-science-project_nb-server\">Creating a data science project</h4>\n<div class=\"paragraph _abstract\">\n<p>To start your data science work, create a data science project. Creating a project helps you organize your work in one place. You can also enhance the capabilities of your data science project by adding workbenches, adding storage to your project&#8217;s cluster, adding data connections, and adding model servers.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click <strong>Create data science project</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Create a data science project</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter a <strong>name</strong> for your data science project.</p>\n</li>\n<li>\n<p>Optional: Edit the <strong>resource name</strong> for your data science project. The resource name must consist of lowercase alphanumeric characters, <em>-</em>, and must start and end with an alphanumeric character.</p>\n</li>\n<li>\n<p>Enter a <strong>description</strong> for your data science project.</p>\n</li>\n<li>\n<p>Click <strong>Create</strong>.</p>\n<div class=\"paragraph\">\n<p>A project details page opens. From here, you can create workbenches, add cluster storage, and add data connections to your project.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The data science project that you created is displayed on the <strong>Data science projects</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"updating-a-data-science-project_nb-server\">Updating a data science project</h4>\n<div class=\"paragraph _abstract\">\n<p>You can update your data science project&#8217;s details by changing your project&#8217;s name and description text.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the project whose details you want to update and click <strong>Edit project</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Edit data science project</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Optional: Update the <strong>name</strong> for your data science project.</p>\n</li>\n<li>\n<p>Optional: Update the <strong>description</strong> for your data science project.</p>\n</li>\n<li>\n<p>Click <strong>Update</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The data science project that you updated is displayed on the <strong>Data science projects</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deleting-a-data-science-project_nb-server\">Deleting a data science project</h4>\n<div class=\"paragraph _abstract\">\n<p>You can delete data science projects so that they do not appear on the Open Data Hub <strong>Data science projects</strong> page when you no longer want to use them.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the project that you want to delete and click <strong>Delete project</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Delete project</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter the project name in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete project</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The data science project that you deleted is no longer displayed on the <strong>Data science projects</strong> page.</p>\n</li>\n<li>\n<p>Deleting a data science project deletes any associated workbenches, cluster storage, and data connections. This data is permanently deleted and is not recoverable.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_using_project_workbenches\">Using project workbenches</h3>\n<div class=\"sect3\">\n<h4 id=\"creating-a-project-workbench_nb-server\">Creating a project workbench</h4>\n<div class=\"paragraph _abstract\">\n<p>To examine and work with data models in an isolated area, you can create a workbench. This workbench enables you to create a new Jupyter notebook from an existing notebook container image to access its resources and properties. For data science projects that require data to be retained, you can add container storage to the workbench you are creating.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that you can add a workbench to.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to add the workbench to.</p>\n<div class=\"paragraph\">\n<p>The <strong>Details</strong> page for the project opens.</p>\n</div>\n</li>\n<li>\n<p>Click <strong>Create workbench</strong> in the <strong>Workbenches</strong> section.</p>\n<div class=\"paragraph\">\n<p>The <strong>Create workbench</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Configure the properties of the workbench you are creating.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Enter a <strong>name</strong> for your workbench.</p>\n</li>\n<li>\n<p>Enter a <strong>description</strong> for your workbench.</p>\n</li>\n<li>\n<p>Select the <strong>notebook image</strong> to use for your workbench server.</p>\n</li>\n<li>\n<p>Select the <strong>container size</strong> for your server.</p>\n</li>\n<li>\n<p>Optional: Select and specify values for any new <strong>environment variables</strong>.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>To enable data science pipelines in JupyterLab, create the following environment variable:\n<code>PIPELINES_SSL_SA_CERTS=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt</code></p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Configure the storage for your Open Data Hub cluster.</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>Select <strong>Create new persistent storage</strong> to create storage that is retained after you log out of Open Data Hub. Fill in the relevant fields to define the storage.</p>\n</li>\n<li>\n<p>Select <strong>Use existing persistent storage</strong> to reuse existing storage then select the storage from the <strong>Persistent storage</strong> list.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Create workbench</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The workbench that you created appears on the <strong>Details</strong> page for the project.</p>\n</li>\n<li>\n<p>Any cluster storage that you associated with the workbench during the creation process appears on the <strong>Details</strong> page for the project.</p>\n</li>\n<li>\n<p>The <strong>Status</strong> column, located in the <strong>Workbenches</strong> section of the <strong>Details</strong> page, displays a status of <strong>Starting</strong> when the workbench server is starting, and <strong>Running</strong> when the workbench has successfully started.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"starting-a-workbench_nb-server\">Starting a workbench</h4>\n<div class=\"paragraph _abstract\">\n<p>You can manually start a data science project&#8217;s workbench from the <strong>Details</strong> page for the project. By default, workbenches start immediately after you create them.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project whose workbench you want to start.</p>\n<div class=\"paragraph\">\n<p>The <strong>Details</strong> page for the project opens.</p>\n</div>\n</li>\n<li>\n<p>Click the toggle in the <strong>Status</strong> column for the relevant workbench to start a workbench that is not running.</p>\n<div class=\"paragraph\">\n<p>The status of the workbench that you started changes from <strong>Stopped</strong> to <strong>Running</strong>. After the workbench has started, click <strong>Open</strong> to open the workbench&#8217;s notebook.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The workbench that you started appears on the <strong>Details</strong> page for the project with the status of <strong>Running</strong>.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"updating-a-project-workbench_nb-server\">Updating a project workbench</h4>\n<div class=\"paragraph _abstract\">\n<p>If your data science work requires you to change your workbench&#8217;s notebook image, container size, or identifying information, you can modify the properties of your project&#8217;s workbench.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project whose workbench you want to update.</p>\n<div class=\"paragraph\">\n<p>The <strong>Details</strong> page for the project opens.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the workbench that you want to update in the <strong>Workbenches</strong> section and click <strong>Edit workbench</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Edit workbench</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Update the workbench&#8217;s properties.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Update the <strong>name</strong> for your workbench, if applicable.</p>\n</li>\n<li>\n<p>Update <strong>description</strong> for your workbench, if applicable.</p>\n</li>\n<li>\n<p>Select a new <strong>notebook image</strong> to use for your workbench server, if applicable.</p>\n</li>\n<li>\n<p>Select a new <strong>container size</strong> for your server, if applicable.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Update workbench</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The workbench that you updated appears on the <strong>Details</strong> page for the project.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deleting-a-workbench-from-a-data-science-project_nb-server\">Deleting a workbench from a data science project</h4>\n<div class=\"paragraph _abstract\">\n<p>You can delete workbenches from your data science projects to help you remove Jupyter notebooks that are no longer relevant to your work.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project with a workbench.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to delete the workbench from.</p>\n<div class=\"paragraph\">\n<p>The <strong>Details</strong> page for the project opens.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the workbench that you want to delete in the <strong>Workbenches</strong> section and click <strong>Delete workbench</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Delete workbench</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter the name of the workbench in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete workbench</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The workbench that you deleted is no longer displayed in the <strong>Workbenches</strong> section on the project <strong>Details</strong> page.</p>\n</li>\n<li>\n<p>The custom resource (CR) associated with the workbench&#8217;s Jupyter notebook is deleted.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_using_data_connections\">Using data connections</h3>\n<div class=\"sect3\">\n<h4 id=\"adding-a-data-connection-to-your-data-science-project_nb-server\">Adding a data connection to your data science project</h4>\n<div class=\"paragraph _abstract\">\n<p>You can enhance your data science project by adding a connection to a data source. When you want to work with a very large data sets, you can store your data in an Amazon Web Services (AWS) Simple Storage Service (S3) bucket so that you do not fill up your local storage. You also have the option of associating the data connection with an existing workbench that does not already have a connection.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that you can add a data connection to.</p>\n</li>\n<li>\n<p>If you intend to add the data connection to an existing workbench, you have saved any data in the workbench to avoid losing work.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to add a data connection to.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>In the <strong>Data connections</strong> section of the page, click <strong>Add data connection</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Add data connection</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter a <strong>name</strong> for the data connection.</p>\n</li>\n<li>\n<p>Enter your access key ID for Amazon Web Services in the <strong>AWS_ACCESS_KEY_ID</strong> field.</p>\n</li>\n<li>\n<p>Enter your secret access key for the account you specified in the <strong>AWS_SECRET_ACCESS_KEY_ID</strong> field.</p>\n</li>\n<li>\n<p>Enter the endpoint of your AWS S3 storage in the <strong>AWS_S3_ENDPOINT</strong> field.</p>\n</li>\n<li>\n<p>Enter the default region of your AWS account in the <strong>AWS_DEFAULT_REGION</strong> field.</p>\n</li>\n<li>\n<p>Enter the name of the AWS S3 bucket in the <strong>AWS_S3_BUCKET</strong> field.</p>\n</li>\n<li>\n<p>Click <strong>Add data connection</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The data connection that you added appears in the <strong>Data connections</strong> section on the <strong>Details</strong> page for the project.</p>\n</li>\n<li>\n<p>If you selected a workbench, the data connection is visible in the <strong>Workbenches</strong> section on your data science project page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deleting-a-data-connection_nb-server\">Deleting a data connection</h4>\n<div class=\"paragraph _abstract\">\n<p>You can delete data connections from your data science projects to help you remove connections that are no longer relevant to your work.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project with a data connection.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to delete the data connection from.</p>\n<div class=\"paragraph\">\n<p>The <strong>Details</strong> page for the project opens.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the data connection that you want to delete in the <strong>Data connections</strong> section and click <strong>Delete data connection</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Delete data connection</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter the name of the data connection in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete data connection</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The data connection that you deleted is no longer displayed in the <strong>Data connections</strong> section on the project <strong>Details</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"updating-a-connected-data-source_nb-server\">Updating a connected data source</h4>\n<div class=\"paragraph _abstract\">\n<p>To use an existing data source with a different workbench, you can change the data source that is connected to your project&#8217;s workbench.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project, created a workbench, and you have defined a data connection.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project whose data source you want to change.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the data source that you want to change in the <strong>Data connections</strong> section and click <strong>Change connected workbenches</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Update connected workbenches</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Select an existing <strong>workbench</strong> to connect the data source to from the list.</p>\n</li>\n<li>\n<p>Click <strong>Update connected workbenches</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The data connection that you changed is displayed in the <strong>Data connections</strong> section on the project <strong>Details</strong> page.</p>\n</li>\n<li>\n<p>You can access your S3 data source using environment variables in the connected workbench.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_configuring_cluster_storage\">Configuring cluster storage</h3>\n<div class=\"sect3\">\n<h4 id=\"adding-cluster-storage-to-your-data-science-project_nb-server\">Adding cluster storage to your data science project</h4>\n<div class=\"paragraph _abstract\">\n<p>For data science projects that require data to be retained, you can add cluster storage to the project. Additionally, you can also connect cluster storage to a specific project&#8217;s workbench.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that you can add cluster storage to.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to add the cluster storage to.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>In the <strong>Cluster storage</strong> section of the page, click <strong>Add cluster storage</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Add storage</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter a <strong>name</strong> for the cluster storage.</p>\n</li>\n<li>\n<p>Enter a <strong>description</strong> for the cluster storage.</p>\n</li>\n<li>\n<p>Under <strong>Persistent storage size</strong>, enter a new size in gibibytes. The minimum size is 1 GiB, and the maximum size is 16384 GiB.</p>\n</li>\n<li>\n<p>Optional: Select a <strong>workbench</strong> from the list to connect the cluster storage to an existing workbench.</p>\n</li>\n<li>\n<p>If you selected a workbench to connect the storage to, enter the storage directory in the <strong>Mount folder</strong> field.</p>\n</li>\n<li>\n<p>Click <strong>Add storage</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The cluster storage that you added appears in the <strong>Cluster storage</strong> section on the <strong>Details</strong> page for the project.</p>\n</li>\n<li>\n<p>A new persistent volume claim (PVC) is created with the storage size that you defined.</p>\n</li>\n<li>\n<p>The persistent volume claim (PVC) is visible as an attached storage in the <strong>Workbenches</strong> section on the <strong>Details</strong> page for the project.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"updating-cluster-storage_nb-server\">Updating cluster storage</h4>\n<div class=\"paragraph _abstract\">\n<p>If your data science work requires you to change the identifying information of a project&#8217;s cluster storage or the workbench that the storage is connected to, you can update your project&#8217;s cluster storage to change these properties.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>You cannot change size of a persistent volume claim (PVC) that you have previously defined as cluster storage.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that contains cluster storage.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project whose storage you want to update.</p>\n<div class=\"paragraph\">\n<p>The <strong>Details</strong> page for the project opens.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the storage that you want to update in the <strong>Cluster storage</strong> section and click <strong>Edit storage</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Edit storage</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Update the storage&#8217;s properties.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Update the <strong>name</strong> for the storage, if applicable.</p>\n</li>\n<li>\n<p>Update <strong>description</strong> for the storage, if applicable.</p>\n</li>\n<li>\n<p>Update the <strong>workbench</strong> that the storage is connected to, if applicable.</p>\n</li>\n<li>\n<p>If you selected a new workbench to connect the storage to, enter the storage directory in the <strong>Mount folder</strong> field.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Update storage</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The storage that you updated appears in the <strong>Cluster storage</strong> section on the <strong>Details</strong> page for the project.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deleting-cluster-storage-from-a-data-science-project_nb-server\">Deleting cluster storage from a data science project</h4>\n<div class=\"paragraph _abstract\">\n<p>You can delete cluster storage from your data science projects to help you free up resources and delete unwanted storage space.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project with cluster storage.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to delete the storage from.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>In the <strong>Cluster storage</strong> section, click the action menu (<strong>&#8942;</strong>) beside the storage that you want to delete and then click <strong>Delete storage</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Delete storage</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter the name of the storage in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete storage</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The storage that you deleted is no longer displayed in the <strong>Cluster storage</strong> section on the project <strong>Details</strong> page.</p>\n</li>\n<li>\n<p>The persistent volume (PV) and persistent volume claim (PVC) associated with the cluster storage are both permanently deleted. This data is not recoverable.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_configuring_model_servers\">Configuring model servers</h3>\n<div class=\"sect3\">\n<h4 id=\"configuring-a-model-server-for-your-data-science-project_nb-server\">Configuring a model server for your data science project</h4>\n<div class=\"paragraph _abstract\">\n<p>Before you can successfully deploy a data science model on Open Data Hub, you must configure a model server. This includes configuring the number of replicas being deployed, the server size, the token authorization, and how the project is accessed.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that you can add a model server to.</p>\n</li>\n<li>\n<p>If you want to use a custom model-serving runtime for your model server, you have added and enabled the runtime. See <a href=\"https://opendatahub.io/docs/working-on-data-science-projects/#adding-a-custom-model-serving-runtime_nb-server\">Adding a custom model-serving runtime</a>.</p>\n</li>\n<li>\n<p>If you want to use graphics processing units (GPUs) with your model server, you have enabled GPU support. This includes installing the Node Feature Discovery and GPU Operators. For more information, see <a href=\"https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/openshift/contents.html\">NVIDIA GPU Operator on OpenShift</a> in the NVIDIA documentation.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to configure a model server for.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>In the <strong>Models and model servers</strong> section, click <strong>Add server</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Add model server</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>In the <strong>Model sever name</strong> field, enter a unique name for the model server.</p>\n</li>\n<li>\n<p>From the <strong>Serving runtime</strong> list, select a model-serving runtime that is installed and enabled in your Open Data Hub deployment.</p>\n</li>\n<li>\n<p>In the <strong>Number of model replicas to deploy</strong> field, specify a value.</p>\n</li>\n<li>\n<p>From the <strong>Model server size</strong> list, select one of the following server sizes:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Small</p>\n</li>\n<li>\n<p>Medium</p>\n</li>\n<li>\n<p>Large</p>\n</li>\n<li>\n<p>Custom</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Optional: If you selected <strong>Custom</strong> in the preceding step, configure the following settings in the <strong>Model server size</strong> section to customize your model server:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>CPUs requested</strong> field, specify a number of CPUs to use with your model server. Use the list beside this field to specify the value in cores or millicores.</p>\n</li>\n<li>\n<p>In the <strong>CPU limit</strong> field, specify the maximum number of CPUs to use with your model server. Use the list beside this field to specify the value in cores or millicores.</p>\n</li>\n<li>\n<p>In the <strong>Memory requested</strong> field, specify the requested memory for the model server in gibibytes (Gi).</p>\n</li>\n<li>\n<p>In the <strong>Memory limit</strong> field, specify the maximum memory limit for the model server in gibibytes (Gi).</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Optional: In the <strong>Model server GPUs</strong> field, specify a number of GPUs to use with your model server.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Open Data Hub includes two versions of the OpenVINO Model Server (OVMS) runtime by default; a version that supports GPUs and one that does not. To use GPUs, from the <strong>Serving runtime</strong> list, you must select the version whose display name includes <code>Supports GPUs</code>.</p>\n</div>\n<div class=\"paragraph\">\n<p>If you are using a <em>custom</em> model-serving runtime with your model server, you must ensure that your custom runtime supports GPUs and is appropriately configured to use them.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Optional: In the <strong>Model route</strong> section, select the <strong>Make deployed models available through an external route</strong> check box to make your deployed models available to external clients.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Token authorization</strong> section, select the <strong>Require token authentication</strong> check box to require token authentication for your model server. To finish configuring token authentication, perform the following actions:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Service account name</strong> field, enter a service account name for which the token will be generated. The generated token is created and displayed in the <strong>Token secret</strong> field when the model server is configured.</p>\n</li>\n<li>\n<p>To add an additional service account, click <strong>Add a service account</strong> and enter another service account name.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Add</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The model server that you configured is displayed in the <strong>Models and model servers</strong> section of the project details page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"adding-a-custom-model-serving-runtime_nb-server\">Adding a custom model-serving runtime</h4>\n<div class=\"paragraph\">\n<p>A model-serving runtime provides integration with a specified model server and the model frameworks that it supports. By default, Open Data Hub includes the OpenVINO Model Server runtime. However, if this runtime doesn&#8217;t meet your needs (it doesn&#8217;t support a particular model framework, for example), you might want to add your own, custom runtimes.</p>\n</div>\n<div class=\"paragraph\">\n<p>As an administrator, you can use the Open Data Hub interface to add and enable custom model-serving runtimes. You can then choose from your enabled runtimes when you create a new model server.</p>\n</div>\n<div class=\"ulist _abstract\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub as an administrator.</p>\n</li>\n<li>\n<p>You are familiar with how to <a href=\"https://opendatahub.io/docs/working-on-data-science-projects/#configuring-a-model-server-for-your-data-science-project_nb-server\">configure a model server for your project</a>. When you have added a custom model-serving runtime, you must configure a new model server to use the runtime.</p>\n</li>\n<li>\n<p>You have reviewed the example runtimes in the <a href=\"https://github.com/kserve/modelmesh-serving/tree/main/config/runtimes\" target=\"_blank\" rel=\"noopener\">kserve/modelmesh-serving</a> repository. You can use these examples as <em>starting points</em>. However, each runtime requires some further modification before you can deploy it in Open Data Hub. The required modifications are described in the following procedure.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nOpen Data Hub includes the OpenVINO Model Server model-serving runtime by default. You do not need to add this runtime to Open Data Hub.\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Settings</strong> &gt; <strong>Serving runtimes</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Serving runtimes</strong> page opens and shows the model-serving runtimes that are already installed and enabled in your Open Data Hub deployment. By default, the OpenVINO Model Server runtime is pre-installed and enabled in Open Data Hub.</p>\n</div>\n</li>\n<li>\n<p>To add a new, custom runtime, click <strong>Add serving runtime</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Add serving runtime</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>To start adding a new runtime, perform one of the following sets of actions:</p>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>To upload a YAML file</strong></p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click <strong>Upload files</strong>.</p>\n<div class=\"paragraph\">\n<p>A file browser opens.</p>\n</div>\n</li>\n<li>\n<p>In the file browser, select a YAML file on your computer. This file might be the one of the example runtimes that you downloaded from the <a href=\"https://github.com/kserve/modelmesh-serving/tree/main/config/runtimes\" target=\"_blank\" rel=\"noopener\">kserve/modelmesh-serving</a> repository.</p>\n<div class=\"paragraph\">\n<p>The embedded YAML editor opens and shows the contents of the file that you uploaded.</p>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p><strong>To enter YAML code directly in the editor</strong></p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click <strong>Start from scratch</strong>.</p>\n<div class=\"paragraph\">\n<p>The embedded YAML editor opens with no content.</p>\n</div>\n</li>\n<li>\n<p>Enter or paste YAML code directly in the embedded editor. The YAML that you paste might be copied from one of the example runtimes in the <a href=\"https://github.com/kserve/modelmesh-serving/tree/main/config/runtimes\" target=\"_blank\" rel=\"noopener\">kserve/modelmesh-serving</a> repository.</p>\n</li>\n</ol>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</li>\n<li>\n<p>Optional: If you are adding one of the example runtimes in the <a href=\"https://github.com/kserve/modelmesh-serving/tree/main/config/runtimes\" target=\"_blank\" rel=\"noopener\">kserve/modelmesh-serving</a> repository, perform the following modifications:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the YAML editor, locate the <code>kind</code> field for your runtime. Update the value of this field to <code>ServingRuntime</code>.</p>\n</li>\n<li>\n<p>In the YAML editor, locate the <code>containers.image</code> field for your runtime. Based on the runtime that you are adding, update the field to one of the following:</p>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">Nvidia Triton Inference Server</dt>\n<dd>\n<div class=\"paragraph\">\n<p><code>image: nvcr.io/nvidia/tritonserver:21.06.1-py3</code></p>\n</div>\n</dd>\n<dt class=\"hdlist1\">Seldon Python MLServer</dt>\n<dd>\n<div class=\"paragraph\">\n<p><code>image: seldonio/mlserver:0.5.2</code></p>\n</div>\n</dd>\n<dt class=\"hdlist1\">TorchServe</dt>\n<dd>\n<div class=\"paragraph\">\n<p><code>image: pytorch/torchserve:0.6.0-cpu</code></p>\n</div>\n</dd>\n</dl>\n</div>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>In the <code>metadata.name</code> field, ensure that the value of the runtime you are adding is unique (that is, the value isn&#8217;t the same as for a runtime you have already added).</p>\n</li>\n<li>\n<p>Optional: To configure a custom display name for the runtime that you are adding, add a <code>metadata.annotations.openshift.io/display-name</code> field and specify a value, as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: serving.kserve.io/v1alpha1\nkind: ServingRuntime\nmetadata:\n  name: mlserver-0.x\n  annotations:\n    openshift.io/display-name: MLServer</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nIf you do not configure a custom display name for your runtime, Open Data Hub shows the value of the <code>metadata.name</code> field.\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Click <strong>Add</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Serving runtimes</strong> page opens and shows the updated list of runtimes that are installed. Observe that the runtime you added is automatically enabled.</p>\n</div>\n</li>\n<li>\n<p>Optional: To edit your custom runtime, click the action menu (&#8942;) and select <strong>Edit</strong>.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nYou cannot directly edit the OpenVINO Model Server runtime that is included in Open Data Hub by default. However, you can <em>clone</em> this runtime and edit the cloned version. You can then add the edited clone as a new, custom runtime. To do this, click the action menu beside the OpenVINO Model Server and select <strong>Clone</strong>.\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The model-serving runtime you added is shown in an enabled state on the <strong>Serving runtimes</strong> page.</p>\n</li>\n</ul>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p>To learn how to configure a model server that uses a custom model-serving runtime that you have added, see <a href=\"https://opendatahub.io/docs/working-on-data-science-projects/#configuring-a-model-server-for-your-data-science-project_nb-server\">Configuring a model server for your data science project</a>.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"updating-a-model-server_nb-server\">Updating a model server</h4>\n<div class=\"paragraph _abstract\">\n<p>You can update your data science project&#8217;s model server by changing details, such as the number of deployed replicas, the server size, the token authorization, and how the project is accessed.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that has a model server assigned.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project whose model server details you want to update.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>In the <strong>Models and model servers</strong> section, locate the model server you want to update. Click the action menu (<strong>&#8942;</strong>)   and select <strong>Edit model server</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Configure model server</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Update the model server properties, as follows:</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nYou cannot change the <strong>Serving runtime</strong> selection for a model server that is already configured. This protects against changing to a runtime that does not support already-deployed models.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Model sever name</strong> field, enter a new, unique name for the model server.</p>\n</li>\n<li>\n<p>In the <strong>Number of model replicas to deploy</strong> field, specify a value.</p>\n</li>\n<li>\n<p>From the <strong>Model server size</strong> list, select one of the following server sizes:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Small</p>\n</li>\n<li>\n<p>Medium</p>\n</li>\n<li>\n<p>Large</p>\n</li>\n<li>\n<p>Custom</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Optional: If you selected <strong>Custom</strong> in the preceding step, configure the following settings in the <strong>Model server size</strong> section to customize your model server:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>CPUs requested</strong> field, specify a number of CPUs to use with your model server. Use the list beside this field to specify the value in cores or millicores.</p>\n</li>\n<li>\n<p>In the <strong>CPU limit</strong> field, specify the maximum number of CPUs to use with your model server. Use the list beside this field to specify the value in cores or millicores.</p>\n</li>\n<li>\n<p>In the <strong>Memory requested</strong> field, specify the requested memory for the model server in gibibytes (Gi).</p>\n</li>\n<li>\n<p>In the <strong>Memory limit</strong> field, specify the maximum memory limit for the model server in gibibytes (Gi).</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Optional: In the <strong>Model server GPUs</strong> field, specify a number of GPUs to use with your model server.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Open Data Hub includes two versions of the OpenVINO Model Server (OVMS) runtime by default; a version that supports GPUs and one that does not. To use GPUs, from the <strong>Serving runtime</strong> list, you must select the version whose display name includes <code>Supports GPUs</code>.</p>\n</div>\n<div class=\"paragraph\">\n<p>If you are using a <em>custom</em> model-serving runtime with your model server, you must ensure that your custom runtime supports GPUs and is appropriately configured to use them.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Optional: In the <strong>Model route</strong> section, select the <strong>Make deployed models available through an external route</strong> check box to make your deployed models available to external clients.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Token authorization</strong> section, select the <strong>Require token authentication</strong> check box to require token authentication for your model server. To finish configuring token authentication, perform the following actions:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Service account name</strong> field, enter a service account name for which the token will be generated. The generated token is created and displayed in the <strong>Token secret</strong> field when the model server is configured.</p>\n</li>\n<li>\n<p>To add an additional service account, click <strong>Add a service account</strong> and enter another service account name.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Configure</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The model server that you updated is displayed in the <strong>Models and model servers</strong> section on the project details page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deleting-a-model-server_nb-server\">Deleting a model server</h4>\n<div class=\"paragraph _abstract\">\n<p>When you no longer need a model server to host models, you can remove it from your data science project.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nWhen you remove a model server, you also remove the models that are hosted on that model server. As a result, the models are no longer available to applications.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have created a data science project and an associated model server.</p>\n</li>\n<li>\n<p>You have notified the users of the applications that access the models that the models will no longer be available.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project from which you want to delete the model server.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the project whose model server you want to delete in the <strong>Models and model servers</strong> section and then click <strong>Delete model server</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Delete model server</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter the name of the model server in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete model server</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The model server that you deleted is no longer displayed in the <strong>Models and model servers</strong> section on the project details page.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_configuring_access_to_data_science_projects\">Configuring access to data science projects</h3>\n<div class=\"sect3\">\n<h4 id=\"configuring-access-to-data-science-projects_nb-server\">Configuring access to data science projects</h4>\n<div class=\"paragraph _abstract\">\n<p>To enable you to work collaboratively on your data science projects with other users, you can share access to your project. After creating your project, you can then set the appropriate access permissions from the Open Data Hub user interface.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can assign the following access permission levels to your data science projects:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Admin - Users can modify all areas of a project, including its details, components, and access permissions.</p>\n</li>\n<li>\n<p>Edit - Users can modify a project&#8217;s components, such as its workbench, but they cannot edit a project&#8217;s access permissions or details.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"sharing-access-to-a-data-science-project_nb-server\">Sharing access to a data science project</h4>\n<div class=\"paragraph _abstract\">\n<p>To enable your organization to work collaboratively, you can share access to your data science project with other users and groups.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the list of data science projects, click the name of the data science project that you want to share access to.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Permissions</strong> tab.</p>\n<div class=\"paragraph\">\n<p>The <strong>Permissions</strong> page for the project opens.</p>\n</div>\n</li>\n<li>\n<p>Provide one or more users with access to the project.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Users</strong> section, click <strong>Add user</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Name</strong> field, enter the user name of the user whom you want to provide access to the project.</p>\n</li>\n<li>\n<p>From the <strong>Permissions</strong> list, select one of the following access permission levels:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Admin: Users with this access level can edit project details and manage access to the project.</p>\n</li>\n<li>\n<p>Edit: Users with this access level can view and edit project components, such as its workbenches, data connections, and storage.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>To confirm your entry, click <strong>Confirm</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhods-confirm-entry-icon.png\" alt=\"The Confirm icon\"></span>).</p>\n</li>\n<li>\n<p>Optional: To add an additional user, click <strong>Add user</strong> and repeat the process.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Provide one or more OpenShift groups with access to the project.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Groups</strong> section, click <strong>Add group</strong>.</p>\n</li>\n<li>\n<p>From the <strong>Name</strong> list, select a group to provide access to the project.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you do not have <code>cluster-admin</code> permissions, the <strong>Name</strong> list is not visible. Instead, an input field is displayed enabling you to configure group permissions.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>From the <strong>Permissions</strong> list, select one of the following access permission levels:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Admin: Groups with this access permission level can edit project details and manage access to the project.</p>\n</li>\n<li>\n<p>Edit: Groups with this access permission level can view and edit project components, such as its workbenches, data connections, and storage.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>To confirm your entry, click <strong>Confirm</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhods-confirm-entry-icon.png\" alt=\"The Confirm icon\"></span>).</p>\n</li>\n<li>\n<p>Optional: To add an additional group, click <strong>Add group</strong> and repeat the process.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Users to whom you provided access to the project can perform only the actions permitted by their access permission level.</p>\n</li>\n<li>\n<p>The <strong>Users</strong> and <strong>Groups</strong> sections on the <strong>Permissions</strong> tab show the respective users and groups that you provided with access to the project.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"updating-access-to-a-data-science-project_nb-server\">Updating access to a data science project</h4>\n<div class=\"paragraph _abstract\">\n<p>To change the level of collaboration on your data science project, you can update the access permissions of users and groups who have access to your project.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project.</p>\n</li>\n<li>\n<p>You have previously shared access to your project with other users or groups.</p>\n</li>\n<li>\n<p>You have administrator permissions or you are the project owner.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to change the access permissions of.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Permissions</strong> tab.</p>\n<div class=\"paragraph\">\n<p>The <strong>Permissions</strong> page for the project opens.</p>\n</div>\n</li>\n<li>\n<p>Update the user access permissions to the project.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Name</strong> field, update the user name of the user whom you want to provide access to the project.</p>\n</li>\n<li>\n<p>From the <strong>Permissions</strong> list, update the user access permissions by selecting one of the following:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Admin: Users with this access level can edit project details and manage access to the project.</p>\n</li>\n<li>\n<p>Edit: Users with this access level can view and edit project components, such as its workbenches, data connections, and storage.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>To confirm the update to the entry, click <strong>Confirm</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhods-confirm-entry-icon.png\" alt=\"The Confirm icon\"></span>).</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Update the OpenShift groups access permissions to the project.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>From the <strong>Name</strong> list, update the group that has access to the project by selecting another group from the list.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you do not have <code>cluster-admin</code> permissions, the <strong>Name</strong> list is not visible. Instead, an input field is displayed enabling you to configure group permissions.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>From the <strong>Permissions</strong> list, update the group access permissions by selecting one of the following:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Admin: Groups with this access permission level can edit project details and manage access to the project.</p>\n</li>\n<li>\n<p>Edit: Groups with this access permission level can view and edit project components, such as its workbenches, data connections, and storage.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>To confirm the update to the entry, click <strong>Confirm</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhods-confirm-entry-icon.png\" alt=\"The Confirm icon\"></span>).</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The <strong>Users</strong> and <strong>Groups</strong> sections on the <strong>Permissions</strong> tab show the respective users and groups whose project access permissions you changed.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"removing-access-to-a-data-science-project_nb-server\">Removing access to a data science project</h4>\n<div class=\"paragraph _abstract\">\n<p>If you no longer want to work collaboratively on your data science project, you can restrict access to your project by removing users and groups that you previously provided access to your project.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project.</p>\n</li>\n<li>\n<p>You have previously shared access to your project with other users or groups.</p>\n</li>\n<li>\n<p>You have administrator permissions or you are the project owner.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to change the access permissions of.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Permissions</strong> tab.</p>\n<div class=\"paragraph\">\n<p>The <strong>Permissions</strong> page for the project opens.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the user or group whose access permissions you want to revoke and click <strong>Delete</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Users whose access you have revoked can no longer perform the actions that were permitted by their access permission level.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-python-packages-installed-on-your-notebook-server_nb-server\">Viewing Python packages installed on your notebook server</h3>\n<div class=\"paragraph _abstract\">\n<p>You can check which Python packages are installed on your notebook server and which version of the package you have by running the <code>pip</code> tool in a notebook cell.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>Log in to Jupyter and open a notebook.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Enter the following in a new cell in your notebook:</p>\n<div class=\"listingblock execute\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>!pip list</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Run the cell.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The output shows an alphabetical list of all installed Python packages and their versions. For example, if you use this command immediately after creating a notebook server that uses the <strong>Minimal</strong> image, the first packages shown are similar to the following:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>Package                           Version\n--------------------------------- ----------\naiohttp                           3.7.3\nalembic                           1.5.2\nappdirs                           1.4.4\nargo-workflows                    3.6.1\nargon2-cffi                       20.1.0\nasync-generator                   1.10\nasync-timeout                     3.0.1\nattrdict                          2.0.1\nattrs                             20.3.0\nbackcall                          0.2.0</code></pre>\n</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"installing-python-packages-on-your-notebook-server_nb-server\">Installing Python packages on your notebook server</h3>\n<div class=\"paragraph _abstract\">\n<p>You can install Python packages that are not part of the default notebook server image by adding the package and the version to a <code>requirements.txt</code> file and then running the <code>pip install</code> command in a notebook cell.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nYou can also install packages directly, but using a <code>requirements.txt</code> file so that the packages stated in the file can be easily re-used across different notebooks is recommended. In addition, using a <code>requirements.txt</code> file is also useful when using a S2I build to deploy a model.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>Log in to Jupyter and open a notebook.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Create a new text file using one of the following methods:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Click <strong>+</strong> to open a new launcher and click <strong>Text file</strong>.</p>\n</li>\n<li>\n<p>Click <strong>File</strong> &#8594; <strong>New</strong> &#8594; <strong>Text File</strong>.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Rename the text file to <code>requirements.txt</code>.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Right-click on the name of the file and click <strong>Rename Text</strong>. The <strong>Rename File</strong> dialog opens.</p>\n</li>\n<li>\n<p>Enter <code>requirements.txt</code> in the <strong>New Name</strong> field and click <strong>Rename</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Add the packages to install to the <code>requirements.txt</code> file.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>altair</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>You can specify the exact version to install by using the <code>==</code> (equal to) operator, for example:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>altair==4.1.0</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Specifying exact package versions to enhance the stability of your notebook server over time is recommended. New package versions can introduce undesirable or unexpected changes in your environment&#8217;s behavior.\nTo install multiple packages at the same time, place each package on a separate line.</p>\n</div>\n</li>\n<li>\n<p>Install the packages in <code>requirements.txt</code> to your server using a notebook cell.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Create a new cell in your notebook and enter the following command:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>!pip install -r requirements.txt</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Run the cell by pressing Shift and Enter.</p>\n</li>\n</ol>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>This command installs the package on your notebook server, but you must still run the <code>import</code> directive in a code cell to use the package in your code.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>import altair</pre>\n</div>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Confirm that the packages in <code>requirements.txt</code> appear in the list of packages installed on the notebook server.\nSee <a href=\"https://opendatahub.io/docs/working-on-data-science-projects#viewing-python-packages-installed-on-your-notebook-server_nb-server\">Viewing Python packages installed on your notebook server</a> for details.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"updating-notebook-server-settings-by-restarting-your-server_nb-server\">Updating notebook server settings by restarting your server</h3>\n<div class=\"paragraph _abstract\">\n<p>You can update the settings on your notebook server by stopping and relaunching the notebook server. For example, if your server runs out of memory, you can restart the server to make the container size larger.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>A running notebook server.</p>\n</li>\n<li>\n<p>Log in to Jupyter.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Click <strong>File</strong> &#8594; <strong>Hub Control Panel</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Notebook server control panel</strong> opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Stop notebook server</strong> button.</p>\n<div class=\"paragraph\">\n<p>The <strong>Stop server</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Click <strong>Stop server</strong> to confirm your decision.</p>\n<div class=\"paragraph\">\n<p>The <strong>Start a notebook server</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Update the relevant notebook server settings and click <strong>Start server</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The notebook server starts and contains your updated settings.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"working-with-data-science-pipelines_ds-pipelines\">Working with data science pipelines</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>As a data scientist, you can enhance your data science projects on Open Data Hub by building portable machine learning (ML) workflows with data science pipelines, using Docker containers. This enables you to standardize and automate machine learning workflows to enable you to develop and deploy your data science models.</p>\n</div>\n<div class=\"paragraph\">\n<p>For example, the steps in a machine learning workflow might include items such as data extraction, data processing, feature extraction, model training, model validation, and model serving. Automating these activities enables your organization to develop a continuous process of retraining and updating a model based on newly received data. This can help address challenges related to building an integrated machine learning deployment and continuously operating it in production.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can also use the Elyra JupyterLab extension to create and run data science pipelines within JupyterLab. For more information, see <a href=\"https://opendatahub.io/docshtml/working_on_data_science_projects/working-with-data-science-pipelines_ds-pipelines#working-with-pipelines-in-jupyterlab\">Working with pipelines in JupyterLab</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>A data science pipeline in Open Data Hub consists of the following components:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Pipeline server: A server that is attached to your data science project and hosts your data science pipeline.</p>\n</li>\n<li>\n<p>Pipeline: A pipeline defines the configuration of your machine learning workflow and the relationship between each component in the workflow.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Pipeline code: A definition of your pipeline in a Tekton-formatted YAML file.</p>\n</li>\n<li>\n<p>Pipeline graph: A graphical illustration of the steps executed in a pipeline run and the relationship between them.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Pipeline run: An execution of your pipeline.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Triggered run: A previously executed pipeline run.</p>\n</li>\n<li>\n<p>Scheduled run: A pipeline run scheduled to execute at least once.</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>This feature is based on Kubeflow Pipelines v1. Use the Kubeflow Pipelines SDK to build your data science pipeline in Python code. After you have built your pipeline, compile it into Tekton-formatted YAML code using kfp-tekton SDK (version 1.5.x only). The Open Data Hub user interface enables you to track and manage pipelines and pipeline runs.</p>\n</div>\n<div class=\"paragraph\">\n<p>Before you can use data science pipelines, you must install the Data Science Pipelines operator as described in <a href=\"https://github.com/opendatahub-io/data-science-pipelines-operator\">Data Science Pipelines Operator</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can store your pipeline artifacts in an Amazon Web Services (AWS) Simple Storage Service (S3) bucket so that you do not consume local storage. To do this, you must first configure write access to your S3 bucket on your AWS account.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_managing_data_science_pipelines\">Managing data science pipelines</h3>\n<div class=\"sect3\">\n<h4 id=\"configuring-a-pipeline-server_ds-pipelines\">Configuring a pipeline server</h4>\n<div class=\"paragraph _abstract\">\n<p>Before you can successfully create a pipeline in Open Data Hub, you must configure a pipeline server. This includes configuring where your pipeline artifacts and data are stored.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that you can add a pipeline server to.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to configure a pipeline server for.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>In the <strong>Pipelines</strong> section, click <strong>Create a pipeline server</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Configure pipeline server</strong> dialog appears.</p>\n</div>\n</li>\n<li>\n<p>In the <strong>Object storage connection</strong> section, to specify the S3-compatible data connection to store your pipeline artifacts, select one of the following sets of actions:</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>After the pipeline server is created, the <code>/metadata</code> and <code>/artifacts</code> folders are automatically created in the default <code>root</code> folder. Therefore, you are not required to specify any storage directories when configuring a data connection for your pipeline server.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Select <strong>Existing data connection</strong> to use a data connection that you previously defined. If you selected this option, from the <strong>Name</strong> list, select the name of the relevant data connection and skip to step 6.</p>\n</li>\n<li>\n<p>Select <strong>Create new data connection</strong> to add a new data connection that your pipeline server can access.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>If you selected <strong>Create new data connection</strong>, perform the following steps:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Name</strong> field, enter a name for the data connection.</p>\n</li>\n<li>\n<p>In the <strong>AWS_ACCESS_KEY_ID</strong> field, enter your access key ID for Amazon Web Services.</p>\n</li>\n<li>\n<p>In the <strong>AWS_SECRET_ACCESS_KEY_ID</strong> field, enter your secret access key for the account you specified.</p>\n</li>\n<li>\n<p>Optional: In the <strong>AWS_S3_ENDPOINT</strong> field, enter the endpoint of your AWS S3 storage.</p>\n</li>\n<li>\n<p>Optional: In the <strong>AWS_DEFAULT_REGION</strong> field, enter the default region of your AWS account.</p>\n</li>\n<li>\n<p>In the <strong>AWS_S3_BUCKET</strong> field, enter the name of the AWS S3 bucket.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you are creating a new data connection, in addition to the other designated mandatory fields, the <strong>AWS_S3_BUCKET</strong> field is mandatory. If you specify incorrect data connection settings, you cannot update these settings on the same pipeline server. Therefore, you must delete the pipeline server and configure another one.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>In the <strong>Database</strong> section, click <strong>Show advanced database options</strong> to specify the database to store your pipeline data and select one of the following sets of actions:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Select <strong>Use default database stored on your cluster</strong> to deploy a MariaDB database in your project.</p>\n</li>\n<li>\n<p>Select <strong>Connect to external MySQL database</strong> to add a new connection to an external database that your pipeline server can access.</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Host</strong> field, enter the database&#8217;s host name.</p>\n</li>\n<li>\n<p>In the <strong>Port</strong> field, enter the database&#8217;s port.</p>\n</li>\n<li>\n<p>In the <strong>Username</strong> field, enter the default user name that is connected to the database.</p>\n</li>\n<li>\n<p>In the <strong>Password</strong> field, enter the password for the default user account.</p>\n</li>\n<li>\n<p>In the <strong>Database</strong> field, enter the database name.</p>\n</li>\n</ol>\n</div>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Click <strong>Configure</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline server that you configured is displayed in the <strong>Pipelines</strong> section on the project details page.</p>\n</li>\n<li>\n<p>The <strong>Import pipeline</strong> button is available in the <strong>Pipelines</strong> section on the project details page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"defining-a-pipeline_ds-pipelines\">Defining a pipeline</h4>\n<div class=\"paragraph _abstract\">\n<p>The Kubeflow Pipelines SDK enables you to define end-to-end machine learning and data pipelines. Use the Kubeflow Pipelines SDK to build your data science pipeline in Python code. After you have built your pipeline, compile it into Tekton-formatted YAML code using kfp-tekton SDK (version 1.5.x only). After defining the pipeline, you can import the YAML file to the Open Data Hub dashboard to enable you to configure its execution settings. For more information about installing and using Kubeflow Pipelines SDK for Tetkon, see <a href=\"https://kubeflow.org/docs/components/pipelines/v1/sdk/pipelines-with-tekton/\">Kubeflow Pipelines SDK for Tekton</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can also use the Elyra JupyterLab extension to create and run data science pipelines within JupyterLab. For more information on the Elyra JupyterLab extension, see <a href=\"https://elyra.readthedocs.io/en/stable/getting_started/overview.html\">Elyra Documentation</a>.</p>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://github.com/kubeflow/kfp-tekton/tree/master/sdk\">Kubeflow Pipelines SDK for Tekton</a></p>\n</li>\n<li>\n<p><a href=\"https://github.com/kubeflow/kfp-tekton/tree/master/samples\">KFP Tekton samples and compiler samples</a></p>\n</li>\n<li>\n<p><a href=\"https://www.kubeflow.org/docs/components/pipelines/v1/\">Kubeflow Pipelines v1 Documentation</a></p>\n</li>\n<li>\n<p><a href=\"https://elyra.readthedocs.io/en/stable/getting_started/overview.html\">Elyra Documentation</a></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"importing-a-data-science-pipeline_ds-pipelines\">Importing a data science pipeline</h4>\n<div class=\"paragraph _abstract\">\n<p>To help you begin working with data science pipelines in Open Data Hub, you can import a YAML file containing your pipeline&#8217;s code to an active pipeline server. This file contains a Kubeflow pipeline compiled with the Tekton compiler. After you have imported the pipeline to a pipeline server, you can execute the pipeline by creating a pipeline run.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Pipelines</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project that you want to import a pipeline to.</p>\n</li>\n<li>\n<p>Click <strong>Import pipeline</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Import pipeline</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter the details for the pipeline that you are importing.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Pipeline name</strong> field, enter a name for the pipeline that you are importing.</p>\n</li>\n<li>\n<p>In the <strong>Pipeline description</strong> field, enter a description for the pipeline that you are importing.</p>\n</li>\n<li>\n<p>Click <strong>Upload</strong>. Alternatively, drag the file from your local machine&#8217;s file system and drop it in the designated area in the <strong>Import pipeline</strong> dialog.</p>\n<div class=\"paragraph\">\n<p>A file browser opens.</p>\n</div>\n</li>\n<li>\n<p>Navigate to the file containing the pipeline code and click <strong>Select</strong>.</p>\n</li>\n<li>\n<p>Click <strong>Import pipeline</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline that you imported is displayed on the <strong>Pipelines</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"downloading-a-data-science-pipeline_ds-pipelines\">Downloading a data science pipeline</h4>\n<div class=\"paragraph _abstract\">\n<p>To make further changes to a data science pipeline that you previously uploaded to Open Data Hub, you can download the pipeline&#8217;s code from the user interface.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have created and imported a pipeline to an active pipeline server that is available to download.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Pipelines</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project whose pipeline that you want to download.</p>\n</li>\n<li>\n<p>In the <strong>Pipeline name</strong> column, click the name of the pipeline that you want to download.</p>\n<div class=\"paragraph\">\n<p>The <strong>Pipeline details</strong> page opens displaying the <strong>Graph</strong> tab.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>YAML</strong> tab.</p>\n<div class=\"paragraph\">\n<p>The page reloads to display an embedded YAML editor showing the pipeline code.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Download</strong> button (<span class=\"image\"><img src=\"/static/docs/images/rhods-download-icon.png\" alt=\"rhods download icon\"></span>) to download the YAML file containing your pipeline&#8217;s code to your local machine.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline code is downloaded to your browser&#8217;s default directory for downloaded files.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deleting-a-data-science-pipeline_ds-pipelines\">Deleting a data science pipeline</h4>\n<div class=\"paragraph _abstract\">\n<p>You can delete data science pipelines so that they do not appear on the Open Data Hub <strong>Pipelines</strong> page.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>There are active pipelines available on the <strong>Pipelines</strong> page.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Pipelines</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project that contains the pipeline that you want to delete.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the pipeline that you want to delete and click <strong>Delete pipeline</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Delete pipeline</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter the pipeline name in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete pipeline</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The data science pipeline that you deleted is no longer displayed on the <strong>Pipelines</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deleting-a-pipeline-server_ds-pipelines\">Deleting a pipeline server</h4>\n<div class=\"paragraph _abstract\">\n<p>After you have finished running your data science pipelines, you can delete the pipeline server. Deleting a pipeline server automatically deletes all of its associated pipelines and runs. If your pipeline data is stored in a database, the database is also deleted along with its meta-data. In addition, after deleting a pipeline server, you cannot create new pipelines or pipeline runs until you create another pipeline server.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Pipelines</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project whose pipeline server you want to delete.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline server actions</strong> list, select <strong>Delete pipeline server</strong>.\nThe <strong>Delete pipeline server</strong> dialog opens.</p>\n</li>\n<li>\n<p>Enter the pipeline server&#8217;s name in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Pipelines previously assigned to the deleted pipeline server are no longer displayed on the <strong>Pipelines</strong> page for the relevant data science project.</p>\n</li>\n<li>\n<p>Pipeline runs previously assigned to the deleted pipeline server are no longer displayed on the <strong>Runs</strong> page for the relevant data science project.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"viewing-the-details-of-a-pipeline-server_ds-pipelines\">Viewing the details of a pipeline server</h4>\n<div class=\"paragraph _abstract\">\n<p>You can view the details of pipeline servers configured in Open Data Hub, such as the pipeline&#8217;s data connection details and where its data is stored.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>You have previously created a data science project that contains an active and available pipeline server.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Pipelines</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project whose pipeline server you want to view.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline server actions</strong> list, select <strong>View pipeline server configuration</strong>.</p>\n</li>\n<li>\n<p>When you have finished inspecting the pipeline server&#8217;s details, click <strong>Done</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>You can view the relevant pipeline server&#8217;s details in the <strong>View pipeline server</strong> dialog.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"viewing-existing-pipelines_ds-pipelines\">Viewing existing pipelines</h4>\n<div class=\"paragraph _abstract\">\n<p>You can view the details of pipelines that you have imported to Open Data Hub, such as the pipeline&#8217;s last run, when it was created, and the pipeline&#8217;s executed runs.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active and available pipeline server.</p>\n</li>\n<li>\n<p>The pipeline you imported is available, or there are other previously imported pipelines available to view.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Pipelines</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the relevant project whose pipelines you want to view.</p>\n</li>\n<li>\n<p>Study the pipelines on the list.</p>\n</li>\n<li>\n<p>Optional: Click <strong>Expand</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhods-expand-icon.png\" alt=\"rhods expand icon\"></span>) on the relevant row to view the pipeline&#8217;s executed runs. If the pipeline does not contain any runs, click <strong>Create run</strong> to create one.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>A list of previously created data science pipelines is displayed on the <strong>Pipelines</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_managing_pipeline_runs\">Managing pipeline runs</h3>\n<div class=\"sect3\">\n<h4 id=\"overview-of-pipeline-runs_ds-pipelines\">Overview of pipeline runs</h4>\n<div class=\"paragraph _abstract\">\n<p>A pipeline run is a single execution of a data science pipeline. As data scientist, you can use Open Data Hub to define, manage, and track executions of a data science pipeline. You can view a record of your data science project&#8217;s previously executed and scheduled runs from the <strong>Runs</strong> page in the Open Data Hub user interface.</p>\n</div>\n<div class=\"paragraph\">\n<p>Runs are intended for portability. Therefore, you can clone your pipeline runs to reproduce and scale them accordingly, or delete them when you longer require them. You can configure a run to execute only once immediately after creation or on a recurring basis. Recurring runs consist of a copy of a pipeline with all of its parameter values and a run trigger. A run trigger indicates when a recurring run executes. You can define the following run triggers:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Periodic: used for scheduling runs to execute in intervals.</p>\n</li>\n<li>\n<p>Cron: used for scheduling runs as a cron job.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>When executed, you can track the run&#8217;s progress from the run&#8217;s <strong>Details</strong> page on the Open Data Hub user interface. From here, you can view the run&#8217;s graph, and output artifacts.</p>\n</div>\n<div class=\"paragraph\">\n<p>A pipeline run can be classified as the following:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Scheduled run: A pipeline run scheduled to execute at least once</p>\n</li>\n<li>\n<p>Triggered run: A previously executed pipeline run.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"scheduling-a-pipeline-run_ds-pipelines\">Scheduling a pipeline run</h4>\n<div class=\"paragraph _abstract\">\n<p>You can instantiate a single execution of a pipeline by scheduling a pipeline run. In Open Data Hub, you can schedule runs to occur at specific times or execute them immediately after creation.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Pipelines</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the relevant pipeline and click <strong>Create run</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Create run</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project that contains the pipeline you want to create a run for.</p>\n</li>\n<li>\n<p>In the <strong>Name</strong> field, enter a name for the run.</p>\n</li>\n<li>\n<p>In the <strong>Description</strong> field, enter a description for the run.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline</strong> list, select the pipeline to create a run for. Alternatively, to upload a new pipeline, click <strong>Upload new pipeline</strong> and fill in the relevant fields in the <strong>Import pipeline</strong> dialog.</p>\n</li>\n<li>\n<p>Configure the run type by performing one of the following sets of actions:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Select <strong>Run once immediately after creation</strong> to specify the run executes once, and immediately after its creation.</p>\n</li>\n<li>\n<p>Select <strong>Schedule recurring run</strong> to schedule the run to recur.</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>Configure the run&#8217;s trigger type.</p>\n<div class=\"olist upperalpha\">\n<ol class=\"upperalpha\" type=\"A\">\n<li>\n<p>Select <strong>Periodic</strong> and select the execution frequency from the list.</p>\n</li>\n<li>\n<p>Select <strong>Cron</strong> to specify the execution schedule in <code>cron</code> format. This creates a cron job to execute the run. Click the <strong>Copy</strong> button (<span class=\"image\"><img src=\"/static/docs/images/osd-copy.png\" alt=\"osd copy\"></span>) to copy the cron job schedule to the clipboard. The field furthest to the left represents seconds. For more information about scheduling tasks using the supported <code>cron</code> format, see <a href=\"https://pkg.go.dev/github.com/robfig/cron#hdr-CRON_Expression_Format\">Cron Expression Format</a>.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Configure the run&#8217;s duration.</p>\n<div class=\"olist upperalpha\">\n<ol class=\"upperalpha\" type=\"A\">\n<li>\n<p>Select the <strong>Start date</strong> check box to specify a start date for the run. Select the run&#8217;s start date using the <strong>Calendar</strong> and the start time from the list of times.</p>\n</li>\n<li>\n<p>Select the <strong>End date</strong> check box to specify an end date for the run. Select the run&#8217;s end date using the <strong>Calendar</strong> and the end time from the list of times.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Configure the input parameters for the run by selecting the parameters from the list.</p>\n</li>\n<li>\n<p>Click <strong>Create</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline run that you created is shown in the <strong>Scheduled</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"cloning-a-scheduled-pipeline-run_ds-pipelines\">Cloning a scheduled pipeline run</h4>\n<div class=\"paragraph _abstract\">\n<p>To make it easier to schedule runs to execute as part of your pipeline configuration, you can duplicate existing scheduled runs by cloning them.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have previously scheduled a run that is available to clone.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Runs</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the relevant run and click <strong>Clone</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Clone</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project that contains the pipeline whose run that you want to clone.</p>\n</li>\n<li>\n<p>In the <strong>Name</strong> field, enter a name for the run that you want to clone.</p>\n</li>\n<li>\n<p>In the <strong>Description</strong> field, enter a description for the run that you want to clone.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline</strong> list, select the pipeline containing the run that you want to clone.</p>\n</li>\n<li>\n<p>To configure the run type for the run that you are cloning, in the <strong>Run type</strong> section, perform one of the following sets of actions:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Select <strong>Run once immediately after create</strong> to specify the run that you are cloning executes once, and immediately after its creation. If you selected this option, skip to step 10.</p>\n</li>\n<li>\n<p>Select <strong>Schedule recurring run</strong> to schedule the run that you are cloning to recur.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>If you selected <strong>Schedule recurring run</strong> in the previous step, configure the trigger type for the run, perform one of the following actions:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Select <strong>Periodic</strong> and select the execution frequency from the <strong>Run every</strong> list.</p>\n</li>\n<li>\n<p>Select <strong>Cron</strong> to specify the execution schedule in <code>cron</code> format. This creates a cron job to execute the run. Click the <strong>Copy</strong> button (<span class=\"image\"><img src=\"/static/docs/images/osd-copy.png\" alt=\"osd copy\"></span>) to copy the cron job schedule to the clipboard. The field furthest to the left represents seconds. For more information about scheduling tasks using the supported <code>cron</code> format, see <a href=\"https://pkg.go.dev/github.com/robfig/cron#hdr-CRON_Expression_Format\">Cron Expression Format</a>.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>If you selected <strong>Schedule recurring run</strong> in step 7, configure the duration for the run that you are cloning.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Select the <strong>Start date</strong> check box to specify a start date for the run. Select the start date using the calendar tool and the start time from the list of times.</p>\n</li>\n<li>\n<p>Select the <strong>End date</strong> check box to specify an end date for the run. Select the end date using the calendar tool and the end time from the list of times.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>In the <strong>Parameters</strong> section, configure the input parameters for the run that you are cloning by selecting the appropriate parameters from the list.</p>\n</li>\n<li>\n<p>Click <strong>Create</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline run that you cloned is shown in the <strong>Scheduled</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"stopping-a-triggered-pipeline-run_ds-pipelines\">Stopping a triggered pipeline run</h4>\n<div class=\"paragraph _abstract\">\n<p>If you no longer require a triggered pipeline run to continue executing, you can stop the run before its defined end date.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>There is a previously created data science project available that contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active and available pipeline server.</p>\n</li>\n<li>\n<p>You have previously triggered a pipeline run.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Runs</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project whose pipeline runs you want to stop.</p>\n</li>\n<li>\n<p>Click the <strong>Triggered</strong> tab.</p>\n</li>\n<li>\n<p>In the <strong>Name</strong> column in the table, click the name of the run that you want to stop.</p>\n<div class=\"paragraph\">\n<p>The <strong>Run details</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Actions</strong> list, select <strong>Stop run</strong></p>\n<div class=\"paragraph\">\n<p>There might be a short delay while the run stops.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>A list of previously triggered runs are displayed in the <strong>Triggered</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deleting-a-scheduled-pipeline-run_ds-pipelines\">Deleting a scheduled pipeline run</h4>\n<div class=\"paragraph _abstract\">\n<p>To discard pipeline runs that you previously scheduled, but no longer require, you can delete them so that they do not appear on the <strong>Runs</strong> page.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have previously scheduled a run that is available to delete.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Runs</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project that contains the pipeline whose scheduled run you want to delete.</p>\n<div class=\"paragraph\">\n<p>The page refreshes to show the pipeline&#8217;s scheduled runs on the <strong>Scheduled</strong> tab.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the scheduled run that you want to delete and click <strong>Delete</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Delete scheduled run</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter the run&#8217;s name in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete scheduled run</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The run that you deleted is no longer displayed on the <strong>Scheduled</strong> tab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deleting-a-triggered-pipeline-run_ds-pipelines\">Deleting a triggered pipeline run</h4>\n<div class=\"paragraph _abstract\">\n<p>To discard pipeline runs that you previously executed, but no longer require a record of, you can delete them so that they do not appear on the <strong>Triggered</strong> tab on the <strong>Runs</strong> page.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have previously executed a run that is available to delete.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Runs</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project that contains the pipeline whose triggered run you want to delete.</p>\n<div class=\"paragraph\">\n<p>The page refreshes to show the pipeline&#8217;s triggered runs on the <strong>Triggered</strong> tab.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the triggered run that you want to delete and click <strong>Delete</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Delete triggered run</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter the run&#8217;s name in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete triggered run</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The run that you deleted is no longer displayed on the <strong>Triggered</strong> tab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"viewing-scheduled-pipeline-runs_ds-pipelines\">Viewing scheduled pipeline runs</h4>\n<div class=\"paragraph _abstract\">\n<p>You can view a list of pipeline runs that are scheduled for execution in Open Data Hub. From this list, you can view details relating to your pipeline&#8217;s runs, such as the pipeline that the run belongs to. You can also view the run&#8217;s status, execution frequency, and schedule.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>You have installed the Data Science Pipelines operator.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active and available pipeline server.</p>\n</li>\n<li>\n<p>You have created and scheduled a pipeline run.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Runs</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project whose scheduled pipeline runs you want to view.</p>\n</li>\n<li>\n<p>Click the <strong>Scheduled</strong> tab.</p>\n</li>\n<li>\n<p>Study the table showing a list of scheduled runs.</p>\n<div class=\"paragraph\">\n<p>After a run has been scheduled, the run&#8217;s status is displayed in the <strong>Status</strong> column in the table, indicating whether the run is ready for execution or unavailable for execution. To enable or disable a previously imported notebook image, on the row containing the relevant notebook image, click the toggle in the <strong>Enabled</strong> column.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>A list of scheduled runs are displayed in the <strong>Scheduled</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"viewing-triggered-pipeline-runs_ds-pipelines\">Viewing triggered pipeline runs</h4>\n<div class=\"paragraph _abstract\">\n<p>You can view a list of pipeline runs that were previously executed in Open Data Hub. From this list, you can view details relating to your pipeline&#8217;s runs, such as the pipeline that the run belongs to, along with the run&#8217;s status, duration, and execution start time.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>You have installed the Data Science Pipelines operator.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active and available pipeline server.</p>\n</li>\n<li>\n<p>You have previously triggered a pipeline run.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Runs</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project whose previously executed pipeline runs you want to view.</p>\n<div class=\"paragraph\">\n<p>The <strong>Run details</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Triggered</strong> tab.</p>\n<div class=\"paragraph\">\n<p>A table opens that shows list of triggered runs. After a run has completed its execution, the run&#8217;s status is displayed in the <strong>Status</strong> column in the table, indicating whether the run has succeeded or failed.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>A list of previously triggered runs are displayed in the <strong>Triggered</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"viewing-the-details-of-a-pipeline-run_ds-pipelines\">Viewing the details of a pipeline run</h4>\n<div class=\"paragraph _abstract\">\n<p>To gain a clearer understanding of your pipeline runs, you can view the details of a previously triggered pipeline run, such as its graph, execution details, and run output.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active and available pipeline server.</p>\n</li>\n<li>\n<p>You have previously triggered a pipeline run.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Pipelines</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project whose pipeline runs you want to view.</p>\n</li>\n<li>\n<p>For a pipeline that you want to see run details for, click <strong>Expand</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhods-expand-icon.png\" alt=\"rhods expand icon\"></span>).</p>\n</li>\n<li>\n<p>From the <strong>Runs</strong> section, click the name of the run that you want to view the details of.</p>\n<div class=\"paragraph\">\n<p>The <strong>Run details</strong> page opens.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>On the <strong>Run details</strong> page, you can view the run&#8217;s graph, execution details, and run output.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_working_with_pipelines_in_jupyterlab\">Working with pipelines in JupyterLab</h3>\n<div class=\"sect3\">\n<h4 id=\"overview-of-pipelines-in-jupyterlab_ds-pipelines\">Overview of pipelines in JupyterLab</h4>\n<div class=\"paragraph _abstract\">\n<p>You can use Elyra to create visual end-to-end pipeline workflows in JupyterLab. Elyra is an extension for JupyterLab that provides you with a Pipeline Editor to create pipeline workflows that can be executed in Open Data Hub.</p>\n</div>\n<div class=\"paragraph\">\n<p>Before you can work with pipelines in JupyterLab, you must install the Data Science Pipelines operator as described in <a href=\"https://github.com/opendatahub-io/data-science-pipelines-operator\">Data Science Pipelines Operator</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can access the Elyra extension within JupyterLab when you create the most recent version of one of the following notebook images:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Standard Data Science</p>\n</li>\n<li>\n<p>PyTorch</p>\n</li>\n<li>\n<p>TensorFlow</p>\n</li>\n<li>\n<p>TrustyAI</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>As you can use the Pipeline Editor to visually design your pipelines, minimal coding is required to create and run pipelines. For more information about Elyra, see <a href=\"https://elyra.readthedocs.io/en/stable/getting_started/overview.html\">Elyra Documentation</a>. For more information on the Pipeline Editor, see <a href=\"https://elyra.readthedocs.io/en/stable/user_guide/jupyterlab-interface.html#visual-pipeline-editor\">Visual Pipeline Editor</a>. After you have created your pipeline, you can run it locally in JupyterLab, or remotely using data science pipelines in Open Data Hub.</p>\n</div>\n<div class=\"paragraph\">\n<p>The pipeline creation process consists of the following tasks:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Create a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>Create a pipeline server.</p>\n</li>\n<li>\n<p>Create a new pipeline in the Pipeline Editor in JupyterLab.</p>\n</li>\n<li>\n<p>Develop your pipeline by adding Python notebooks or Python scripts and defining their runtime properties.</p>\n</li>\n<li>\n<p>Define execution dependencies.</p>\n</li>\n<li>\n<p>Run or export your pipeline.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>Before you can run a pipeline in JupyterLab, your pipeline instance must contain a runtime configuration. A runtime configuration defines connectivity information for your pipeline instance and S3-compatible cloud storage.</p>\n</div>\n<div class=\"paragraph\">\n<p>If you create a workbench as part of a data science project, a default runtime configuration is created automatically. However, if you create a notebook from the Jupyter tile in the Open Data Hub dashboard, you must create a runtime configuration before you can run your pipeline in JupyterLab. For more information about runtime configurations, see <a href=\"https://elyra.readthedocs.io/en/stable/user_guide/runtime-conf.html\">Runtime Configuration</a>. As a prerequisite, before you create a workbench, ensure that you have created and configured a pipeline server within the same data science project as your workbench.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can use S3-compatible cloud storage to make data available to your notebooks and scripts while they are executed. Your cloud storage must be accessible from the machine in your deployment that runs JupyterLab and from the cluster that hosts Data Science Pipelines. Before you create and run pipelines in JupyterLab, ensure that you have your s3-compatible storage credentials readily available.</p>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://elyra.readthedocs.io/en/stable/getting_started/overview.html\">Elyra Documentation</a></p>\n</li>\n<li>\n<p><a href=\"https://elyra.readthedocs.io/en/stable/user_guide/jupyterlab-interface.html#visual-pipeline-editor\">Visual Pipeline Editor</a></p>\n</li>\n<li>\n<p><a href=\"https://elyra.readthedocs.io/en/stable/user_guide/runtime-conf.html\">Runtime Configuration</a>.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"accessing-the-pipeline-editor_ds-pipelines\">Accessing the pipeline editor</h4>\n<div class=\"paragraph _abstract\">\n<p>You can use Elyra to create visual end-to-end pipeline workflows in JupyterLab. Elyra is an extension for JupyterLab that provides you with a Pipeline Editor to create pipeline workflows that can be executed in Open Data Hub.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, or  PyTorch).</p>\n</li>\n<li>\n<p>You have access to S3-compatible storage.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>After you open JupyterLab, confirm that the JupyterLab launcher is automatically displayed.</p>\n</li>\n<li>\n<p>In the <strong>Elyra</strong> section of the JupyterLab launcher, click the <strong>Pipeline Editor</strong> tile.</p>\n<div class=\"paragraph\">\n<p>The Pipeline Editor opens.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>You can view the Pipeline Editor in JupyterLab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"creating-a-runtime-configuration_ds-pipelines\">Creating a runtime configuration</h4>\n<div class=\"paragraph _abstract\">\n<p>If you create a workbench as part of a data science project, a default runtime configuration is created automatically. However, if you create a notebook from the Jupyter tile in the Open Data Hub dashboard, you must create a runtime configuration before you can run your pipeline in JupyterLab. This enables you to specify connectivity information for your pipeline instance and S3-compatible cloud storage.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have access to S3-compatible cloud storage.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, or  PyTorch).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left sidebar of JupyterLab, click <strong>Runtimes</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>).</p>\n</li>\n<li>\n<p>Click the <strong>Create new runtime configuration</strong> button (<span class=\"image\"><img src=\"/static/docs/images/jupyter-create-runtime.png\" alt=\"Create new runtime configuration\"></span>).</p>\n<div class=\"paragraph\">\n<p>The <strong>Add new Data Science Pipelines runtime configuration</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Fill in the relevant fields to define your runtime configuration.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Display Name</strong> field, enter a name for your runtime configuration.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Description</strong> field, enter a description to define your runtime configuration.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Tags</strong> field, click <strong>Add Tag</strong> to define a category for your pipeline instance. Enter a name for the tag and press Enter.</p>\n</li>\n<li>\n<p>Define the credentials of your data science pipeline:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint</strong> field, enter the API endpoint of your data science pipeline. Do not specify the pipelines namespace in this field.</p>\n</li>\n<li>\n<p>In the <strong>Public Data Science Pipelines API Endpoint</strong> field, enter the public API endpoint of your data science pipeline.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>You can obtain the Data Science Pipelines API endpoint from the <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong> page in the dashboard. Copy the relevant end point and enter it in the <strong>Public Data Science Pipelines API Endpoint</strong> field.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Optional: In the <strong>Data Science Pipelines User Namespace</strong> field, enter the relevant user namespace to run pipelines.</p>\n</li>\n<li>\n<p>From the <strong>Data Science Pipelines engine</strong> list, select <code>Tekton</code>.</p>\n</li>\n<li>\n<p>From the <strong>Authentication Type</strong> list, select the authentication type required to authenticate your pipeline.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you created a notebook directly from the Jupyter tile on the dashboard, select <code>EXISTING_BEARER_TOKEN</code> from the <strong>Authentication Type</strong> list.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint Username</strong> field, enter the user name required for the authentication type.</p>\n</li>\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint Password Or Token</strong>, enter the password or token required for the authentication type.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>To obtain the Data Science Pipelines API endpoint token, in the upper-right corner of the OpenShift web console, click your user name and select <strong>Copy login command</strong>. After you have logged in, click <strong>Display token</strong> and copy the value of <code>--token=</code> from the <strong>Log in with this token</strong> command.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Define the connectivity information of your S3-compatible storage:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Cloud Object Storage Endpoint</strong> field, enter the endpoint of your S3-compatible storage. For more information about Amazon s3 endpoints, see <a href=\"https://docs.aws.amazon.com/general/latest/gr/s3.html\">Amazon Simple Storage Service endpoints and quotas</a>.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Public Cloud Object Storage Endpoint</strong> field, enter the URL of your S3-compatible storage.</p>\n</li>\n<li>\n<p>In the <strong>Cloud Object Storage Bucket Name</strong> field, enter the name of the bucket where your pipeline artifacts are stored. If the bucket name does not exist, it is created automatically.</p>\n</li>\n<li>\n<p>From the <strong>Cloud Object Storage Authentication Type</strong> list, select the authentication type required to access to your S3-compatible cloud storage. If you use AWS S3 buckets, select <code>KUBERNETES_SECRET</code> from the list.</p>\n</li>\n<li>\n<p>In the <strong>Cloud Object Storage Credentials Secret</strong> field, enter the secret that contains the storage user name and password. This secret is defined in the relevant user namespace, if applicable. In addition, it must be stored on the cluster that hosts your pipeline runtime.</p>\n</li>\n<li>\n<p>In the <strong>Cloud Object Storage Username</strong> field, enter the user name to connect to your S3-compatible cloud storage, if applicable. If you use AWS S3 buckets, enter your AWS Secret Access Key ID.</p>\n</li>\n<li>\n<p>In the <strong>Cloud Object Storage Password</strong> field, enter the password to connect to your S3-compatible cloud storage, if applicable. If you use AWS S3 buckets, enter your AWS Secret Access Key.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Save &amp; Close</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The runtime configuration that you created is shown in the <strong>Runtimes</strong> tab (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>) in the left sidebar of JupyterLab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"updating-a-runtime-configuration_ds-pipelines\">Updating a runtime configuration</h4>\n<div class=\"paragraph _abstract\">\n<p>To ensure that your runtime configuration is accurate and updated, you can change the settings of an existing runtime configuration.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have access to S3-compatible storage.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>A previously created runtime configuration is available in the JupyterLab interface.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, or  PyTorch).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left sidebar of JupyterLab, click <strong>Runtimes</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>).</p>\n</li>\n<li>\n<p>Hover the cursor over the runtime configuration that you want to update and click the <strong>Edit</strong> button (<span class=\"image\"><img src=\"/static/docs/images/rhods-edit-icon.png\" alt=\"Edit runtime configuration\"></span>).</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Pipelines runtime configuration</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Fill in the relevant fields to update your runtime configuration.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Display Name</strong> field, update name for your runtime configuration, if applicable.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Description</strong> field, update the description of your runtime configuration, if applicable.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Tags</strong> field, click <strong>Add Tag</strong> to define a category for your pipeline instance. Enter a name for the tag and press Enter.</p>\n</li>\n<li>\n<p>Define the credentials of your data science pipeline:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint</strong> field, update the API endpoint of your data science pipeline, if applicable. Do not specify the pipelines namespace in this field.</p>\n</li>\n<li>\n<p>In the <strong>Public Data Science Pipelines API Endpoint</strong> field, update the API endpoint of your data science pipeline, if applicable.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Data Science Pipelines User Namespace</strong> field, update the relevant user namespace to run pipelines, if applicable.</p>\n</li>\n<li>\n<p>From the <strong>Data Science Pipelines engine</strong> list, select <code>Tekton</code>.</p>\n</li>\n<li>\n<p>From the <strong>Authentication Type</strong> list, select a new authentication type required to authenticate your pipeline, if applicable.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you created a notebook directly from the Jupyter tile on the dashboard, select <code>EXISTING_BEARER_TOKEN</code> from the <strong>Authentication Type</strong> list.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint Username</strong> field, update the user name required for the authentication type, if applicable.</p>\n</li>\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint Password Or Token</strong>, update the password or token required for the authentication type, if applicable.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>To obtain the Data Science Pipelines API endpoint token, in the upper-right corner of the OpenShift web console, click your user name and select <strong>Copy login command</strong>. After you have logged in, click <strong>Display token</strong> and copy the value of <code>--token=</code> from the <strong>Log in with this token</strong> command.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Define the connectivity information of your S3-compatible storage:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Cloud Object Storage Endpoint</strong> field, update the endpoint of your S3-compatible storage, if applicable. For more information about Amazon s3 endpoints, see <a href=\"https://docs.aws.amazon.com/general/latest/gr/s3.html\">Amazon Simple Storage Service endpoints and quotas</a>.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Public Cloud Object Storage Endpoint</strong> field, update the URL of your S3-compatible storage, if applicable.</p>\n</li>\n<li>\n<p>In the <strong>Cloud Object Storage Bucket Name</strong> field, update the name of the bucket where your pipeline artifacts are stored, if applicable. If the bucket name does not exist, it is created automatically.</p>\n</li>\n<li>\n<p>From the <strong>Cloud Object Storage Authentication Type</strong> list, update the authentication type required to access to your S3-compatible cloud storage, if applicable. If you use AWS S3 buckets, you must select <code>USER_CREDENTIALS</code> from the list.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Cloud Object Storage Credentials Secret</strong> field, update the secret that contains the storage user name and password, if applicable. This secret is defined in the relevant user namespace. You must save the secret on the cluster that hosts your pipeline runtime.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Cloud Object Storage Username</strong> field, update the user name to connect to your S3-compatible cloud storage, if applicable. If you use AWS S3 buckets, update your AWS Secret Access Key ID.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Cloud Object Storage Password</strong> field, update the password to connect to your S3-compatible cloud storage, if applicable. If you use AWS S3 buckets, update your AWS Secret Access Key.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Save &amp; Close</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The runtime configuration that you updated is shown in the <strong>Runtimes</strong> tab (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>) in the left sidebar of JupyterLab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deleting-a-runtime-configuration_ds-pipelines\">Deleting a runtime configuration</h4>\n<div class=\"paragraph _abstract\">\n<p>After you have finished using your runtime configuration, you can delete it from the JupyterLab interface. After deleting a runtime configuration, you cannot run pipelines in JupyterLab until you create another runtime configuration.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>A previously created runtime configuration is visible in the JupyterLab interface.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, or  PyTorch).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left sidebar of JupyterLab, click <strong>Runtimes</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>).</p>\n</li>\n<li>\n<p>Hover the cursor over the runtime configuration that you want to delete and click the <strong>Delete Item</strong> button (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-trash-button.png\" alt=\"Delete item\"></span>).</p>\n<div class=\"paragraph\">\n<p>A dialog box appears prompting you to confirm the deletion of your runtime configuration.</p>\n</div>\n</li>\n<li>\n<p>Click <strong>OK</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The runtime configuration that you deleted is no longer shown in the <strong>Runtimes</strong> tab (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>) in the left sidebar of JupyterLab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"duplicating-a-runtime-configuration_ds-pipelines\">Duplicating a runtime configuration</h4>\n<div class=\"paragraph _abstract\">\n<p>To prevent you from re-creating runtime configurations with similar values in their entirety, you can duplicate an existing runtime configuration in the JupyterLab interface.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>A previously created runtime configuration is visible in the JupyterLab interface.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, or  PyTorch).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left sidebar of JupyterLab, click <strong>Runtimes</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>).</p>\n</li>\n<li>\n<p>Hover the cursor over the runtime configuration that you want to duplicate and click the <strong>Duplicate</strong> button (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-duplicate.png\" alt=\"Duplicate\"></span>).</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The runtime configuration that you duplicated is shown in the <strong>Runtimes</strong> tab (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>) in the left sidebar of JupyterLab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"running-a-pipeline-in-jupyterlab_ds-pipelines\">Running a pipeline in JupyterLab</h4>\n<div class=\"paragraph _abstract\">\n<p>You can run pipelines that you have created in JupyterLab from the Pipeline Editor user interface. Before you can run a pipeline, you must create a data science project and a pipeline server. After you create a pipeline server, you must create a workbench within the same project as your pipeline server.\nYour pipeline instance in JupyterLab must contain a runtime configuration. If you create a workbench as part of a data science project, a default runtime configuration is created automatically. However, if you create a notebook from the Jupyter tile in the Open Data Hub dashboard, you must create a runtime configuration before you can run your pipeline in JupyterLab. A runtime configuration defines connectivity information for your pipeline instance and S3-compatible cloud storage.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have access to S3-compatible storage.</p>\n</li>\n<li>\n<p>You have created a pipeline in JupyterLab.</p>\n</li>\n<li>\n<p>You have opened your pipeline in the Pipeline Editor in JupyterLab.</p>\n</li>\n<li>\n<p>Your pipeline instance contains a runtime configuration.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, or  PyTorch).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the Pipeline Editor user interface, click <strong>Run Pipeline</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-run-pipeline-button.png\" alt=\"The Runtimes icon\"></span>).</p>\n<div class=\"paragraph\">\n<p>The <strong>Run Pipeline</strong> dialog appears. The <strong>Pipeline Name</strong> field is automatically populated with the pipeline file name.</p>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>You must enter a unique pipeline name. The pipeline name that you enter must not match the name of any previously executed pipelines.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Define the settings for your pipeline run.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>From the <strong>Runtime Configuration</strong> list, select the relevant runtime configuration to run your pipeline.</p>\n</li>\n<li>\n<p>Optional: Configure your pipeline parameters, if applicable. If your pipeline contains nodes that reference pipeline parameters, you can change the default parameter values. If a parameter is required and has no default value, you must enter a value.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>OK</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>You can view the output artifacts of your pipeline run. The artifacts are stored in your designated object storage bucket.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"exporting-a-pipeline-in-jupyterlab_ds-pipelines\">Exporting a pipeline in JupyterLab</h4>\n<div class=\"paragraph _abstract\">\n<p>You can export pipelines that you have created in JupyterLab. When you export a pipeline, the pipeline is prepared for later execution, but is not uploaded or executed immediately. During the export process, any package dependencies are uploaded to S3-compatible storage. Also, pipeline code is generated for the target runtime.</p>\n</div>\n<div class=\"paragraph\">\n<p>Before you can export a pipeline, you must create a data science project and a pipeline server. After you create a pipeline server, you must create a workbench within the same project as your pipeline server. In addition, your pipeline instance in JupyterLab must contain a runtime configuration. If you create a workbench as part of a data science project, a default runtime configuration is created automatically. However, if you create a notebook from the Jupyter tile in the Open Data Hub dashboard, you must create a runtime configuration before you can export your pipeline in JupyterLab. A runtime configuration defines connectivity information for your pipeline instance and S3-compatible cloud storage.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the OpenShift Pipelines operator.</p>\n</li>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>You have access to S3-compatible storage.</p>\n</li>\n<li>\n<p>You have a created a pipeline in JupyterLab.</p>\n</li>\n<li>\n<p>You have opened your pipeline in the Pipeline Editor in JupyterLab.</p>\n</li>\n<li>\n<p>Your pipeline instance contains a runtime configuration.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, or  PyTorch).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the Pipeline Editor user interface, click <strong>Export Pipeline</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-export-pipeline-button.png\" alt=\"Export pipeline\"></span>).</p>\n<div class=\"paragraph\">\n<p>The <strong>Export Pipeline</strong> dialog appears. The <strong>Pipeline Name</strong> field is automatically populated with the pipeline file name.</p>\n</div>\n</li>\n<li>\n<p>Define the settings to export your pipeline.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>From the <strong>Runtime Configuration</strong> list, select the relevant runtime configuration to export your pipeline.</p>\n</li>\n<li>\n<p>From the <strong>Export Pipeline as</strong> select an appropriate file format</p>\n</li>\n<li>\n<p>In the <strong>Export Filename</strong> field, enter a file name for the exported pipeline.</p>\n</li>\n<li>\n<p>Select the <strong>Replace if file already exists</strong> check box to replace an existing file of the same name as the pipeline you are exporting.</p>\n</li>\n<li>\n<p>Optional: Configure your pipeline parameters, if applicable. If your pipeline contains nodes that reference pipeline parameters, you can change the default parameter values. If a parameter is required and has no default value, you must enter a value.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>OK</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>You can view the file containing the pipeline that you exported in your designated object storage bucket.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2 _additional-resources\">\n<h3 id=\"_additional_resources\">Additional resources</h3>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><a href=\"https://www.kubeflow.org/docs/components/pipelines/v1/\">Kubeflow Pipelines v1 Documentation</a>\n<a href=\"https://opendatahub.io/docshtml/working_on_data_science_projects/working-with-data-science-pipelines_ds-pipelines#working-with-pipelines-in-jupyterlab\">Working with pipelines in JupyterLab</a>.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"model-serving-on-openshift-data-science_model-serving\">Model serving on Open Data Hub</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>As a data scientist, you can deploy your trained machine-learning models to serve intelligent applications in production. After you have deployed your model, applications can send requests to the model using its deployed API endpoint.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_configuring_model_servers_2\">Configuring model servers</h3>\n<div class=\"sect3\">\n<h4 id=\"configuring-a-model-server-for-your-data-science-project_model-serving\">Configuring a model server for your data science project</h4>\n<div class=\"paragraph _abstract\">\n<p>Before you can successfully deploy a data science model on Open Data Hub, you must configure a model server. This includes configuring the number of replicas being deployed, the server size, the token authorization, and how the project is accessed.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that you can add a model server to.</p>\n</li>\n<li>\n<p>If you want to use a custom model-serving runtime for your model server, you have added and enabled the runtime. See <a href=\"https://opendatahub.io/docs/working-on-data-science-projects/#adding-a-custom-model-serving-runtime_nb-server\">Adding a custom model-serving runtime</a>.</p>\n</li>\n<li>\n<p>If you want to use graphics processing units (GPUs) with your model server, you have enabled GPU support. This includes installing the Node Feature Discovery and GPU Operators. For more information, see <a href=\"https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/openshift/contents.html\">NVIDIA GPU Operator on OpenShift</a> in the NVIDIA documentation.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to configure a model server for.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>In the <strong>Models and model servers</strong> section, click <strong>Add server</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Add model server</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>In the <strong>Model sever name</strong> field, enter a unique name for the model server.</p>\n</li>\n<li>\n<p>From the <strong>Serving runtime</strong> list, select a model-serving runtime that is installed and enabled in your Open Data Hub deployment.</p>\n</li>\n<li>\n<p>In the <strong>Number of model replicas to deploy</strong> field, specify a value.</p>\n</li>\n<li>\n<p>From the <strong>Model server size</strong> list, select one of the following server sizes:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Small</p>\n</li>\n<li>\n<p>Medium</p>\n</li>\n<li>\n<p>Large</p>\n</li>\n<li>\n<p>Custom</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Optional: If you selected <strong>Custom</strong> in the preceding step, configure the following settings in the <strong>Model server size</strong> section to customize your model server:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>CPUs requested</strong> field, specify a number of CPUs to use with your model server. Use the list beside this field to specify the value in cores or millicores.</p>\n</li>\n<li>\n<p>In the <strong>CPU limit</strong> field, specify the maximum number of CPUs to use with your model server. Use the list beside this field to specify the value in cores or millicores.</p>\n</li>\n<li>\n<p>In the <strong>Memory requested</strong> field, specify the requested memory for the model server in gibibytes (Gi).</p>\n</li>\n<li>\n<p>In the <strong>Memory limit</strong> field, specify the maximum memory limit for the model server in gibibytes (Gi).</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Optional: In the <strong>Model server GPUs</strong> field, specify a number of GPUs to use with your model server.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Open Data Hub includes two versions of the OpenVINO Model Server (OVMS) runtime by default; a version that supports GPUs and one that does not. To use GPUs, from the <strong>Serving runtime</strong> list, you must select the version whose display name includes <code>Supports GPUs</code>.</p>\n</div>\n<div class=\"paragraph\">\n<p>If you are using a <em>custom</em> model-serving runtime with your model server, you must ensure that your custom runtime supports GPUs and is appropriately configured to use them.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Optional: In the <strong>Model route</strong> section, select the <strong>Make deployed models available through an external route</strong> check box to make your deployed models available to external clients.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Token authorization</strong> section, select the <strong>Require token authentication</strong> check box to require token authentication for your model server. To finish configuring token authentication, perform the following actions:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Service account name</strong> field, enter a service account name for which the token will be generated. The generated token is created and displayed in the <strong>Token secret</strong> field when the model server is configured.</p>\n</li>\n<li>\n<p>To add an additional service account, click <strong>Add a service account</strong> and enter another service account name.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Add</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The model server that you configured is displayed in the <strong>Models and model servers</strong> section of the project details page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"adding-a-custom-model-serving-runtime_model-serving\">Adding a custom model-serving runtime</h4>\n<div class=\"paragraph\">\n<p>A model-serving runtime provides integration with a specified model server and the model frameworks that it supports. By default, Open Data Hub includes the OpenVINO Model Server runtime. However, if this runtime doesn&#8217;t meet your needs (it doesn&#8217;t support a particular model framework, for example), you might want to add your own, custom runtimes.</p>\n</div>\n<div class=\"paragraph\">\n<p>As an administrator, you can use the Open Data Hub interface to add and enable custom model-serving runtimes. You can then choose from your enabled runtimes when you create a new model server.</p>\n</div>\n<div class=\"ulist _abstract\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub as an administrator.</p>\n</li>\n<li>\n<p>You are familiar with how to <a href=\"https://opendatahub.io/docs/working-on-data-science-projects/#configuring-a-model-server-for-your-data-science-project_nb-server\">configure a model server for your project</a>. When you have added a custom model-serving runtime, you must configure a new model server to use the runtime.</p>\n</li>\n<li>\n<p>You have reviewed the example runtimes in the <a href=\"https://github.com/kserve/modelmesh-serving/tree/main/config/runtimes\" target=\"_blank\" rel=\"noopener\">kserve/modelmesh-serving</a> repository. You can use these examples as <em>starting points</em>. However, each runtime requires some further modification before you can deploy it in Open Data Hub. The required modifications are described in the following procedure.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nOpen Data Hub includes the OpenVINO Model Server model-serving runtime by default. You do not need to add this runtime to Open Data Hub.\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Settings</strong> &gt; <strong>Serving runtimes</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Serving runtimes</strong> page opens and shows the model-serving runtimes that are already installed and enabled in your Open Data Hub deployment. By default, the OpenVINO Model Server runtime is pre-installed and enabled in Open Data Hub.</p>\n</div>\n</li>\n<li>\n<p>To add a new, custom runtime, click <strong>Add serving runtime</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Add serving runtime</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>To start adding a new runtime, perform one of the following sets of actions:</p>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>To upload a YAML file</strong></p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click <strong>Upload files</strong>.</p>\n<div class=\"paragraph\">\n<p>A file browser opens.</p>\n</div>\n</li>\n<li>\n<p>In the file browser, select a YAML file on your computer. This file might be the one of the example runtimes that you downloaded from the <a href=\"https://github.com/kserve/modelmesh-serving/tree/main/config/runtimes\" target=\"_blank\" rel=\"noopener\">kserve/modelmesh-serving</a> repository.</p>\n<div class=\"paragraph\">\n<p>The embedded YAML editor opens and shows the contents of the file that you uploaded.</p>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p><strong>To enter YAML code directly in the editor</strong></p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click <strong>Start from scratch</strong>.</p>\n<div class=\"paragraph\">\n<p>The embedded YAML editor opens with no content.</p>\n</div>\n</li>\n<li>\n<p>Enter or paste YAML code directly in the embedded editor. The YAML that you paste might be copied from one of the example runtimes in the <a href=\"https://github.com/kserve/modelmesh-serving/tree/main/config/runtimes\" target=\"_blank\" rel=\"noopener\">kserve/modelmesh-serving</a> repository.</p>\n</li>\n</ol>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</li>\n<li>\n<p>Optional: If you are adding one of the example runtimes in the <a href=\"https://github.com/kserve/modelmesh-serving/tree/main/config/runtimes\" target=\"_blank\" rel=\"noopener\">kserve/modelmesh-serving</a> repository, perform the following modifications:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the YAML editor, locate the <code>kind</code> field for your runtime. Update the value of this field to <code>ServingRuntime</code>.</p>\n</li>\n<li>\n<p>In the YAML editor, locate the <code>containers.image</code> field for your runtime. Based on the runtime that you are adding, update the field to one of the following:</p>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">Nvidia Triton Inference Server</dt>\n<dd>\n<div class=\"paragraph\">\n<p><code>image: nvcr.io/nvidia/tritonserver:21.06.1-py3</code></p>\n</div>\n</dd>\n<dt class=\"hdlist1\">Seldon Python MLServer</dt>\n<dd>\n<div class=\"paragraph\">\n<p><code>image: seldonio/mlserver:0.5.2</code></p>\n</div>\n</dd>\n<dt class=\"hdlist1\">TorchServe</dt>\n<dd>\n<div class=\"paragraph\">\n<p><code>image: pytorch/torchserve:0.6.0-cpu</code></p>\n</div>\n</dd>\n</dl>\n</div>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>In the <code>metadata.name</code> field, ensure that the value of the runtime you are adding is unique (that is, the value isn&#8217;t the same as for a runtime you have already added).</p>\n</li>\n<li>\n<p>Optional: To configure a custom display name for the runtime that you are adding, add a <code>metadata.annotations.openshift.io/display-name</code> field and specify a value, as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: serving.kserve.io/v1alpha1\nkind: ServingRuntime\nmetadata:\n  name: mlserver-0.x\n  annotations:\n    openshift.io/display-name: MLServer</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nIf you do not configure a custom display name for your runtime, Open Data Hub shows the value of the <code>metadata.name</code> field.\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Click <strong>Add</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Serving runtimes</strong> page opens and shows the updated list of runtimes that are installed. Observe that the runtime you added is automatically enabled.</p>\n</div>\n</li>\n<li>\n<p>Optional: To edit your custom runtime, click the action menu (&#8942;) and select <strong>Edit</strong>.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nYou cannot directly edit the OpenVINO Model Server runtime that is included in Open Data Hub by default. However, you can <em>clone</em> this runtime and edit the cloned version. You can then add the edited clone as a new, custom runtime. To do this, click the action menu beside the OpenVINO Model Server and select <strong>Clone</strong>.\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The model-serving runtime you added is shown in an enabled state on the <strong>Serving runtimes</strong> page.</p>\n</li>\n</ul>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p>To learn how to configure a model server that uses a custom model-serving runtime that you have added, see <a href=\"https://opendatahub.io/docs/working-on-data-science-projects/#configuring-a-model-server-for-your-data-science-project_nb-server\">Configuring a model server for your data science project</a>.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"updating-a-model-server_model-serving\">Updating a model server</h4>\n<div class=\"paragraph _abstract\">\n<p>You can update your data science project&#8217;s model server by changing details, such as the number of deployed replicas, the server size, the token authorization, and how the project is accessed.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that has a model server assigned.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project whose model server details you want to update.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>In the <strong>Models and model servers</strong> section, locate the model server you want to update. Click the action menu (<strong>&#8942;</strong>)   and select <strong>Edit model server</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Configure model server</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Update the model server properties, as follows:</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nYou cannot change the <strong>Serving runtime</strong> selection for a model server that is already configured. This protects against changing to a runtime that does not support already-deployed models.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Model sever name</strong> field, enter a new, unique name for the model server.</p>\n</li>\n<li>\n<p>In the <strong>Number of model replicas to deploy</strong> field, specify a value.</p>\n</li>\n<li>\n<p>From the <strong>Model server size</strong> list, select one of the following server sizes:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Small</p>\n</li>\n<li>\n<p>Medium</p>\n</li>\n<li>\n<p>Large</p>\n</li>\n<li>\n<p>Custom</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Optional: If you selected <strong>Custom</strong> in the preceding step, configure the following settings in the <strong>Model server size</strong> section to customize your model server:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>CPUs requested</strong> field, specify a number of CPUs to use with your model server. Use the list beside this field to specify the value in cores or millicores.</p>\n</li>\n<li>\n<p>In the <strong>CPU limit</strong> field, specify the maximum number of CPUs to use with your model server. Use the list beside this field to specify the value in cores or millicores.</p>\n</li>\n<li>\n<p>In the <strong>Memory requested</strong> field, specify the requested memory for the model server in gibibytes (Gi).</p>\n</li>\n<li>\n<p>In the <strong>Memory limit</strong> field, specify the maximum memory limit for the model server in gibibytes (Gi).</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Optional: In the <strong>Model server GPUs</strong> field, specify a number of GPUs to use with your model server.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Open Data Hub includes two versions of the OpenVINO Model Server (OVMS) runtime by default; a version that supports GPUs and one that does not. To use GPUs, from the <strong>Serving runtime</strong> list, you must select the version whose display name includes <code>Supports GPUs</code>.</p>\n</div>\n<div class=\"paragraph\">\n<p>If you are using a <em>custom</em> model-serving runtime with your model server, you must ensure that your custom runtime supports GPUs and is appropriately configured to use them.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Optional: In the <strong>Model route</strong> section, select the <strong>Make deployed models available through an external route</strong> check box to make your deployed models available to external clients.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Token authorization</strong> section, select the <strong>Require token authentication</strong> check box to require token authentication for your model server. To finish configuring token authentication, perform the following actions:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Service account name</strong> field, enter a service account name for which the token will be generated. The generated token is created and displayed in the <strong>Token secret</strong> field when the model server is configured.</p>\n</li>\n<li>\n<p>To add an additional service account, click <strong>Add a service account</strong> and enter another service account name.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Configure</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The model server that you updated is displayed in the <strong>Models and model servers</strong> section on the project details page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deleting-a-model-server_model-serving\">Deleting a model server</h4>\n<div class=\"paragraph _abstract\">\n<p>When you no longer need a model server to host models, you can remove it from your data science project.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nWhen you remove a model server, you also remove the models that are hosted on that model server. As a result, the models are no longer available to applications.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have created a data science project and an associated model server.</p>\n</li>\n<li>\n<p>You have notified the users of the applications that access the models that the models will no longer be available.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project from which you want to delete the model server.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the project whose model server you want to delete in the <strong>Models and model servers</strong> section and then click <strong>Delete model server</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Delete model server</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter the name of the model server in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete model server</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The model server that you deleted is no longer displayed in the <strong>Models and model servers</strong> section on the project details page.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_working_with_deployed_models\">Working with deployed models</h3>\n<div class=\"sect3\">\n<h4 id=\"deploying-a-model_model-serving\">Deploying a model in Open Data Hub</h4>\n<div class=\"paragraph _abstract\">\n<p>You can deploy trained models on Open Data Hub to enable you to test and implement them into intelligent applications. Deploying a model makes it available as a service that you can access using an API. This enables you to return predictions based on data inputs.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that contains an associated model server.</p>\n</li>\n<li>\n<p>You know the folder path for the data connection that you want the model to access.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {product-short} dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data science projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project containing the model that you want to deploy.</p>\n<div class=\"paragraph\">\n<p>A project details opens.</p>\n</div>\n</li>\n<li>\n<p>In the <strong>Models and model servers</strong> section, next to the name of your model server, click <strong>Deploy model</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Deploy model</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Configure properties for deploying your model as follows:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Model Name</strong> field, enter a unique name for the model that you are deploying.</p>\n</li>\n<li>\n<p>From the <strong>Model framework</strong> list, select a framework for your model.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nThe <strong>Model framework</strong> list shows only the frameworks that are supported by the model-serving runtime that you specified when you configured your model server.\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>To specify the location of your model, perform one of the following sets of actions:</p>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>To use an existing data connection</strong></p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>Select <strong>Existing data connection</strong>.</p>\n</li>\n<li>\n<p>From the <strong>Name</strong> list, select a data connection that you previously defined.</p>\n</li>\n<li>\n<p>In the <strong>Folder path</strong> field, enter the folder path that contains the model in your specified data source.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p><strong>To use a new data connection</strong></p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>To define a new data connection that your model can access, select <strong>New data connection</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Name</strong> field, enter a unique name for the data connection.</p>\n</li>\n<li>\n<p>In the <strong>AWS_ACCESS_KEY_ID</strong> field, enter your access key ID for Amazon Web Services (AWS).</p>\n</li>\n<li>\n<p>In the <strong>AWS_SECRET_ACCESS_KEY</strong> field, enter your secret access key for the AWS account you specified.</p>\n</li>\n<li>\n<p>In the <strong>AWS_S3_ENDPOINT</strong> field, enter the endpoint of your AWS S3 storage.</p>\n</li>\n<li>\n<p>In the <strong>AWS_DEFAULT_REGION</strong> field, enter the default region of your AWS account.</p>\n</li>\n<li>\n<p>In the <strong>AWS_S3_BUCKET</strong> field, enter the name of the AWS S3 bucket.</p>\n</li>\n<li>\n<p>In the <strong>Folder path</strong> field, enter the folder path in your AWS S3 bucket that contains your data file.</p>\n</li>\n</ol>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</li>\n<li>\n<p>Click <strong>Deploy</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The model you deployed is displayed on the <strong>Model Serving</strong> page of the dashboard.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"viewing-a-deployed-model_model-serving\">Viewing a deployed model</h4>\n<div class=\"paragraph _abstract\">\n<p>To analyse the results of your work, you can view a list of deployed models on Open Data Hub. You can also view the current statuses of deployed models and their endpoints.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code>) in OpenShift.</p>\n</li>\n<li>\n<p>There are active and deployed data science models available on the <strong>Model Serving</strong> page.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Model Serving</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Model Serving</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Review the list of deployed models.</p>\n<div class=\"paragraph\">\n<p>Inference endpoints are displayed in the <strong>Inference endpoint</strong> column in the <strong>Deployed models</strong> table.</p>\n</div>\n</li>\n<li>\n<p>Optional: Click the <strong>Copy</strong> button (<span class=\"image\"><img src=\"/static/docs/images/osd-copy.png\" alt=\"osd copy\"></span>) on the relevant row to copy the model&#8217;s inference endpoint to the clipboard.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>A list of previously deployed data science models is displayed on the <strong>Model Serving</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"updating-the-deployment-properties-of-a-deployed-model_model-serving\">Updating the deployment properties of a deployed model</h4>\n<div class=\"paragraph _abstract\">\n<p>You can update the deployment properties of a model that has been deployed previously. This allows you to change the model&#8217;s data connection and name.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have deployed a model on Open Data Hub.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Model serving</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Model Serving</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the model whose deployment properties you want to update and click <strong>Edit</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Deploy model</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Update the deployment properties of the model as follows:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Model Name</strong> field, enter a new, unique name for the model.</p>\n</li>\n<li>\n<p>From the <strong>Model framework</strong> list, select a framework for your model.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nThe <strong>Model framework</strong> list shows only the frameworks that are supported by the model-serving runtime that you specified when you configured your model server.\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>To update how you have specified the location of your model, perform one of the following sets of actions:</p>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>If you previously specified an existing data connection</strong></p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Folder path</strong> field, update the folder path that contains the model in your specified data source.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p><strong>If you previously specified a new data connection</strong></p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Name</strong> field, update the name of the data connection.</p>\n</li>\n<li>\n<p>In the <strong>AWS_ACCESS_KEY_ID</strong> field, update your access key ID for Amazon Web Services (AWS).</p>\n</li>\n<li>\n<p>In the <strong>AWS_SECRET_ACCESS_KEY</strong> field, update your secret access key for the AWS account you specified.</p>\n</li>\n<li>\n<p>In the <strong>AWS_S3_ENDPOINT</strong> field, update the endpoint of your AWS S3 storage.</p>\n</li>\n<li>\n<p>In the <strong>AWS_DEFAULT_REGION</strong> field, update the default region of your AWS account.</p>\n</li>\n<li>\n<p>In the <strong>AWS_S3_BUCKET</strong> field, update the name of the AWS S3 bucket.</p>\n</li>\n<li>\n<p>In the <strong>Folder path</strong> field, update the folder path in your AWS S3 bucket that contains your data file.</p>\n</li>\n</ol>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</li>\n<li>\n<p>Click <strong>Configure</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The model whose deployment properties you updated is displayed on the <strong>Model Serving</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deleting-a-deployed-model_model-serving\">Deleting a deployed model</h4>\n<div class=\"paragraph _abstract\">\n<p>You can delete models you have previously deployed. This enables you to remove deployed models that are no longer required.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have deployed a model.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Model serving</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Model Serving</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the deployed model that you want to delete and click <strong>Delete</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Delete deployed model</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter the name of the deployed model in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete deployed model</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The model that you deleted is no longer displayed on the <strong>Model Serving</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"troubleshooting-common-problems-in-jupyter-for-administrators_model-serving\">Troubleshooting common problems in Jupyter for administrators</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>If your users are experiencing errors in Open Data Hub relating to Jupyter, their notebooks, or their notebook server, read this section to understand what could be causing the problem, and how to resolve the problem.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter\">A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter</h3>\n<div class=\"paragraph\">\n<div class=\"title\">Problem</div>\n<p>If you have configured specialized Open Data Hub user groups, the user name might not be added to the default user group for Open Data Hub.</p>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Diagnosis</div>\n<p>Check whether the user is part of the default user group.</p>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>Find the names of groups allowed access to Jupyter.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Log in to Open Data Hub web console.</p>\n</li>\n<li>\n<p>Click <strong>User Management</strong> &#8594; <strong>Groups</strong>.</p>\n</li>\n<li>\n<p>Click the name of your user group, for example, <code>rhods-users</code>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Group details</strong> page for that group appears.</p>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click the <strong>Details</strong> tab for the group and confirm that the <strong>Users</strong> section for the relevant group, contains the users who have permission to access Jupyter.</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Resolution</div>\n<p>If the user is not added to any of the groups allowed access to Jupyter, add them.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_a_users_notebook_server_does_not_start\">A user&#8217;s notebook server does not start</h3>\n<div class=\"paragraph\">\n<div class=\"title\">Problem</div>\n<p>The OpenShift Container Platform cluster that hosts the user&#8217;s notebook server might not have access to enough resources, or the Jupyter pod may have failed.</p>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Diagnosis</div>\n<ol class=\"arabic\">\n<li>\n<p>Log in to Open Data Hub web console.</p>\n</li>\n<li>\n<p>Delete and restart the notebook server pod for this user.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click <strong>Workloads</strong> &#8594; <strong>Pods</strong> and set the <strong>Project</strong> to <code>rhods-notebooks</code>.</p>\n</li>\n<li>\n<p>Search for the notebook server pod that belongs to this user, for example, <code>jupyter-nb-&lt;username&gt;-*</code>.</p>\n<div class=\"paragraph\">\n<p>If the notebook server pod exists, an intermittent failure may have occurred in the notebook server pod.</p>\n</div>\n<div class=\"paragraph\">\n<p>If the notebook server pod for the user does not exist, continue with diagnosis.</p>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Check the resources currently available in the OpenShift cluster against the resources required by the selected notebook server image.</p>\n<div class=\"paragraph\">\n<p>If worker nodes with sufficient CPU and RAM are available for scheduling in the cluster, continue with diagnosis.</p>\n</div>\n</li>\n<li>\n<p>Check the state of the Jupyter pod.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Resolution</div>\n<ul>\n<li>\n<p>If there was an intermittent failure of the notebook server pod:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Delete the notebook server pod that belongs to the user.</p>\n</li>\n<li>\n<p>Ask the user to start their notebook server again.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>If the notebook server does not have sufficient resources to run the selected notebook server image, either add more resources to the OpenShift cluster, or choose a smaller image size.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells\">The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells</h3>\n<div class=\"paragraph\">\n<div class=\"title\">Problem</div>\n<p>The user might have run out of storage space on their notebook server.</p>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Diagnosis</div>\n<ol class=\"arabic\">\n<li>\n<p>Log in to Jupyter and start the notebook server that belongs to the user having problems. If the notebook server does not start, follow these steps to check whether the user has run out of storage space:</p>\n</li>\n<li>\n<p>Log in to Open Data Hub web console.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click <strong>Workloads</strong> &#8594; <strong>Pods</strong> and set the <strong>Project</strong> to <code>rhods-notebooks</code>.</p>\n</li>\n<li>\n<p>Click the notebook server pod that belongs to this user, for example, <code>jupyter-nb-&lt;idp&gt;-&lt;username&gt;-*</code>.</p>\n</li>\n<li>\n<p>Click <strong>Logs</strong>. The user has exceeded their available capacity if you see lines similar to the following:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>Unexpected error while saving file: XXXX database or disk is full</pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Resolution</div>\n<ul>\n<li>\n<p>Increase the user&#8217;s available storage by expanding their persistent volume.</p>\n</li>\n<li>\n<p>Work with the user to identify files that can be deleted from the <code>/opt/app-root/src</code> directory on their notebook server to free up their existing storage space.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"troubleshooting-common-problems-in-jupyter-for-users_model-serving\">Troubleshooting common problems in Jupyter for users</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>If you are seeing errors in Open Data Hub related to Jupyter, your notebooks, or your notebook server, read this section to understand what could be causing the problem.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_i_see_a_403_forbidden_error_when_i_log_in_to_jupyter\">I see a <strong>403: Forbidden</strong> error when I log in to Jupyter</h3>\n<div class=\"paragraph\">\n<div class=\"title\">Problem</div>\n<p>If your administrator has configured specialized Open Data Hub user groups, your user name might not be added to the default user group or the default administrator group for Open Data Hub.</p>\n</div>\n<div class=\"literalblock\">\n<div class=\"title\">Resolution</div>\n<div class=\"content\">\n<pre>Contact your administrator so that they can add you to the correct group/s.</pre>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_my_notebook_server_does_not_start\">My notebook server does not start</h3>\n<div class=\"paragraph\">\n<div class=\"title\">Problem</div>\n<p>The OpenShift Container Platform cluster that hosts your notebook server might not have access to enough resources, or the Jupyter pod may have failed.</p>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Resolution</div>\n<p>Check the logs in the <strong>Events</strong> section in OpenShift for error messages associated with the problem. For example:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>Server requested\n2021-10-28T13:31:29.830991Z [Warning] 0/7 nodes are available: 2 Insufficient memory,\n2 node(s) had taint {node-role.kubernetes.io/infra: }, that the pod didn't tolerate, 3 node(s) had taint {node-role.kubernetes.io/master: },\nthat the pod didn't tolerate.</pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Contact your administrator with details of any relevant error messages so that they can perform further checks.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_i_see_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_i_run_my_notebook_cells\">I see a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when I run my notebook cells</h3>\n<div class=\"paragraph\">\n<div class=\"title\">Problem</div>\n<p>You might have run out of storage space on your notebook server.</p>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Resolution</div>\n<p>Contact your administrator so that they can perform further checks.</p>\n</div>\n</div>\n</div>\n</div>","id":"47af8a52-b33d-554d-995f-8b56bac3948f","document":{"title":"Working on data science projects"}},"markdownRemark":null},"pageContext":{"id":"47af8a52-b33d-554d-995f-8b56bac3948f"}},"staticQueryHashes":["2604506565"],"slicesMap":{}}