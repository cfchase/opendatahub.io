{"componentChunkName":"component---src-templates-docs-page-tsx","path":"/docs/working-on-data-science-projects/","result":{"data":{"allFile":{"edges":[{"node":{"childAsciidoc":{"fields":{"slug":"/docs/README/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/getting-started-with-open-data-hub/"},"sections":[{"parentId":null,"name":"Logging in to Open Data Hub","level":1,"index":0,"id":"logging-in_get-started"},{"parentId":null,"name":"The Open Data Hub user interface","level":1,"index":1,"id":"user-interface_get-started"},{"parentId":"user-interface_get-started","name":"Global navigation","level":2,"index":0,"id":"_global_navigation"},{"parentId":"user-interface_get-started","name":"Side navigation","level":2,"index":1,"id":"_side_navigation"},{"parentId":null,"name":"Notifications in Open Data Hub","level":1,"index":2,"id":"notifications_get-started"},{"parentId":null,"name":"Creating a data science project","level":1,"index":3,"id":"creating-a-data-science-project_get-started"},{"parentId":null,"name":"Creating a project workbench","level":1,"index":4,"id":"creating-a-project-workbench_get-started"},{"parentId":"creating-a-project-workbench_get-started","name":"Launching Jupyter and starting a notebook server","level":2,"index":0,"id":"launching-jupyter-and-starting-a-notebook-server_get-started"},{"parentId":"creating-a-project-workbench_get-started","name":"Options for notebook server environments","level":2,"index":1,"id":"options-for-notebook-server-environments_get-started"},{"parentId":null,"name":"Tutorials for data scientists","level":1,"index":5,"id":"tutorials-for-data-scientists_get-started"},{"parentId":"tutorials-for-data-scientists_get-started","name":"Accessing tutorials","level":2,"index":0,"id":"accessing-tutorials_get-started"},{"parentId":null,"name":"Configuring your IDE","level":1,"index":6,"id":"configuring-your-ide_get-started"},{"parentId":"configuring-your-ide_get-started","name":"Configuring your code-server workbench","level":2,"index":0,"id":"_configuring_your_code_server_workbench"},{"parentId":"_configuring_your_code_server_workbench","name":"Installing extensions with code-server","level":3,"index":0,"id":"_installing_extensions_with_code_server"},{"parentId":"_installing_extensions_with_code_server","name":"Extensions","level":4,"index":0,"id":"_extensions"},{"parentId":null,"name":"Enabling services connected to Open Data Hub","level":1,"index":7,"id":"enabling-services_get-started"},{"parentId":null,"name":"Disabling applications connected to Open Data Hub","level":1,"index":8,"id":"disabling-applications-connected_get-started"},{"parentId":"disabling-applications-connected_get-started","name":"Removing disabled applications from Open Data Hub","level":2,"index":0,"id":"removing-disabled-applications_get-started"},{"parentId":null,"name":"Support requirements and limitations","level":1,"index":9,"id":"support-requirements-and-limitations_requirements"},{"parentId":"support-requirements-and-limitations_requirements","name":"Supported browsers","level":2,"index":0,"id":"supported-browsers_requirements"},{"parentId":"support-requirements-and-limitations_requirements","name":"Supported services","level":2,"index":1,"id":"supported-services_requirements"},{"parentId":"support-requirements-and-limitations_requirements","name":"Supported packages","level":2,"index":2,"id":"supported-packages_requirements"},{"parentId":null,"name":"Common questions","level":1,"index":10,"id":"common-questions_get-started"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/installing-open-data-hub/"},"sections":[{"parentId":null,"name":"Installing Open Data Hub version 2","level":1,"index":0,"id":"installing-odh-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing the Open Data Hub Operator version 2","level":2,"index":0,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing Open Data Hub components","level":2,"index":1,"id":"installing-odh-components_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Accessing the Open Data Hub dashboard","level":2,"index":2,"id":"accessing-the-odh-dashboard_installv2"},{"parentId":null,"name":"Installing Open Data Hub version 1","level":1,"index":1,"id":"installing-odh-v1_installv1"},{"parentId":"installing-odh-v1_installv1","name":"Installing the Open Data Hub Operator version 1","level":2,"index":0,"id":"installing-the-odh-operator-v1_installv1"},{"parentId":"installing-odh-v1_installv1","name":"Creating a new project for your Open Data Hub instance","level":2,"index":1,"id":"creating-a-new-project-for-your-odh-instance_installv1"},{"parentId":"installing-odh-v1_installv1","name":"Adding an Open Data Hub instance","level":2,"index":2,"id":"adding-an-odh-instance_installv1"},{"parentId":"installing-odh-v1_installv1","name":"Accessing the Open Data Hub dashboard","level":2,"index":3,"id":"accessing-the-odh-dashboard_installv1"},{"parentId":null,"name":"Working with certificates","level":1,"index":2,"id":"working-with-certificates_certs"},{"parentId":"working-with-certificates_certs","name":"Understanding certificates in Open Data Hub","level":2,"index":0,"id":"understanding-certificates_certs"},{"parentId":"understanding-certificates_certs","name":"How CA bundles are injected","level":3,"index":0,"id":"_how_ca_bundles_are_injected"},{"parentId":"understanding-certificates_certs","name":"How the ConfigMap is managed","level":3,"index":1,"id":"_how_the_configmap_is_managed"},{"parentId":"working-with-certificates_certs","name":"Adding a CA bundle","level":2,"index":1,"id":"adding-a-ca-bundle_certs"},{"parentId":"working-with-certificates_certs","name":"Removing a CA bundle","level":2,"index":2,"id":"removing-a-ca-bundle_certs"},{"parentId":"working-with-certificates_certs","name":"Removing a CA bundle from a namespace","level":2,"index":3,"id":"removing-a-ca-bundle-from-a-namespace_certs"},{"parentId":"working-with-certificates_certs","name":"Managing certificates","level":2,"index":4,"id":"managing-certificates_certs"},{"parentId":"working-with-certificates_certs","name":"Using self-signed certificates with Open Data Hub components","level":2,"index":5,"id":"_using_self_signed_certificates_with_open_data_hub_components"},{"parentId":"_using_self_signed_certificates_with_open_data_hub_components","name":"Using certificates with data science pipelines","level":3,"index":0,"id":"using-certificates-with-data-science-pipelines_certs"},{"parentId":"using-certificates-with-data-science-pipelines_certs","name":"Providing a CA bundle only for data science pipelines","level":4,"index":0,"id":"_providing_a_ca_bundle_only_for_data_science_pipelines"},{"parentId":"_using_self_signed_certificates_with_open_data_hub_components","name":"Using certificates with workbenches","level":3,"index":1,"id":"using-certificates-with-workbenches_certs"},{"parentId":"using-certificates-with-workbenches_certs","name":"Creating data science pipelines with Elyra and self-signed certificates","level":4,"index":0,"id":"_creating_data_science_pipelines_with_elyra_and_self_signed_certificates"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/managing-resources/"},"sections":[{"parentId":null,"name":"Customizing the dashboard","level":1,"index":0,"id":"customizing-the-dashboard"},{"parentId":"customizing-the-dashboard","name":"Editing the dashboard configuration file","level":2,"index":0,"id":"editing-the-dashboard-configuration-file_dashboard"},{"parentId":"customizing-the-dashboard","name":"Dashboard configuration options","level":2,"index":1,"id":"ref-dashboard-configuration-options_dashboard"},{"parentId":null,"name":"Managing applications that show in the dashboard","level":1,"index":1,"id":"managing-applications-that-show-in-the-dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Adding an application to the dashboard","level":2,"index":0,"id":"adding-an-application-to-the-dashboard_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Preventing users from adding applications to the dashboard","level":2,"index":1,"id":"preventing-users-from-adding-applications-to-the-dashboard_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Showing or hiding information about enabled applications","level":2,"index":2,"id":"showing-hiding-information-about-enabled-applications_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Hiding the default Jupyter application","level":2,"index":3,"id":"hiding-the-default-jupyter-application_dashboard"},{"parentId":null,"name":"Managing cluster resources","level":1,"index":2,"id":"managing-cluster-resources"},{"parentId":"managing-cluster-resources","name":"Configuring the default PVC size for your cluster","level":2,"index":0,"id":"configuring-the-default-pvc-size-for-your-cluster_managing-resources"},{"parentId":"managing-cluster-resources","name":"Restoring the default PVC size for your cluster","level":2,"index":1,"id":"restoring-the-default-pvc-size-for-your-cluster_managing-resources"},{"parentId":"managing-cluster-resources","name":"Overview of accelerators","level":2,"index":2,"id":"overview-of-accelerators_managing-resources"},{"parentId":"overview-of-accelerators_managing-resources","name":"Enabling GPU support in Open Data Hub","level":3,"index":0,"id":"enabling-gpu-support_managing-resources"},{"parentId":"overview-of-accelerators_managing-resources","name":"Enabling Habana Gaudi devices","level":3,"index":1,"id":"enabling-habana-gaudi-devices_managing-resources"},{"parentId":"managing-cluster-resources","name":"Allocating additional resources to Open Data Hub users","level":2,"index":3,"id":"allocating-additional-resources-to-data-science-users_managing-resources"},{"parentId":null,"name":"Managing Jupyter notebook servers","level":1,"index":3,"id":"managing-notebook-servers"},{"parentId":"managing-notebook-servers","name":"Accessing the Jupyter administration interface","level":2,"index":0,"id":"accessing-the-jupyter-administration-interface_managing-resources"},{"parentId":"managing-notebook-servers","name":"Starting notebook servers owned by other users","level":2,"index":1,"id":"starting-notebook-servers-owned-by-other-users_managing-resources"},{"parentId":"managing-notebook-servers","name":"Accessing notebook servers owned by other users","level":2,"index":2,"id":"accessing-notebook-servers-owned-by-other-users_managing-resources"},{"parentId":"managing-notebook-servers","name":"Stopping notebook servers owned by other users","level":2,"index":3,"id":"stopping-notebook-servers-owned-by-other-users_managing-resources"},{"parentId":"managing-notebook-servers","name":"Stopping idle notebooks","level":2,"index":4,"id":"stopping-idle-notebooks_managing-resources"},{"parentId":"managing-notebook-servers","name":"Adding notebook pod tolerations","level":2,"index":5,"id":"adding-notebook-pod-tolerations_managing-resources"},{"parentId":"managing-notebook-servers","name":"Configuring a custom notebook image","level":2,"index":6,"id":"configuring-a-custom-notebook-image_managing-resources"},{"parentId":null,"name":"Backing up storage data","level":1,"index":4,"id":"backing-up-storage-data_managing-resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/managing-users/"},"sections":[{"parentId":null,"name":"Adding users","level":1,"index":0,"id":"adding-users"},{"parentId":"adding-users","name":"Overview of user types and permissions","level":2,"index":0,"id":"overview-of-user-types-and-permissions_managing-users"},{"parentId":"adding-users","name":"Defining Open Data Hub administrator and user groups","level":2,"index":1,"id":"defining-data-science-admin-and-user-groups_managing-users"},{"parentId":"adding-users","name":"Adding users to specialized Open Data Hub user groups","level":2,"index":2,"id":"adding-users-to-specialized-data-science-user-groups_managing-users"},{"parentId":"adding-users","name":"Viewing Open Data Hub users","level":2,"index":3,"id":"viewing-data-science-users_managing-users"},{"parentId":null,"name":"Deleting users and their resources","level":1,"index":1,"id":"deleting-users"},{"parentId":"deleting-users","name":"About deleting users and their resources","level":2,"index":0,"id":"about-deleting-users-and-resources_managing-users"},{"parentId":"deleting-users","name":"Backing up storage data","level":2,"index":1,"id":"backing-up-storage-data_managing-users"},{"parentId":"deleting-users","name":"Stopping notebook servers owned by other users","level":2,"index":2,"id":"stopping-notebook-servers-owned-by-other-users_managing-users"},{"parentId":"deleting-users","name":"Revoking user access to Jupyter","level":2,"index":3,"id":"revoking-user-access-to-jupyter_managing-users"},{"parentId":"deleting-users","name":"Cleaning up after deleting users","level":2,"index":4,"id":"cleaning-up-after-deleting-users_managing-users"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/serving-models/"},"sections":[{"parentId":null,"name":"About model serving","level":1,"index":0,"id":"about-model-serving_about-model-serving"},{"parentId":null,"name":"Serving small and medium-sized models","level":1,"index":1,"id":"serving-small-and-medium-sized-models_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Configuring model servers","level":2,"index":0,"id":"_configuring_model_servers"},{"parentId":"_configuring_model_servers","name":"Enabling the multi-model serving platform","level":3,"index":0,"id":"enabling-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a custom model-serving runtime for the multi-model serving platform","level":3,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a model server for the multi-model serving platform","level":3,"index":2,"id":"adding-a-model-server-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Deleting a model server","level":3,"index":3,"id":"deleting-a-model-server_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Working with deployed models","level":2,"index":1,"id":"_working_with_deployed_models"},{"parentId":"_working_with_deployed_models","name":"Deploying a model by using the multi-model serving platform","level":3,"index":0,"id":"deploying-a-model-using-the-multi-model-serving-platform_model-serving"},{"parentId":"_working_with_deployed_models","name":"Viewing a deployed model","level":3,"index":1,"id":"viewing-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Updating the deployment properties of a deployed model","level":3,"index":2,"id":"updating-the-deployment-properties-of-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Deleting a deployed model","level":3,"index":3,"id":"deleting-a-deployed-model_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Configuring monitoring for the multi-model serving platform","level":2,"index":2,"id":"configuring-monitoring-for-the-multi-model-serving-platform_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Viewing model-serving runtime metrics for the multi-model serving platform","level":2,"index":3,"id":"viewing-metrics-for-the-multi-model-serving-platform_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Monitoring model performance","level":2,"index":4,"id":"_monitoring_model_performance"},{"parentId":"_monitoring_model_performance","name":"Viewing performance metrics for all models on a model server","level":3,"index":0,"id":"viewing-performance-metrics-for-model-server_model-serving"},{"parentId":"_monitoring_model_performance","name":"Viewing HTTP request metrics for a deployed model","level":3,"index":1,"id":"viewing-http-request-metrics-for-a-deployed-model_model-serving"},{"parentId":null,"name":"Serving large models","level":1,"index":2,"id":"serving-large-models_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"About the single-model serving platform","level":2,"index":0,"id":"about-the-single-model-serving-platform_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Installing KServe","level":2,"index":1,"id":"installing-kserve_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Deploying models by using the single-model serving platform","level":2,"index":2,"id":"deploying-models-using-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Enabling the single-model serving platform","level":3,"index":0,"id":"enabling-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Adding a custom model-serving runtime for the single-model serving platform","level":3,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Deploying models on the single-model serving platform","level":3,"index":2,"id":"deploying-models-on-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Accessing the inference endpoint for a deployed model","level":3,"index":3,"id":"accessing-inference-endpoint-for-deployed-model_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Configuring monitoring for the single-model serving platform","level":2,"index":3,"id":"configuring-monitoring-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Viewing model-serving runtime metrics for the single-model serving platform","level":2,"index":4,"id":"viewing-metrics-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Performance tuning on the single-model serving platform","level":2,"index":5,"id":"_performance_tuning_on_the_single_model_serving_platform"},{"parentId":"_performance_tuning_on_the_single_model_serving_platform","name":"Resolving CUDA out-of-memory errors","level":3,"index":0,"id":"resolving-cuda-oom-errors-for-the-single-model-serving-platform_serving-large-models"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/monitoring-data-science-models/"},"sections":[{"parentId":null,"name":"Enabling the TrustyAI Service for a data science project","level":1,"index":0,"id":"enabling-trustyai-service_monitor"},{"parentId":"enabling-trustyai-service_monitor","name":"Enabling the TrustyAI Service by using the dashboard","level":2,"index":0,"id":"enabling-trustyai-service-using-dashboard_monitor"},{"parentId":"enabling-trustyai-service_monitor","name":"Enabling the TrustyAI Service by using the CLI","level":2,"index":1,"id":"enabling-trustyai-service-using-cli_monitor"},{"parentId":null,"name":"Authenticating the TrustyAI service","level":1,"index":1,"id":"authenticating-trustyai-service_monitor"},{"parentId":null,"name":"Sending training data to a model","level":1,"index":2,"id":"sending-training-data-to-a-model_monitor"},{"parentId":null,"name":"Configuring bias metrics for a model","level":1,"index":3,"id":"configuring-bias-metrics-for-a-model_bias-monitoring"},{"parentId":"configuring-bias-metrics-for-a-model_bias-monitoring","name":"Creating a bias metric","level":2,"index":0,"id":"creating-a-bias-metric_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the dashboard","level":3,"index":0,"id":"creating-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the CLI","level":3,"index":1,"id":"creating-a-bias-metric-using-cli_bias-monitoring"},{"parentId":"configuring-bias-metrics-for-a-model_bias-monitoring","name":"Duplicating a bias metric","level":2,"index":1,"id":"duplicating-a-bias-metric_bias-monitoring"},{"parentId":"configuring-bias-metrics-for-a-model_bias-monitoring","name":"Deleting a bias metric","level":2,"index":2,"id":"deleting-a-bias-metric_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the dashboard","level":3,"index":0,"id":"deleting-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the CLI","level":3,"index":1,"id":"deleting-a-bias-metric-using-cli_bias-monitoring"},{"parentId":null,"name":"Viewing bias metrics for a model","level":1,"index":4,"id":"viewing-bias-metrics_monitor"},{"parentId":null,"name":"Supported bias metrics","level":1,"index":5,"id":"supported-bias-metrics_monitor"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/upgrading-open-data-hub/"},"sections":[{"parentId":null,"name":"Overview of upgrading Open Data Hub","level":1,"index":0,"id":"overview-of-upgrading-odh_upgrade"},{"parentId":null,"name":"Requirements for upgrading Open Data Hub","level":1,"index":1,"id":"requirements-for-upgrading-odh_upgrade"},{"parentId":null,"name":"Upgrading Open Data Hub version 1 to version 2","level":1,"index":2,"id":"upgrading-odh-v1-to-v2_upgradev1"},{"parentId":"upgrading-odh-v1-to-v2_upgradev1","name":"Upgrading the Open Data Hub Operator version 1","level":2,"index":0,"id":"upgrading-the-odh-operator-v1_upgradev1"},{"parentId":"upgrading-odh-v1-to-v2_upgradev1","name":"Installing Open Data Hub components","level":2,"index":1,"id":"installing-odh-components_upgradev1"},{"parentId":"upgrading-odh-v1-to-v2_upgradev1","name":"Accessing the Open Data Hub dashboard","level":2,"index":2,"id":"accessing-the-odh-dashboard_upgradev1"},{"parentId":null,"name":"Upgrading Open Data Hub version 2.0 to version 2.2","level":1,"index":3,"id":"upgrading-odh-v2_upgradev2"},{"parentId":"upgrading-odh-v2_upgradev2","name":"Cleaning up resources","level":2,"index":0,"id":"cleaning-up-resources_upgradev2"},{"parentId":"upgrading-odh-v2_upgradev2","name":"Installing Open Data Hub version 2","level":2,"index":1,"id":"installing-odh-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing the Open Data Hub Operator version 2","level":3,"index":0,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing Open Data Hub components","level":3,"index":1,"id":"installing-odh-components_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Accessing the Open Data Hub dashboard","level":3,"index":2,"id":"accessing-the-odh-dashboard_installv2"},{"parentId":"upgrading-odh-v2_upgradev2","name":"Adding a CA bundle after upgrading","level":2,"index":2,"id":"adding-a-ca-bundle-after-upgrading_upgradev2"},{"parentId":null,"name":"Adding a CA bundle after upgrading","level":1,"index":4,"id":"adding-a-ca-bundle-after-upgrading_upgradev2"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-on-data-science-projects/"},"sections":[{"parentId":null,"name":"Creating and importing notebooks","level":1,"index":0,"id":"creating-and-importing-notebooks_notebooks"},{"parentId":"creating-and-importing-notebooks_notebooks","name":"Creating a new notebook","level":2,"index":0,"id":"creating-a-new-notebook_notebooks"},{"parentId":"creating-a-new-notebook_notebooks","name":"Notebook images for data scientists","level":3,"index":0,"id":"notebook-images-for-data-scientists_notebooks"},{"parentId":"creating-and-importing-notebooks_notebooks","name":"Uploading an existing notebook file from local storage","level":2,"index":1,"id":"uploading-an-existing-notebook-file-from-local-storage_notebooks"},{"parentId":"creating-and-importing-notebooks_notebooks","name":"Uploading an existing notebook file from a Git repository using JupyterLab","level":2,"index":2,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_notebooks"},{"parentId":"creating-and-importing-notebooks_notebooks","name":"Uploading an existing notebook file from a Git repository using the command line interface","level":2,"index":3,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_notebooks"},{"parentId":null,"name":"Collaborating on notebooks using Git","level":1,"index":1,"id":"collaborating-on-notebooks-using-git_git-collab"},{"parentId":"collaborating-on-notebooks-using-git_git-collab","name":"Uploading an existing notebook file from a Git repository using JupyterLab","level":2,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_git-collab"},{"parentId":"collaborating-on-notebooks-using-git_git-collab","name":"Uploading an existing notebook file from a Git repository using the command line interface","level":2,"index":1,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_git-collab"},{"parentId":"collaborating-on-notebooks-using-git_git-collab","name":"Updating your project with changes from a remote Git repository","level":2,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_git-collab"},{"parentId":"collaborating-on-notebooks-using-git_git-collab","name":"Pushing project changes to a Git repository","level":2,"index":3,"id":"pushing-project-changes-to-a-git-repository_git-collab"},{"parentId":null,"name":"Working on data science projects","level":1,"index":2,"id":"working-on-data-science-projects_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Using data science projects","level":2,"index":0,"id":"_using_data_science_projects"},{"parentId":"_using_data_science_projects","name":"Creating a data science project","level":3,"index":0,"id":"creating-a-data-science-project_nb-server"},{"parentId":"_using_data_science_projects","name":"Updating a data science project","level":3,"index":1,"id":"updating-a-data-science-project_nb-server"},{"parentId":"_using_data_science_projects","name":"Deleting a data science project","level":3,"index":2,"id":"deleting-a-data-science-project_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Using project workbenches","level":2,"index":1,"id":"_using_project_workbenches"},{"parentId":"_using_project_workbenches","name":"Creating a project workbench","level":3,"index":0,"id":"creating-a-project-workbench_nb-server"},{"parentId":"_using_project_workbenches","name":"Starting a workbench","level":3,"index":1,"id":"starting-a-workbench_nb-server"},{"parentId":"_using_project_workbenches","name":"Updating a project workbench","level":3,"index":2,"id":"updating-a-project-workbench_nb-server"},{"parentId":"_using_project_workbenches","name":"Deleting a workbench from a data science project","level":3,"index":3,"id":"deleting-a-workbench-from-a-data-science-project_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Using data connections","level":2,"index":2,"id":"_using_data_connections"},{"parentId":"_using_data_connections","name":"Adding a data connection to your data science project","level":3,"index":0,"id":"adding-a-data-connection-to-your-data-science-project_nb-server"},{"parentId":"_using_data_connections","name":"Deleting a data connection","level":3,"index":1,"id":"deleting-a-data-connection_nb-server"},{"parentId":"_using_data_connections","name":"Updating a connected data source","level":3,"index":2,"id":"updating-a-connected-data-source_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Configuring cluster storage","level":2,"index":3,"id":"_configuring_cluster_storage"},{"parentId":"_configuring_cluster_storage","name":"Adding cluster storage to your data science project","level":3,"index":0,"id":"adding-cluster-storage-to-your-data-science-project_nb-server"},{"parentId":"_configuring_cluster_storage","name":"Updating cluster storage","level":3,"index":1,"id":"updating-cluster-storage_nb-server"},{"parentId":"_configuring_cluster_storage","name":"Deleting cluster storage from a data science project","level":3,"index":2,"id":"deleting-cluster-storage-from-a-data-science-project_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Configuring data science pipelines","level":2,"index":4,"id":"_configuring_data_science_pipelines"},{"parentId":"_configuring_data_science_pipelines","name":"Configuring a pipeline server","level":3,"index":0,"id":"configuring-a-pipeline-server_nb-server"},{"parentId":"_configuring_data_science_pipelines","name":"Defining a pipeline","level":3,"index":1,"id":"defining-a-pipeline_nb-server"},{"parentId":"_configuring_data_science_pipelines","name":"Importing a data science pipeline","level":3,"index":2,"id":"importing-a-data-science-pipeline_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Configuring access to data science projects","level":2,"index":5,"id":"_configuring_access_to_data_science_projects"},{"parentId":"_configuring_access_to_data_science_projects","name":"Configuring access to data science projects","level":3,"index":0,"id":"configuring-access-to-data-science-projects_nb-server"},{"parentId":"_configuring_access_to_data_science_projects","name":"Sharing access to a data science project","level":3,"index":1,"id":"sharing-access-to-a-data-science-project_nb-server"},{"parentId":"_configuring_access_to_data_science_projects","name":"Updating access to a data science project","level":3,"index":2,"id":"updating-access-to-a-data-science-project_nb-server"},{"parentId":"_configuring_access_to_data_science_projects","name":"Removing access to a data science project","level":3,"index":3,"id":"removing-access-to-a-data-science-project_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Viewing Python packages installed on your notebook server","level":2,"index":6,"id":"viewing-python-packages-installed-on-your-notebook-server_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Installing Python packages on your notebook server","level":2,"index":7,"id":"installing-python-packages-on-your-notebook-server_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Updating notebook server settings by restarting your server","level":2,"index":8,"id":"updating-notebook-server-settings-by-restarting-your-server_nb-server"},{"parentId":null,"name":"Working with data science pipelines","level":1,"index":3,"id":"working-with-data-science-pipelines_ds-pipelines"},{"parentId":"working-with-data-science-pipelines_ds-pipelines","name":"Enabling Data Science Pipelines 2.0","level":2,"index":0,"id":"enabling-data-science-pipelines-2_ds-pipelines"},{"parentId":"enabling-data-science-pipelines-2_ds-pipelines","name":"Installing Open Data Hub with DSP 2.0","level":3,"index":0,"id":"_installing_open_data_hub_with_dsp_2_0"},{"parentId":"enabling-data-science-pipelines-2_ds-pipelines","name":"Upgrading to DSP 2.0","level":3,"index":1,"id":"_upgrading_to_dsp_2_0"},{"parentId":"working-with-data-science-pipelines_ds-pipelines","name":"Managing data science pipelines","level":2,"index":1,"id":"_managing_data_science_pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Configuring a pipeline server","level":3,"index":0,"id":"configuring-a-pipeline-server_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Defining a pipeline","level":3,"index":1,"id":"defining-a-pipeline_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Importing a data science pipeline","level":3,"index":2,"id":"importing-a-data-science-pipeline_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Downloading a data science pipeline version","level":3,"index":3,"id":"downloading-a-data-science-pipeline-version_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Deleting a data science pipeline","level":3,"index":4,"id":"deleting-a-data-science-pipeline_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Deleting a pipeline server","level":3,"index":5,"id":"deleting-a-pipeline-server_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Viewing the details of a pipeline server","level":3,"index":6,"id":"viewing-the-details-of-a-pipeline-server_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Viewing existing pipelines","level":3,"index":7,"id":"viewing-existing-pipelines_ds-pipelines"},{"parentId":"working-with-data-science-pipelines_ds-pipelines","name":"Managing pipeline runs","level":2,"index":2,"id":"_managing_pipeline_runs"},{"parentId":"_managing_pipeline_runs","name":"Overview of pipeline runs","level":3,"index":0,"id":"overview-of-pipeline-runs_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Viewing active pipeline runs","level":3,"index":1,"id":"viewing-active-pipeline-runs_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Executing a pipeline run","level":3,"index":2,"id":"executing-a-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Stopping an active pipeline run","level":3,"index":3,"id":"stopping-an-active-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Duplicating an active pipeline run","level":3,"index":4,"id":"duplicating-an-active-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Viewing scheduled pipeline runs","level":3,"index":5,"id":"viewing-scheduled-pipeline-runs_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Scheduling a pipeline run using a cron job","level":3,"index":6,"id":"scheduling-a-pipeline-run-using-a-cron-job_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Scheduling a pipeline run","level":3,"index":7,"id":"scheduling-a-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Duplicating a scheduled pipeline run","level":3,"index":8,"id":"duplicating-a-scheduled-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Deleting a scheduled pipeline run","level":3,"index":9,"id":"deleting-a-scheduled-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Viewing the details of a pipeline run","level":3,"index":10,"id":"viewing-the-details-of-a-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Viewing archived pipeline runs","level":3,"index":11,"id":"viewing-archived-pipeline-runs_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Archiving a pipeline run","level":3,"index":12,"id":"archiving-a-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Restoring an archived pipeline run","level":3,"index":13,"id":"restoring-an-archived-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Deleting an archived pipeline run","level":3,"index":14,"id":"deleting-an-archived-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Duplicating an archived pipeline run","level":3,"index":15,"id":"duplicating-an-archived-pipeline-run_ds-pipelines"},{"parentId":"working-with-data-science-pipelines_ds-pipelines","name":"Working with pipeline logs","level":2,"index":3,"id":"_working_with_pipeline_logs"},{"parentId":"_working_with_pipeline_logs","name":"About pipeline logs","level":3,"index":0,"id":"about-pipeline-logs_ds-pipelines"},{"parentId":"_working_with_pipeline_logs","name":"Viewing pipeline step logs","level":3,"index":1,"id":"viewing-pipeline-step-logs_ds-pipelines"},{"parentId":"_working_with_pipeline_logs","name":"Downloading pipeline step logs","level":3,"index":2,"id":"downloading-pipeline-step-logs_ds-pipelines"},{"parentId":"working-with-data-science-pipelines_ds-pipelines","name":"Working with pipelines in JupyterLab","level":2,"index":4,"id":"_working_with_pipelines_in_jupyterlab"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Overview of pipelines in JupyterLab","level":3,"index":0,"id":"overview-of-pipelines-in-jupyterlab_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Accessing the pipeline editor","level":3,"index":1,"id":"accessing-the-pipeline-editor_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Creating a runtime configuration","level":3,"index":2,"id":"creating-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Updating a runtime configuration","level":3,"index":3,"id":"updating-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Deleting a runtime configuration","level":3,"index":4,"id":"deleting-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Duplicating a runtime configuration","level":3,"index":5,"id":"duplicating-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Running a pipeline in JupyterLab","level":3,"index":6,"id":"running-a-pipeline-in-jupyterlab_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Exporting a pipeline in JupyterLab","level":3,"index":7,"id":"exporting-a-pipeline-in-jupyterlab_ds-pipelines"},{"parentId":"working-with-data-science-pipelines_ds-pipelines","name":"Additional resources","level":2,"index":5,"id":"_additional_resources"},{"parentId":null,"name":"Working with accelerators","level":1,"index":4,"id":"working-with-accelerators_accelerators"},{"parentId":"working-with-accelerators_accelerators","name":"Overview of accelerators","level":2,"index":0,"id":"overview-of-accelerators_accelerators"},{"parentId":"working-with-accelerators_accelerators","name":"Working with accelerator profiles","level":2,"index":1,"id":"working-with-accelerator-profiles_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Creating an accelerator profile","level":3,"index":0,"id":"creating-an-accelerator-profile_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Updating an accelerator profile","level":3,"index":1,"id":"updating-an-accelerator-profile_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Deleting an accelerator profile","level":3,"index":2,"id":"deleting-an-accelerator-profile_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Viewing accelerator profiles","level":3,"index":3,"id":"viewing-accelerator-profiles_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Configuring a recommended accelerator for notebook images","level":3,"index":4,"id":"configuring-a-recommended-accelerator-for-notebook-images_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Configuring a recommended accelerator for serving runtimes","level":3,"index":5,"id":"configuring-a-recommended-accelerator-for-serving-runtimes_accelerators"},{"parentId":"working-with-accelerators_accelerators","name":"Habana Gaudi integration","level":2,"index":2,"id":"habana-gaudi-integration_accelerators"},{"parentId":"habana-gaudi-integration_accelerators","name":"Enabling Habana Gaudi devices","level":3,"index":0,"id":"enabling-habana-gaudi-devices_accelerators"},{"parentId":null,"name":"Troubleshooting common problems in Jupyter for administrators","level":1,"index":5,"id":"troubleshooting-common-problems-in-jupyter-for-administrators_accelerators"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_accelerators","name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":2,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_accelerators","name":"A user&#8217;s notebook server does not start","level":2,"index":1,"id":"_a_users_notebook_server_does_not_start"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_accelerators","name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":2,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"},{"parentId":null,"name":"Troubleshooting common problems in Jupyter for users","level":1,"index":6,"id":"troubleshooting-common-problems-in-jupyter-for-users_accelerators"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-users_accelerators","name":"I see a <strong>403: Forbidden</strong> error when I log in to Jupyter","level":2,"index":0,"id":"_i_see_a_403_forbidden_error_when_i_log_in_to_jupyter"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-users_accelerators","name":"My notebook server does not start","level":2,"index":1,"id":"_my_notebook_server_does_not_start"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-users_accelerators","name":"I see a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when I run my notebook cells","level":2,"index":2,"id":"_i_see_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_i_run_my_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-distributed-workloads/"},"sections":[{"parentId":null,"name":"Overview of distributed workloads","level":1,"index":0,"id":"overview-of-distributed-workloads_distributed-workloads"},{"parentId":null,"name":"Configuring distributed workloads","level":1,"index":1,"id":"configuring-distributed-workloads_distributed-workloads"},{"parentId":"configuring-distributed-workloads_distributed-workloads","name":"Configuring the distributed workloads components","level":2,"index":0,"id":"configuring-the-distributed-workloads-components_distributed-workloads"},{"parentId":"configuring-distributed-workloads_distributed-workloads","name":"Configuring quota management for distributed workloads","level":2,"index":1,"id":"configuring-quota-management-for-distributed-workloads_distributed-workloads"},{"parentId":null,"name":"Running distributed workloads","level":1,"index":2,"id":"running-distributed-workloads_distributed-workloads"},{"parentId":"running-distributed-workloads_distributed-workloads","name":"Running distributed data science workloads from notebooks","level":2,"index":0,"id":"running-distributed-data-science-workloads-from-notebooks_distributed-workloads"},{"parentId":"running-distributed-workloads_distributed-workloads","name":"Running distributed data science workloads from data science pipelines","level":2,"index":1,"id":"running-distributed-data-science-workloads-from-ds-pipelines_distributed-workloads"},{"parentId":null,"name":"Monitoring distributed workloads","level":1,"index":3,"id":"monitoring-distributed-workloads_distributed-workloads"},{"parentId":"monitoring-distributed-workloads_distributed-workloads","name":"Viewing project metrics for distributed workloads","level":2,"index":0,"id":"viewing-project-metrics-for-distributed-workloads_distributed-workloads"},{"parentId":"monitoring-distributed-workloads_distributed-workloads","name":"Viewing the status of distributed workloads","level":2,"index":1,"id":"viewing-the-status-of-distributed-workloads_distributed-workloads"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/_artifacts/document-attributes-global/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/adding-users/"},"sections":[{"parentId":null,"name":"Overview of user types and permissions","level":1,"index":0,"id":"overview-of-user-types-and-permissions_{context}"},{"parentId":null,"name":"Defining {productname-short} administrator and user groups","level":1,"index":1,"id":"defining-data-science-admin-and-user-groups_{context}"},{"parentId":null,"name":"Adding users to specialized {productname-short} user groups","level":1,"index":2,"id":"adding-users-to-specialized-data-science-user-groups_{context}"},{"parentId":null,"name":"Viewing {productname-short} users","level":1,"index":3,"id":"viewing-data-science-users_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/collaborating-on-notebooks-using-git/"},"sections":[{"parentId":null,"name":"Uploading an existing notebook file from a Git repository using JupyterLab","level":1,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_git-collab"},{"parentId":null,"name":"Uploading an existing notebook file from a Git repository using the command line interface","level":1,"index":1,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_git-collab"},{"parentId":null,"name":"Updating your project with changes from a remote Git repository","level":1,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_git-collab"},{"parentId":null,"name":"Pushing project changes to a Git repository","level":1,"index":3,"id":"pushing-project-changes-to-a-git-repository_git-collab"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-bias-metrics-for-a-model/"},"sections":[{"parentId":null,"name":"Creating a bias metric","level":1,"index":0,"id":"creating-a-bias-metric_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the dashboard","level":2,"index":0,"id":"creating-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the CLI","level":2,"index":1,"id":"creating-a-bias-metric-using-cli_bias-monitoring"},{"parentId":null,"name":"Duplicating a bias metric","level":1,"index":1,"id":"duplicating-a-bias-metric_bias-monitoring"},{"parentId":null,"name":"Deleting a bias metric","level":1,"index":2,"id":"deleting-a-bias-metric_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the dashboard","level":2,"index":0,"id":"deleting-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the CLI","level":2,"index":1,"id":"deleting-a-bias-metric-using-cli_bias-monitoring"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/creating-and-importing-notebooks/"},"sections":[{"parentId":null,"name":"Creating a new notebook","level":1,"index":0,"id":"creating-a-new-notebook_notebooks"},{"parentId":"creating-a-new-notebook_notebooks","name":"Notebook images for data scientists","level":2,"index":0,"id":"notebook-images-for-data-scientists_notebooks"},{"parentId":null,"name":"Uploading an existing notebook file from local storage","level":1,"index":1,"id":"uploading-an-existing-notebook-file-from-local-storage_notebooks"},{"parentId":null,"name":"Uploading an existing notebook file from a Git repository using JupyterLab","level":1,"index":2,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_notebooks"},{"parentId":null,"name":"Uploading an existing notebook file from a Git repository using the command line interface","level":1,"index":3,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_notebooks"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/customizing-the-dashboard/"},"sections":[{"parentId":null,"name":"Editing the dashboard configuration file","level":1,"index":0,"id":"editing-the-dashboard-configuration-file_dashboard"},{"parentId":null,"name":"Dashboard configuration options","level":1,"index":1,"id":"ref-dashboard-configuration-options_dashboard"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/deleting-users/"},"sections":[{"parentId":null,"name":"About deleting users and their resources","level":1,"index":0,"id":"about-deleting-users-and-resources_{context}"},{"parentId":null,"name":"Backing up storage data","level":1,"index":1,"id":"backing-up-storage-data_{context}"},{"parentId":null,"name":"Stopping notebook servers owned by other users","level":1,"index":2,"id":"stopping-notebook-servers-owned-by-other-users_{context}"},{"parentId":null,"name":"Revoking user access to Jupyter","level":1,"index":3,"id":"revoking-user-access-to-jupyter_{context}"},{"parentId":null,"name":"Cleaning up after deleting users","level":1,"index":4,"id":"cleaning-up-after-deleting-users_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/installing-odh-v1/"},"sections":[{"parentId":null,"name":"Installing the Open Data Hub Operator version 1","level":1,"index":0,"id":"installing-the-odh-operator-v1_installv1"},{"parentId":null,"name":"Creating a new project for your Open Data Hub instance","level":1,"index":1,"id":"creating-a-new-project-for-your-odh-instance_installv1"},{"parentId":null,"name":"Adding an Open Data Hub instance","level":1,"index":2,"id":"adding-an-odh-instance_installv1"},{"parentId":null,"name":"Accessing the Open Data Hub dashboard","level":1,"index":3,"id":"accessing-the-odh-dashboard_installv1"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/installing-odh-v2/"},"sections":[{"parentId":null,"name":"Installing the Open Data Hub Operator version 2","level":1,"index":0,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":null,"name":"Installing Open Data Hub components","level":1,"index":1,"id":"installing-odh-components_installv2"},{"parentId":null,"name":"Accessing the Open Data Hub dashboard","level":1,"index":2,"id":"accessing-the-odh-dashboard_installv2"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-applications-that-show-in-the-dashboard/"},"sections":[{"parentId":null,"name":"Adding an application to the dashboard","level":1,"index":0,"id":"adding-an-application-to-the-dashboard_dashboard"},{"parentId":null,"name":"Preventing users from adding applications to the dashboard","level":1,"index":1,"id":"preventing-users-from-adding-applications-to-the-dashboard_dashboard"},{"parentId":null,"name":"Showing or hiding information about enabled applications","level":1,"index":2,"id":"showing-hiding-information-about-enabled-applications_dashboard"},{"parentId":null,"name":"Hiding the default Jupyter application","level":1,"index":3,"id":"hiding-the-default-jupyter-application_dashboard"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-notebook-servers/"},"sections":[{"parentId":null,"name":"Accessing the Jupyter administration interface","level":1,"index":0,"id":"accessing-the-jupyter-administration-interface_{context}"},{"parentId":null,"name":"Starting notebook servers owned by other users","level":1,"index":1,"id":"starting-notebook-servers-owned-by-other-users_{context}"},{"parentId":null,"name":"Accessing notebook servers owned by other users","level":1,"index":2,"id":"accessing-notebook-servers-owned-by-other-users_{context}"},{"parentId":null,"name":"Stopping notebook servers owned by other users","level":1,"index":3,"id":"stopping-notebook-servers-owned-by-other-users_{context}"},{"parentId":null,"name":"Stopping idle notebooks","level":1,"index":4,"id":"stopping-idle-notebooks_{context}"},{"parentId":null,"name":"Adding notebook pod tolerations","level":1,"index":5,"id":"adding-notebook-pod-tolerations_{context}"},{"parentId":null,"name":"Configuring a custom notebook image","level":1,"index":6,"id":"configuring-a-custom-notebook-image_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-cluster-resources/"},"sections":[{"parentId":null,"name":"Configuring the default PVC size for your cluster","level":1,"index":0,"id":"configuring-the-default-pvc-size-for-your-cluster_{context}"},{"parentId":null,"name":"Restoring the default PVC size for your cluster","level":1,"index":1,"id":"restoring-the-default-pvc-size-for-your-cluster_{context}"},{"parentId":null,"name":"Overview of accelerators","level":1,"index":2,"id":"overview-of-accelerators_{context}"},{"parentId":"overview-of-accelerators_{context}","name":"Enabling GPU support in {productname-short}","level":2,"index":0,"id":"enabling-gpu-support_{context}"},{"parentId":"overview-of-accelerators_{context}","name":"Enabling Habana Gaudi devices","level":2,"index":1,"id":"enabling-habana-gaudi-devices_{context}"},{"parentId":null,"name":"Allocating additional resources to {productname-short} users","level":1,"index":3,"id":"allocating-additional-resources-to-data-science-users_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-model-performance/"},"sections":[{"parentId":null,"name":"Viewing performance metrics for all models on a model server","level":1,"index":0,"id":"viewing-performance-metrics-for-model-server_monitoring-model-performance"},{"parentId":null,"name":"Viewing HTTP request metrics for a deployed model","level":1,"index":1,"id":"viewing-http-request-metrics-for-a-deployed-model_monitoring-model-performance"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/serving-large-models/"},"sections":[{"parentId":null,"name":"About the single-model serving platform","level":1,"index":0,"id":"about-the-single-model-serving-platform_serving-large-models"},{"parentId":null,"name":"Installing KServe","level":1,"index":1,"id":"installing-kserve_serving-large-models"},{"parentId":null,"name":"Deploying models by using the single-model serving platform","level":1,"index":2,"id":"deploying-models-using-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Enabling the single-model serving platform","level":2,"index":0,"id":"enabling-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Adding a custom model-serving runtime for the single-model serving platform","level":2,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Deploying models on the single-model serving platform","level":2,"index":2,"id":"deploying-models-on-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Accessing the inference endpoint for a deployed model","level":2,"index":3,"id":"accessing-inference-endpoint-for-deployed-model_serving-large-models"},{"parentId":null,"name":"Configuring monitoring for the single-model serving platform","level":1,"index":3,"id":"configuring-monitoring-for-the-single-model-serving-platform_serving-large-models"},{"parentId":null,"name":"Viewing model-serving runtime metrics for the single-model serving platform","level":1,"index":4,"id":"viewing-metrics-for-the-single-model-serving-platform_serving-large-models"},{"parentId":null,"name":"Performance tuning on the single-model serving platform","level":1,"index":5,"id":"_performance_tuning_on_the_single_model_serving_platform"},{"parentId":"_performance_tuning_on_the_single_model_serving_platform","name":"Resolving CUDA out-of-memory errors","level":2,"index":0,"id":"resolving-cuda-oom-errors-for-the-single-model-serving-platform_serving-large-models"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/serving-small-and-medium-sized-models/"},"sections":[{"parentId":null,"name":"Configuring model servers","level":1,"index":0,"id":"_configuring_model_servers"},{"parentId":"_configuring_model_servers","name":"Enabling the multi-model serving platform","level":2,"index":0,"id":"enabling-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a custom model-serving runtime for the multi-model serving platform","level":2,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a model server for the multi-model serving platform","level":2,"index":2,"id":"adding-a-model-server-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Deleting a model server","level":2,"index":3,"id":"deleting-a-model-server_model-serving"},{"parentId":null,"name":"Working with deployed models","level":1,"index":1,"id":"_working_with_deployed_models"},{"parentId":"_working_with_deployed_models","name":"Deploying a model by using the multi-model serving platform","level":2,"index":0,"id":"deploying-a-model-using-the-multi-model-serving-platform_model-serving"},{"parentId":"_working_with_deployed_models","name":"Viewing a deployed model","level":2,"index":1,"id":"viewing-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Updating the deployment properties of a deployed model","level":2,"index":2,"id":"updating-the-deployment-properties-of-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Deleting a deployed model","level":2,"index":3,"id":"deleting-a-deployed-model_model-serving"},{"parentId":null,"name":"Configuring monitoring for the multi-model serving platform","level":1,"index":2,"id":"configuring-monitoring-for-the-multi-model-serving-platform_model-serving"},{"parentId":null,"name":"Viewing model-serving runtime metrics for the multi-model serving platform","level":1,"index":3,"id":"viewing-metrics-for-the-multi-model-serving-platform_model-serving"},{"parentId":null,"name":"Monitoring model performance","level":1,"index":4,"id":"_monitoring_model_performance"},{"parentId":"_monitoring_model_performance","name":"Viewing performance metrics for all models on a model server","level":2,"index":0,"id":"viewing-performance-metrics-for-model-server_model-serving"},{"parentId":"_monitoring_model_performance","name":"Viewing HTTP request metrics for a deployed model","level":2,"index":1,"id":"viewing-http-request-metrics-for-a-deployed-model_model-serving"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/support-requirements-and-limitations/"},"sections":[{"parentId":null,"name":"Supported browsers","level":1,"index":0,"id":"supported-browsers_requirements"},{"parentId":null,"name":"Supported services","level":1,"index":1,"id":"supported-services_requirements"},{"parentId":null,"name":"Supported packages","level":1,"index":2,"id":"supported-packages_requirements"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/upgrading-odh-v2/"},"sections":[{"parentId":null,"name":"Cleaning up resources","level":1,"index":0,"id":"cleaning-up-resources_upgradev2"},{"parentId":null,"name":"Installing Open Data Hub version 2","level":1,"index":1,"id":"installing-odh-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing the Open Data Hub Operator version 2","level":2,"index":0,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing Open Data Hub components","level":2,"index":1,"id":"installing-odh-components_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Accessing the Open Data Hub dashboard","level":2,"index":2,"id":"accessing-the-odh-dashboard_installv2"},{"parentId":null,"name":"Adding a CA bundle after upgrading","level":1,"index":2,"id":"adding-a-ca-bundle-after-upgrading_upgradev2"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/upgrading-odh-v1-to-v2/"},"sections":[{"parentId":null,"name":"Upgrading the Open Data Hub Operator version 1","level":1,"index":0,"id":"upgrading-the-odh-operator-v1_upgradev1"},{"parentId":null,"name":"Installing Open Data Hub components","level":1,"index":1,"id":"installing-odh-components_upgradev1"},{"parentId":null,"name":"Accessing the Open Data Hub dashboard","level":1,"index":2,"id":"accessing-the-odh-dashboard_upgradev1"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-accelerators/"},"sections":[{"parentId":null,"name":"Overview of accelerators","level":1,"index":0,"id":"overview-of-accelerators_accelerators"},{"parentId":null,"name":"Working with accelerator profiles","level":1,"index":1,"id":"working-with-accelerator-profiles_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Creating an accelerator profile","level":2,"index":0,"id":"creating-an-accelerator-profile_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Updating an accelerator profile","level":2,"index":1,"id":"updating-an-accelerator-profile_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Deleting an accelerator profile","level":2,"index":2,"id":"deleting-an-accelerator-profile_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Viewing accelerator profiles","level":2,"index":3,"id":"viewing-accelerator-profiles_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Configuring a recommended accelerator for notebook images","level":2,"index":4,"id":"configuring-a-recommended-accelerator-for-notebook-images_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Configuring a recommended accelerator for serving runtimes","level":2,"index":5,"id":"configuring-a-recommended-accelerator-for-serving-runtimes_accelerators"},{"parentId":null,"name":"Habana Gaudi integration","level":1,"index":2,"id":"habana-gaudi-integration_accelerators"},{"parentId":"habana-gaudi-integration_accelerators","name":"Enabling Habana Gaudi devices","level":2,"index":0,"id":"enabling-habana-gaudi-devices_accelerators"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-certificates/"},"sections":[{"parentId":null,"name":"Understanding certificates in {productname-short}","level":1,"index":0,"id":"understanding-certificates_certs"},{"parentId":"understanding-certificates_certs","name":"How CA bundles are injected","level":2,"index":0,"id":"_how_ca_bundles_are_injected"},{"parentId":"understanding-certificates_certs","name":"How the ConfigMap is managed","level":2,"index":1,"id":"_how_the_configmap_is_managed"},{"parentId":null,"name":"Adding a CA bundle","level":1,"index":1,"id":"adding-a-ca-bundle_certs"},{"parentId":null,"name":"Removing a CA bundle","level":1,"index":2,"id":"removing-a-ca-bundle_certs"},{"parentId":null,"name":"Removing a CA bundle from a namespace","level":1,"index":3,"id":"removing-a-ca-bundle-from-a-namespace_certs"},{"parentId":null,"name":"Managing certificates","level":1,"index":4,"id":"managing-certificates_certs"},{"parentId":null,"name":"Using self-signed certificates with {productname-short} components","level":1,"index":5,"id":"_using_self_signed_certificates_with_productname_short_components"},{"parentId":"_using_self_signed_certificates_with_productname_short_components","name":"Using certificates with data science pipelines","level":2,"index":0,"id":"using-certificates-with-data-science-pipelines_certs"},{"parentId":"using-certificates-with-data-science-pipelines_certs","name":"Providing a CA bundle only for data science pipelines","level":3,"index":0,"id":"_providing_a_ca_bundle_only_for_data_science_pipelines"},{"parentId":"_using_self_signed_certificates_with_productname_short_components","name":"Using certificates with workbenches","level":2,"index":1,"id":"using-certificates-with-workbenches_certs"},{"parentId":"using-certificates-with-workbenches_certs","name":"Creating data science pipelines with Elyra and self-signed certificates","level":3,"index":0,"id":"_creating_data_science_pipelines_with_elyra_and_self_signed_certificates"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-on-data-science-projects/"},"sections":[{"parentId":null,"name":"Using data science projects","level":1,"index":0,"id":"_using_data_science_projects"},{"parentId":"_using_data_science_projects","name":"Creating a data science project","level":2,"index":0,"id":"creating-a-data-science-project_nb-server"},{"parentId":"_using_data_science_projects","name":"Updating a data science project","level":2,"index":1,"id":"updating-a-data-science-project_nb-server"},{"parentId":"_using_data_science_projects","name":"Deleting a data science project","level":2,"index":2,"id":"deleting-a-data-science-project_nb-server"},{"parentId":null,"name":"Using project workbenches","level":1,"index":1,"id":"_using_project_workbenches"},{"parentId":"_using_project_workbenches","name":"Creating a project workbench","level":2,"index":0,"id":"creating-a-project-workbench_nb-server"},{"parentId":"_using_project_workbenches","name":"Starting a workbench","level":2,"index":1,"id":"starting-a-workbench_nb-server"},{"parentId":"_using_project_workbenches","name":"Updating a project workbench","level":2,"index":2,"id":"updating-a-project-workbench_nb-server"},{"parentId":"_using_project_workbenches","name":"Deleting a workbench from a data science project","level":2,"index":3,"id":"deleting-a-workbench-from-a-data-science-project_nb-server"},{"parentId":null,"name":"Using data connections","level":1,"index":2,"id":"_using_data_connections"},{"parentId":"_using_data_connections","name":"Adding a data connection to your data science project","level":2,"index":0,"id":"adding-a-data-connection-to-your-data-science-project_nb-server"},{"parentId":"_using_data_connections","name":"Deleting a data connection","level":2,"index":1,"id":"deleting-a-data-connection_nb-server"},{"parentId":"_using_data_connections","name":"Updating a connected data source","level":2,"index":2,"id":"updating-a-connected-data-source_nb-server"},{"parentId":null,"name":"Configuring cluster storage","level":1,"index":3,"id":"_configuring_cluster_storage"},{"parentId":"_configuring_cluster_storage","name":"Adding cluster storage to your data science project","level":2,"index":0,"id":"adding-cluster-storage-to-your-data-science-project_nb-server"},{"parentId":"_configuring_cluster_storage","name":"Updating cluster storage","level":2,"index":1,"id":"updating-cluster-storage_nb-server"},{"parentId":"_configuring_cluster_storage","name":"Deleting cluster storage from a data science project","level":2,"index":2,"id":"deleting-cluster-storage-from-a-data-science-project_nb-server"},{"parentId":null,"name":"Configuring data science pipelines","level":1,"index":4,"id":"_configuring_data_science_pipelines"},{"parentId":"_configuring_data_science_pipelines","name":"Configuring a pipeline server","level":2,"index":0,"id":"configuring-a-pipeline-server_nb-server"},{"parentId":"_configuring_data_science_pipelines","name":"Defining a pipeline","level":2,"index":1,"id":"defining-a-pipeline_nb-server"},{"parentId":"_configuring_data_science_pipelines","name":"Importing a data science pipeline","level":2,"index":2,"id":"importing-a-data-science-pipeline_nb-server"},{"parentId":null,"name":"Configuring access to data science projects","level":1,"index":5,"id":"_configuring_access_to_data_science_projects"},{"parentId":"_configuring_access_to_data_science_projects","name":"Configuring access to data science projects","level":2,"index":0,"id":"configuring-access-to-data-science-projects_nb-server"},{"parentId":"_configuring_access_to_data_science_projects","name":"Sharing access to a data science project","level":2,"index":1,"id":"sharing-access-to-a-data-science-project_nb-server"},{"parentId":"_configuring_access_to_data_science_projects","name":"Updating access to a data science project","level":2,"index":2,"id":"updating-access-to-a-data-science-project_nb-server"},{"parentId":"_configuring_access_to_data_science_projects","name":"Removing access to a data science project","level":2,"index":3,"id":"removing-access-to-a-data-science-project_nb-server"},{"parentId":null,"name":"Viewing Python packages installed on your notebook server","level":1,"index":6,"id":"viewing-python-packages-installed-on-your-notebook-server_nb-server"},{"parentId":null,"name":"Installing Python packages on your notebook server","level":1,"index":7,"id":"installing-python-packages-on-your-notebook-server_nb-server"},{"parentId":null,"name":"Updating notebook server settings by restarting your server","level":1,"index":8,"id":"updating-notebook-server-settings-by-restarting-your-server_nb-server"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Enabling Data Science Pipelines 2.0","level":1,"index":0,"id":"enabling-data-science-pipelines-2_ds-pipelines"},{"parentId":"enabling-data-science-pipelines-2_ds-pipelines","name":"Installing {productname-short} with DSP 2.0","level":2,"index":0,"id":"_installing_productname_short_with_dsp_2_0"},{"parentId":"enabling-data-science-pipelines-2_ds-pipelines","name":"Upgrading to DSP 2.0","level":2,"index":1,"id":"_upgrading_to_dsp_2_0"},{"parentId":null,"name":"Managing data science pipelines","level":1,"index":1,"id":"_managing_data_science_pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Configuring a pipeline server","level":2,"index":0,"id":"configuring-a-pipeline-server_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Defining a pipeline","level":2,"index":1,"id":"defining-a-pipeline_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Importing a data science pipeline","level":2,"index":2,"id":"importing-a-data-science-pipeline_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Downloading a data science pipeline version","level":2,"index":3,"id":"downloading-a-data-science-pipeline-version_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Deleting a data science pipeline","level":2,"index":4,"id":"deleting-a-data-science-pipeline_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Deleting a pipeline server","level":2,"index":5,"id":"deleting-a-pipeline-server_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Viewing the details of a pipeline server","level":2,"index":6,"id":"viewing-the-details-of-a-pipeline-server_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Viewing existing pipelines","level":2,"index":7,"id":"viewing-existing-pipelines_ds-pipelines"},{"parentId":null,"name":"Managing pipeline runs","level":1,"index":2,"id":"_managing_pipeline_runs"},{"parentId":"_managing_pipeline_runs","name":"Overview of pipeline runs","level":2,"index":0,"id":"overview-of-pipeline-runs_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Viewing active pipeline runs","level":2,"index":1,"id":"viewing-active-pipeline-runs_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Executing a pipeline run","level":2,"index":2,"id":"executing-a-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Stopping an active pipeline run","level":2,"index":3,"id":"stopping-an-active-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Duplicating an active pipeline run","level":2,"index":4,"id":"duplicating-an-active-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Viewing scheduled pipeline runs","level":2,"index":5,"id":"viewing-scheduled-pipeline-runs_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Scheduling a pipeline run using a cron job","level":2,"index":6,"id":"scheduling-a-pipeline-run-using-a-cron-job_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Scheduling a pipeline run","level":2,"index":7,"id":"scheduling-a-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Duplicating a scheduled pipeline run","level":2,"index":8,"id":"duplicating-a-scheduled-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Deleting a scheduled pipeline run","level":2,"index":9,"id":"deleting-a-scheduled-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Viewing the details of a pipeline run","level":2,"index":10,"id":"viewing-the-details-of-a-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Viewing archived pipeline runs","level":2,"index":11,"id":"viewing-archived-pipeline-runs_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Archiving a pipeline run","level":2,"index":12,"id":"archiving-a-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Restoring an archived pipeline run","level":2,"index":13,"id":"restoring-an-archived-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Deleting an archived pipeline run","level":2,"index":14,"id":"deleting-an-archived-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Duplicating an archived pipeline run","level":2,"index":15,"id":"duplicating-an-archived-pipeline-run_ds-pipelines"},{"parentId":null,"name":"Working with pipeline logs","level":1,"index":3,"id":"_working_with_pipeline_logs"},{"parentId":"_working_with_pipeline_logs","name":"About pipeline logs","level":2,"index":0,"id":"about-pipeline-logs_ds-pipelines"},{"parentId":"_working_with_pipeline_logs","name":"Viewing pipeline step logs","level":2,"index":1,"id":"viewing-pipeline-step-logs_ds-pipelines"},{"parentId":"_working_with_pipeline_logs","name":"Downloading pipeline step logs","level":2,"index":2,"id":"downloading-pipeline-step-logs_ds-pipelines"},{"parentId":null,"name":"Working with pipelines in JupyterLab","level":1,"index":4,"id":"_working_with_pipelines_in_jupyterlab"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Overview of pipelines in JupyterLab","level":2,"index":0,"id":"overview-of-pipelines-in-jupyterlab_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Accessing the pipeline editor","level":2,"index":1,"id":"accessing-the-pipeline-editor_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Creating a runtime configuration","level":2,"index":2,"id":"creating-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Updating a runtime configuration","level":2,"index":3,"id":"updating-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Deleting a runtime configuration","level":2,"index":4,"id":"deleting-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Duplicating a runtime configuration","level":2,"index":5,"id":"duplicating-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Running a pipeline in JupyterLab","level":2,"index":6,"id":"running-a-pipeline-in-jupyterlab_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Exporting a pipeline in JupyterLab","level":2,"index":7,"id":"exporting-a-pipeline-in-jupyterlab_ds-pipelines"},{"parentId":null,"name":"Additional resources","level":1,"index":5,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-deleting-users-and-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-model-serving/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-pipeline-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-authorization-token-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-inference-endpoint-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-jupyter-administration-interface/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-odh-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-pipeline-editor/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-tutorials/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-ca-bundle-after-upgrading/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-data-connection-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-an-application-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-model-server-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-an-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-cluster-storage-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-users-to-specialized-data-science-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/allocating-additional-resources-to-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-notebook-pod-tolerations/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/backing-up-storage-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/authenticating-trustyai-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/cleaning-up-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/cleaning-up-after-deleting-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/common-questions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-recommended-accelerator-for-notebook-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-custom-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-recommended-accelerator-for-serving-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-access-to-data-science-projects/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-monitoring-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-quota-management-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-monitoring-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-codeflare-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-your-ide/"},"sections":[{"parentId":null,"name":"Configuring your code-server workbench","level":1,"index":0,"id":"_configuring_your_code_server_workbench"},{"parentId":"_configuring_your_code_server_workbench","name":"Installing extensions with code-server","level":2,"index":0,"id":"_installing_extensions_with_code_server"},{"parentId":"_installing_extensions_with_code_server","name":"Extensions","level":3,"index":0,"id":"_extensions"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-distributed-workloads-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric/"},"sections":[{"parentId":null,"name":"Creating a bias metric by using the dashboard","level":1,"index":0,"id":"creating-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Creating a bias metric by using the CLI","level":1,"index":1,"id":"creating-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-new-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-new-project-for-your-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/defining-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/defining-data-science-admin-and-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric/"},"sections":[{"parentId":null,"name":"Deleting a bias metric by using the dashboard","level":1,"index":0,"id":"deleting-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Deleting a bias metric by using the CLI","level":1,"index":1,"id":"deleting-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-data-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-workbench-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-model-using-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-cluster-storage-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-on-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-using-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-a-data-science-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/disabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-bias-metric/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-the-dashboard-configuration-file/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-gpu-support-in-data-science/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-data-science-pipelines-2/"},"sections":[{"parentId":null,"name":"Installing {productname-short} with DSP 2.0","level":1,"index":0,"id":"_installing_productname_short_with_dsp_2_0"},{"parentId":null,"name":"Upgrading to DSP 2.0","level":1,"index":1,"id":"_upgrading_to_dsp_2_0"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-services-connected-to-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-habana-gaudi-devices/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-trustyai-service-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-trustyai-service-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-trustyai-service/"},"sections":[{"parentId":null,"name":"Enabling the TrustyAI Service by using the dashboard","level":1,"index":0,"id":"enabling-trustyai-service-using-dashboard_{context}"},{"parentId":null,"name":"Enabling the TrustyAI Service by using the CLI","level":1,"index":1,"id":"enabling-trustyai-service-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/executing-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/exporting-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/habana-gaudi-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/hiding-information-about-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/hiding-the-default-jupyter-application/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/importing-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-odh-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-the-odh-operator-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-python-packages-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/making-inference-requests-to-models-deployed-on-single-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/logging-in-to-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/launching-jupyter-and-starting-a-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/monitoring-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/managing-certificates/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/notifications-in-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/notebook-images-for-data-scientists/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/options-for-notebook-server-environments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipelines-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-user-types-and-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/pushing-project-changes-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/preventing-users-from-adding-applications-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-dashboard-configuration-options/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-a-ca-bundle-from-a-namespace/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-a-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-disabled-applications-from-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requirements-for-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/resolving-cuda-oom-errors/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/revoking-user-access-to-jupyter/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-disconnected-env/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/scheduling-a-pipeline-run-using-a-cron-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-from-ds-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-from-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/sending-training-data-to-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/scheduling-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/sharing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/showing-hiding-information-about-enabled-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-a-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-idle-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/supported-browsers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/supported-packages/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/supported-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/supported-services/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/the-open-data-hub-user-interface/"},"sections":[{"parentId":null,"name":"Global navigation","level":1,"index":0,"id":"_global_navigation"},{"parentId":null,"name":"Side navigation","level":1,"index":1,"id":"_side_navigation"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-in-jupyter-for-administrators/"},"sections":[{"parentId":null,"name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":1,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":null,"name":"A user&#8217;s notebook server does not start","level":1,"index":1,"id":"_a_users_notebook_server_does_not_start"},{"parentId":null,"name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":1,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-in-jupyter-for-users/"},"sections":[{"parentId":null,"name":"I see a <strong>403: Forbidden</strong> error when I log in to Jupyter","level":1,"index":0,"id":"_i_see_a_403_forbidden_error_when_i_log_in_to_jupyter"},{"parentId":null,"name":"My notebook server does not start","level":1,"index":1,"id":"_my_notebook_server_does_not_start"},{"parentId":null,"name":"I see a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when I run my notebook cells","level":1,"index":2,"id":"_i_see_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_i_run_my_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/understanding-certificates/"},"sections":[{"parentId":null,"name":"How CA bundles are injected","level":1,"index":0,"id":"_how_ca_bundles_are_injected"},{"parentId":null,"name":"How the ConfigMap is managed","level":1,"index":1,"id":"_how_the_configmap_is_managed"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-connected-data-source/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/tutorials-for-data-scientists/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-cluster-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-notebook-server-settings-by-restarting-your-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-the-deployment-properties-of-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-your-project-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/upgrading-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-certificates-with-workbenches/"},"sections":[{"parentId":null,"name":"Creating data science pipelines with Elyra and self-signed certificates","level":1,"index":0,"id":"_creating_data_science_pipelines_with_elyra_and_self_signed_certificates"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-active-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-archived-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-existing-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-http-request-metrics-for-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-metrics-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-performance-metrics-for-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-metrics-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-project-metrics-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-scheduled-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-python-packages-installed-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-status-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/working-with-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-deleting-users-and-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-model-serving/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-pipeline-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-authorization-token-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-inference-endpoint-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-jupyter-administration-interface/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-odh-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-tutorials/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-pipeline-editor/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-ca-bundle-after-upgrading/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-data-connection-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-an-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-model-server-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-cluster-storage-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-notebook-pod-tolerations/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-an-application-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/authenticating-trustyai-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/allocating-additional-resources-to-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/backing-up-storage-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-users-to-specialized-data-science-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/cleaning-up-after-deleting-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/common-questions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/cleaning-up-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-recommended-accelerator-for-notebook-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-custom-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-recommended-accelerator-for-serving-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-access-to-data-science-projects/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-monitoring-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-quota-management-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-codeflare-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-monitoring-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-your-ide/"},"sections":[{"parentId":null,"name":"Configuring your code-server workbench","level":1,"index":0,"id":"_configuring_your_code_server_workbench"},{"parentId":"_configuring_your_code_server_workbench","name":"Installing extensions with code-server","level":2,"index":0,"id":"_installing_extensions_with_code_server"},{"parentId":"_installing_extensions_with_code_server","name":"Extensions","level":3,"index":0,"id":"_extensions"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-distributed-workloads-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric/"},"sections":[{"parentId":null,"name":"Creating a bias metric by using the dashboard","level":1,"index":0,"id":"creating-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Creating a bias metric by using the CLI","level":1,"index":1,"id":"creating-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-new-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-new-project-for-your-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/defining-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/defining-data-science-admin-and-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric/"},"sections":[{"parentId":null,"name":"Deleting a bias metric by using the dashboard","level":1,"index":0,"id":"deleting-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Deleting a bias metric by using the CLI","level":1,"index":1,"id":"deleting-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-data-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-workbench-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-model-using-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-cluster-storage-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-using-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-on-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-a-data-science-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/disabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-bias-metric/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-the-dashboard-configuration-file/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-gpu-support-in-data-science/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-services-connected-to-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-habana-gaudi-devices/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-trustyai-service-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-data-science-pipelines-2/"},"sections":[{"parentId":null,"name":"Installing {productname-short} with DSP 2.0","level":1,"index":0,"id":"_installing_productname_short_with_dsp_2_0"},{"parentId":null,"name":"Upgrading to DSP 2.0","level":1,"index":1,"id":"_upgrading_to_dsp_2_0"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-trustyai-service-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-trustyai-service/"},"sections":[{"parentId":null,"name":"Enabling the TrustyAI Service by using the dashboard","level":1,"index":0,"id":"enabling-trustyai-service-using-dashboard_{context}"},{"parentId":null,"name":"Enabling the TrustyAI Service by using the CLI","level":1,"index":1,"id":"enabling-trustyai-service-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/executing-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/exporting-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/hiding-information-about-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/hiding-the-default-jupyter-application/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/habana-gaudi-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-odh-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/importing-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-the-odh-operator-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-python-packages-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/making-inference-requests-to-models-deployed-on-single-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/logging-in-to-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/monitoring-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/managing-certificates/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/notifications-in-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/launching-jupyter-and-starting-a-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/notebook-images-for-data-scientists/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/options-for-notebook-server-environments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipelines-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/preventing-users-from-adding-applications-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/pushing-project-changes-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-user-types-and-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-a-ca-bundle-from-a-namespace/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-a-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-disabled-applications-from-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requirements-for-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/resolving-cuda-oom-errors/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-dashboard-configuration-options/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/revoking-user-access-to-jupyter/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-disconnected-env/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/scheduling-a-pipeline-run-using-a-cron-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-from-ds-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/sending-training-data-to-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/scheduling-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/sharing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-from-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-a-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/showing-hiding-information-about-enabled-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-idle-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/supported-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/supported-browsers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/supported-packages/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/supported-services/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-in-jupyter-for-administrators/"},"sections":[{"parentId":null,"name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":1,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":null,"name":"A user&#8217;s notebook server does not start","level":1,"index":1,"id":"_a_users_notebook_server_does_not_start"},{"parentId":null,"name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":1,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-in-jupyter-for-users/"},"sections":[{"parentId":null,"name":"I see a <strong>403: Forbidden</strong> error when I log in to Jupyter","level":1,"index":0,"id":"_i_see_a_403_forbidden_error_when_i_log_in_to_jupyter"},{"parentId":null,"name":"My notebook server does not start","level":1,"index":1,"id":"_my_notebook_server_does_not_start"},{"parentId":null,"name":"I see a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when I run my notebook cells","level":1,"index":2,"id":"_i_see_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_i_run_my_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/understanding-certificates/"},"sections":[{"parentId":null,"name":"How CA bundles are injected","level":1,"index":0,"id":"_how_ca_bundles_are_injected"},{"parentId":null,"name":"How the ConfigMap is managed","level":1,"index":1,"id":"_how_the_configmap_is_managed"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/the-open-data-hub-user-interface/"},"sections":[{"parentId":null,"name":"Global navigation","level":1,"index":0,"id":"_global_navigation"},{"parentId":null,"name":"Side navigation","level":1,"index":1,"id":"_side_navigation"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/tutorials-for-data-scientists/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-connected-data-source/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-cluster-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-notebook-server-settings-by-restarting-your-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-your-project-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-the-deployment-properties-of-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/upgrading-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-certificates-with-workbenches/"},"sections":[{"parentId":null,"name":"Creating data science pipelines with Elyra and self-signed certificates","level":1,"index":0,"id":"_creating_data_science_pipelines_with_elyra_and_self_signed_certificates"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-certificates-with-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Providing a CA bundle only for data science pipelines","level":1,"index":0,"id":"_providing_a_ca_bundle_only_for_data_science_pipelines"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-active-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-archived-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-metrics-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-existing-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-http-request-metrics-for-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-performance-metrics-for-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-metrics-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-project-metrics-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-python-packages-installed-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-scheduled-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-status-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/working-with-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-certificates-with-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Providing a CA bundle only for data science pipelines","level":1,"index":0,"id":"_providing_a_ca_bundle_only_for_data_science_pipelines"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-local-storage/"},"sections":null}}}]},"asciidoc":{"html":"<div id=\"toc\" class=\"toc\">\n<div id=\"toctitle\">Table of Contents</div>\n<ul class=\"sectlevel1\">\n<li><a href=\"#creating-and-importing-notebooks_notebooks\">Creating and importing notebooks</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#creating-a-new-notebook_notebooks\">Creating a new notebook</a></li>\n<li><a href=\"#uploading-an-existing-notebook-file-from-local-storage_notebooks\">Uploading an existing notebook file from local storage</a></li>\n<li><a href=\"#uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_notebooks\">Uploading an existing notebook file from a Git repository using JupyterLab</a></li>\n<li><a href=\"#uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_notebooks\">Uploading an existing notebook file from a Git repository using the command line interface</a></li>\n</ul>\n</li>\n<li><a href=\"#collaborating-on-notebooks-using-git_git-collab\">Collaborating on notebooks using Git</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_git-collab\">Uploading an existing notebook file from a Git repository using JupyterLab</a></li>\n<li><a href=\"#uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_git-collab\">Uploading an existing notebook file from a Git repository using the command line interface</a></li>\n<li><a href=\"#updating-your-project-with-changes-from-a-remote-git-repository_git-collab\">Updating your project with changes from a remote Git repository</a></li>\n<li><a href=\"#pushing-project-changes-to-a-git-repository_git-collab\">Pushing project changes to a Git repository</a></li>\n</ul>\n</li>\n<li><a href=\"#working-on-data-science-projects_nb-server\">Working on data science projects</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#_using_data_science_projects\">Using data science projects</a></li>\n<li><a href=\"#_using_project_workbenches\">Using project workbenches</a></li>\n<li><a href=\"#_using_data_connections\">Using data connections</a></li>\n<li><a href=\"#_configuring_cluster_storage\">Configuring cluster storage</a></li>\n<li><a href=\"#_configuring_data_science_pipelines\">Configuring data science pipelines</a></li>\n<li><a href=\"#_configuring_access_to_data_science_projects\">Configuring access to data science projects</a></li>\n<li><a href=\"#viewing-python-packages-installed-on-your-notebook-server_nb-server\">Viewing Python packages installed on your notebook server</a></li>\n<li><a href=\"#installing-python-packages-on-your-notebook-server_nb-server\">Installing Python packages on your notebook server</a></li>\n<li><a href=\"#updating-notebook-server-settings-by-restarting-your-server_nb-server\">Updating notebook server settings by restarting your server</a></li>\n</ul>\n</li>\n<li><a href=\"#working-with-data-science-pipelines_ds-pipelines\">Working with data science pipelines</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#enabling-data-science-pipelines-2_ds-pipelines\">Enabling Data Science Pipelines 2.0</a></li>\n<li><a href=\"#_managing_data_science_pipelines\">Managing data science pipelines</a></li>\n<li><a href=\"#_managing_pipeline_runs\">Managing pipeline runs</a></li>\n<li><a href=\"#_working_with_pipeline_logs\">Working with pipeline logs</a></li>\n<li><a href=\"#_working_with_pipelines_in_jupyterlab\">Working with pipelines in JupyterLab</a></li>\n<li><a href=\"#_additional_resources\">Additional resources</a></li>\n</ul>\n</li>\n<li><a href=\"#working-with-accelerators_accelerators\">Working with accelerators</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#overview-of-accelerators_accelerators\">Overview of accelerators</a></li>\n<li><a href=\"#working-with-accelerator-profiles_accelerators\">Working with accelerator profiles</a></li>\n<li><a href=\"#habana-gaudi-integration_accelerators\">Habana Gaudi integration</a></li>\n</ul>\n</li>\n<li><a href=\"#troubleshooting-common-problems-in-jupyter-for-administrators_accelerators\">Troubleshooting common problems in Jupyter for administrators</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter\">A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter</a></li>\n<li><a href=\"#_a_users_notebook_server_does_not_start\">A user&#8217;s notebook server does not start</a></li>\n<li><a href=\"#_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells\">The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells</a></li>\n</ul>\n</li>\n<li><a href=\"#troubleshooting-common-problems-in-jupyter-for-users_accelerators\">Troubleshooting common problems in Jupyter for users</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#_i_see_a_403_forbidden_error_when_i_log_in_to_jupyter\">I see a <strong>403: Forbidden</strong> error when I log in to Jupyter</a></li>\n<li><a href=\"#_my_notebook_server_does_not_start\">My notebook server does not start</a></li>\n<li><a href=\"#_i_see_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_i_run_my_notebook_cells\">I see a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when I run my notebook cells</a></li>\n</ul>\n</li>\n</ul>\n</div>\n<div class=\"sect1\">\n<h2 id=\"creating-and-importing-notebooks_notebooks\">Creating and importing notebooks</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>You can create a blank notebook or import a notebook from several different sources.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"creating-a-new-notebook_notebooks\">Creating a new notebook</h3>\n<div class=\"paragraph _abstract\">\n<p>You can create a new Jupyter notebook from an existing notebook container image to access its resources and properties. The <strong>Notebook server control panel</strong> contains a list of available container images that you can run as a single-user notebook server.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>Ensure that you have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>Ensure that you have launched your notebook server and logged in to Jupyter.</p>\n</li>\n<li>\n<p>The notebook image exists in a registry, image stream, and is accessible.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Click <strong>File</strong> &#8594; <strong>New</strong> &#8594; <strong>Notebook</strong>.</p>\n</li>\n<li>\n<p>If prompted, select a kernel for your notebook from the list.</p>\n<div class=\"paragraph\">\n<p>If you want to use a kernel, click <strong>Select</strong>. If you do not want to use a kernel, click <strong>No Kernel</strong>.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Check that the notebook file is visible in the JupyterLab interface.</p>\n</li>\n</ul>\n</div>\n<div class=\"sect3\">\n<h4 id=\"notebook-images-for-data-scientists_notebooks\">Notebook images for data scientists</h4>\n<div class=\"paragraph _abstract\">\n<p>Open Data Hub contains Jupyter notebook images optimized with industry-leading tools and libraries required for your data science work. To provide a consistent, stable platform for your model development, all notebook images contain the same version of Python. Notebook images available on Open Data Hub are pre-built and ready for you to use immediately after Open Data Hub is installed or upgraded.</p>\n</div>\n<div class=\"paragraph\">\n<p>When a new version of a notebook image is released, the previous version remains available on the cluster. This gives you time to migrate your work to the latest version of the notebook image. Legacy notebook image versions, that is, not the two most recent versions, might still be available for selection. Legacy image versions include a label that indicates that the image is out-of-date. To use the latest package versions, use the most recently added notebook image.</p>\n</div>\n<div class=\"paragraph\">\n<p>Open Data Hub contains the following notebook images that are available by default.</p>\n</div>\n<table class=\"tableblock frame-all grid-all stretch\">\n<caption class=\"title\">Table 1. Default notebook images</caption>\n<colgroup>\n<col style=\"width: 16.6666%;\">\n<col style=\"width: 83.3334%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Image name</th>\n<th class=\"tableblock halign-left valign-top\">Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">CUDA</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">If you are working with compute-intensive data science models that require GPU support, use the Compute Unified Device Architecture (CUDA) notebook image to gain access to the NVIDIA CUDA Toolkit. Using this toolkit, you can optimize your work using GPU-accelerated libraries and optimization tools.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Standard Data Science</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Use the Standard Data Science notebook image for models that do not require TensorFlow or PyTorch. This image contains commonly used libraries to assist you in developing your machine learning models.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">TensorFlow</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">TensorFlow is an open source platform for machine learning. With TensorFlow, you can build, train and deploy your machine learning models. TensorFlow contains advanced data visualization features, such as computational graph visualizations. It also allows you to easily monitor and track the progress of your models.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">PyTorch</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">PyTorch is an open source machine learning library optimized for deep learning. If you are working with computer vision or natural language processing models, use the Pytorch notebook image.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Minimal Python</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">If you do not require advanced machine learning features, or additional resources for compute-intensive data science work, you can use the Minimal Python image to develop your models.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">TrustyAI</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Use the TrustyAI notebook image to leverage your data science work with model explainability, tracing, and accountability, and runtime monitoring.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">HabanaAI</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">The HabanaAI notebook image optimizes high-performance deep learning (DL) with Habana Gaudi devices. Habana Gaudi devices accelerate DL training workloads and maximize training throughput and efficiency.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">code-server</p></td>\n<td class=\"tableblock halign-left valign-top\"><div class=\"content\"><div class=\"paragraph\">\n<p>With the code-server notebook image, you can customize your notebook environment to meet your needs using a variety of extensions to add new languages, themes, debuggers, and connect to additional services. Enhance the efficiency of your data science work with syntax highlighting, auto-indentation, and bracket matching, as well as an automatic task runner for seamless automation. See <a href=\"https://github.com/coder/code-server\">code-server in GitHub</a> for more information.<br></p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nElyra-based pipelines are not available with the code-server notebook image.\n</td>\n</tr>\n</table>\n</div></div></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">RStudio Server</p></td>\n<td class=\"tableblock halign-left valign-top\"><div class=\"content\"><div class=\"paragraph\">\n<p>Use the RStudio Server notebook image to access the RStudio IDE, an integrated development environment for R, a programming language for statistical computing and graphics.\nSee <a href=\"https://posit.co/products/open-source/rstudio-server/\">the RStudio Server site</a> for more information.<br></p>\n</div></div></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">CUDA - RStudio Server</p></td>\n<td class=\"tableblock halign-left valign-top\"><div class=\"content\"><div class=\"paragraph\">\n<p>Use the CUDA - RStudio Server notebook image to access the RStudio IDE and NVIDIA CUDA Toolkit. RStudio is an integrated development environment for R, a programming language for statistical computing and graphics. With the NVIDIA CUDA toolkit, you can optimize your work using GPU-accelerated libraries and optimization tools.\nSee <a href=\"https://posit.co/products/open-source/rstudio-server/\">the RStudio Server site</a> for more information.<br></p>\n</div></div></td>\n</tr>\n</tbody>\n</table>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"uploading-an-existing-notebook-file-from-local-storage_notebooks\">Uploading an existing notebook file from local storage</h3>\n<div class=\"paragraph _abstract\">\n<p>You can load an existing notebook from local storage into JupyterLab to continue work, or adapt a project for a new use case.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>Credentials for logging in to Jupyter.</p>\n</li>\n<li>\n<p>A launched and running notebook server.</p>\n</li>\n<li>\n<p>A notebook file exists in your local storage.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the <strong>File Browser</strong> in the left sidebar of the JupyterLab interface, click <strong>Upload Files</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyter-upload-file.png\" alt=\"Upload Files\"></span>).</p>\n</li>\n<li>\n<p>Locate and select the notebook file and click <strong>Open</strong>.</p>\n<div class=\"paragraph\">\n<p>The file is displayed in the <strong>File Browser</strong>.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The notebook file displays in the <strong>File Browser</strong> in the left sidebar of the JupyterLab interface.</p>\n</li>\n<li>\n<p>You can open the notebook file in JupyterLab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_notebooks\">Uploading an existing notebook file from a Git repository using JupyterLab</h3>\n<div class=\"paragraph _abstract\">\n<p>You can use the JupyterLab user interface to clone a Git repository into your workspace to continue your work or integrate files from an external project.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>A launched and running Jupyter server.</p>\n</li>\n<li>\n<p>Read access for the Git repository you want to clone.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Copy the HTTPS URL for the Git repository.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>On GitHub, click <strong>&#10515; Code</strong> &#8594; <strong>HTTPS</strong> and click the Clipboard button.</p>\n</li>\n<li>\n<p>On GitLab, click <strong>Clone</strong> and click the Clipboard button under <strong>Clone with HTTPS</strong>.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>In the JupyterLab interface, click the <strong>Git Clone</strong> button (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-git-clone-button.png\" alt=\"Git Clone button\"></span>).</p>\n<div class=\"paragraph\">\n<p>You can also click <strong>Git</strong> &#8594; <strong>Clone a repository</strong> in the menu, or click the Git icon (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-git-button.png\" alt=\"Git button\"></span>) and click the <strong>Clone a repository</strong> button.</p>\n</div>\n<div class=\"paragraph\">\n<p>The <em>Clone a repo</em> dialog appears.</p>\n</div>\n</li>\n<li>\n<p>Enter the HTTPS URL of the repository that contains your notebook.</p>\n</li>\n<li>\n<p>Click <strong>CLONE</strong>.</p>\n</li>\n<li>\n<p>If prompted, enter your username and password for the Git repository.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Check that the contents of the repository are visible in the file browser in JupyterLab, or run the <strong>ls</strong> command in the terminal to verify that the repository is shown as a directory.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_notebooks\">Uploading an existing notebook file from a Git repository using the command line interface</h3>\n<div class=\"paragraph _abstract\">\n<p>You can use the command line interface to clone a Git repository into your workspace to continue your work or integrate files from an external project.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>A launched and running Jupyter server.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Copy the HTTPS URL for the Git repository.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>On GitHub, click <strong>&#10515; Code</strong> &#8594; <strong>HTTPS</strong> and click the Clipboard button.</p>\n</li>\n<li>\n<p>On GitLab, click <strong>Clone</strong> and click the Clipboard button under <strong>Clone with HTTPS</strong>.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>In JupyterLab, click <strong>File</strong> &#8594; <strong>New</strong> &#8594; <strong>Terminal</strong> to open a terminal window.</p>\n</li>\n<li>\n<p>Enter the <code>git clone</code> command.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>git clone <em>&lt;git-clone-URL&gt;</em></code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Replace <em>`&lt;git-clone-URL&gt;`</em> with the HTTPS URL, for example:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>[1234567890@jupyter-nb-jdoe ~]$ <strong>git clone https://github.com/example/myrepo.git</strong>\nCloning into <em>myrepo</em>...\nremote: Enumerating objects: 11, done.\nremote: Counting objects: 100% (11/11), done.\nremote: Compressing objects: 100% (10/10), done.\nremote: Total 2821 (delta 1), reused 5 (delta 1), pack-reused 2810\nReceiving objects: 100% (2821/2821), 39.17 MiB | 23.89 MiB/s, done.\nResolving deltas: 100% (1416/1416), done.</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Check that the contents of the repository are visible in the file browser in JupyterLab, or run the <strong>ls</strong> command in the terminal to verify that the repository is shown as a directory.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"collaborating-on-notebooks-using-git_git-collab\">Collaborating on notebooks using Git</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>If your notebooks or other files are stored in Git version control, you can import them from a Git repository onto your notebook server to work with them in JupyterLab. When you are ready, you can push your changes back to the Git repository so that others can review or use your models.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_git-collab\">Uploading an existing notebook file from a Git repository using JupyterLab</h3>\n<div class=\"paragraph _abstract\">\n<p>You can use the JupyterLab user interface to clone a Git repository into your workspace to continue your work or integrate files from an external project.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>A launched and running Jupyter server.</p>\n</li>\n<li>\n<p>Read access for the Git repository you want to clone.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Copy the HTTPS URL for the Git repository.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>On GitHub, click <strong>&#10515; Code</strong> &#8594; <strong>HTTPS</strong> and click the Clipboard button.</p>\n</li>\n<li>\n<p>On GitLab, click <strong>Clone</strong> and click the Clipboard button under <strong>Clone with HTTPS</strong>.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>In the JupyterLab interface, click the <strong>Git Clone</strong> button (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-git-clone-button.png\" alt=\"Git Clone button\"></span>).</p>\n<div class=\"paragraph\">\n<p>You can also click <strong>Git</strong> &#8594; <strong>Clone a repository</strong> in the menu, or click the Git icon (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-git-button.png\" alt=\"Git button\"></span>) and click the <strong>Clone a repository</strong> button.</p>\n</div>\n<div class=\"paragraph\">\n<p>The <em>Clone a repo</em> dialog appears.</p>\n</div>\n</li>\n<li>\n<p>Enter the HTTPS URL of the repository that contains your notebook.</p>\n</li>\n<li>\n<p>Click <strong>CLONE</strong>.</p>\n</li>\n<li>\n<p>If prompted, enter your username and password for the Git repository.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Check that the contents of the repository are visible in the file browser in JupyterLab, or run the <strong>ls</strong> command in the terminal to verify that the repository is shown as a directory.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_git-collab\">Uploading an existing notebook file from a Git repository using the command line interface</h3>\n<div class=\"paragraph _abstract\">\n<p>You can use the command line interface to clone a Git repository into your workspace to continue your work or integrate files from an external project.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>A launched and running Jupyter server.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Copy the HTTPS URL for the Git repository.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>On GitHub, click <strong>&#10515; Code</strong> &#8594; <strong>HTTPS</strong> and click the Clipboard button.</p>\n</li>\n<li>\n<p>On GitLab, click <strong>Clone</strong> and click the Clipboard button under <strong>Clone with HTTPS</strong>.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>In JupyterLab, click <strong>File</strong> &#8594; <strong>New</strong> &#8594; <strong>Terminal</strong> to open a terminal window.</p>\n</li>\n<li>\n<p>Enter the <code>git clone</code> command.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>git clone <em>&lt;git-clone-URL&gt;</em></code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Replace <em>`&lt;git-clone-URL&gt;`</em> with the HTTPS URL, for example:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>[1234567890@jupyter-nb-jdoe ~]$ <strong>git clone https://github.com/example/myrepo.git</strong>\nCloning into <em>myrepo</em>...\nremote: Enumerating objects: 11, done.\nremote: Counting objects: 100% (11/11), done.\nremote: Compressing objects: 100% (10/10), done.\nremote: Total 2821 (delta 1), reused 5 (delta 1), pack-reused 2810\nReceiving objects: 100% (2821/2821), 39.17 MiB | 23.89 MiB/s, done.\nResolving deltas: 100% (1416/1416), done.</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Check that the contents of the repository are visible in the file browser in JupyterLab, or run the <strong>ls</strong> command in the terminal to verify that the repository is shown as a directory.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"updating-your-project-with-changes-from-a-remote-git-repository_git-collab\">Updating your project with changes from a remote Git repository</h3>\n<div class=\"paragraph _abstract\">\n<p>You can pull changes made by other users into your data science project from a remote Git repository.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have configured the remote Git repository.</p>\n</li>\n<li>\n<p>You have already imported the Git repository into JupyterLab, and the contents of the repository are visible in the file browser in JupyterLab.</p>\n</li>\n<li>\n<p>You have permissions to pull files from the remote Git repository to your local repository.</p>\n</li>\n<li>\n<p>You have credentials for logging in to Jupyter.</p>\n</li>\n<li>\n<p>You have a launched and running Jupyter server.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the JupyterLab interface, click the<strong>Git</strong> button (<span class=\"image\"><img src=\"/static/docs/images/jupyter-git-sidebar.png\" alt=\"Git button\"></span>).</p>\n</li>\n<li>\n<p>Click the <strong>Pull latest changes</strong> button (<span class=\"image\"><img src=\"/static/docs/images/jupyter-git-pull-button.png\" alt=\"Pull latest changes button\"></span>).</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>You can view the changes pulled from the remote repository in the <strong>History</strong> tab of the Git pane.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"pushing-project-changes-to-a-git-repository_git-collab\">Pushing project changes to a Git repository</h3>\n<div class=\"paragraph _abstract\">\n<p>To build and deploy your application in a production environment, upload your work to a remote Git repository.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have opened a notebook in the JupyterLab interface.</p>\n</li>\n<li>\n<p>You have already added the relevant Git repository to your notebook server.</p>\n</li>\n<li>\n<p>You have permission to push changes to the relevant Git repository.</p>\n</li>\n<li>\n<p>You have installed the Git version control extension.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Click <strong>File</strong> &#8594; <strong>Save All</strong> to save any unsaved changes.</p>\n</li>\n<li>\n<p>Click the Git icon (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-git-button.png\" alt=\"Git button\"></span>) to open the Git pane in the JupyterLab interface.</p>\n</li>\n<li>\n<p>Confirm that your changed files appear under <strong>Changed</strong>.</p>\n<div class=\"paragraph\">\n<p>If your changed files appear under <strong>Untracked</strong>, click <strong>Git</strong> &#8594; <strong>Simple Staging</strong> to enable a simplified Git process.</p>\n</div>\n</li>\n<li>\n<p>Commit your changes.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Ensure that all files under <strong>Changed</strong> have a blue checkmark beside them.</p>\n</li>\n<li>\n<p>In the <strong>Summary</strong> field, enter a brief description of the changes you made.</p>\n</li>\n<li>\n<p>Click <strong>Commit</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Git</strong> &#8594; <strong>Push to Remote</strong> to push your changes to the remote repository.</p>\n</li>\n<li>\n<p>When prompted, enter your Git credentials and click <strong>OK</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Your most recently pushed changes are visible in the remote Git repository.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"working-on-data-science-projects_nb-server\">Working on data science projects</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>As a data scientist, you can organize your data science work into a single project. A data science project in Open Data Hub can consist of the following components:</p>\n</div>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">Workbenches</dt>\n<dd>\n<p>Creating a workbench allows you to add a Jupyter notebook to your project.</p>\n</dd>\n<dt class=\"hdlist1\">Cluster storage</dt>\n<dd>\n<p>For data science projects that require data retention, you can add cluster storage to the project.</p>\n</dd>\n<dt class=\"hdlist1\">Data connections</dt>\n<dd>\n<p>Adding a data connection to your project allows you to connect data inputs to your workbenches.</p>\n</dd>\n<dt class=\"hdlist1\">Pipelines</dt>\n<dd>\n<p>Standardize and automate machine learning workflows to enable you to further enhance and deploy your data science models.</p>\n</dd>\n<dt class=\"hdlist1\">Models and model servers</dt>\n<dd>\n<p>Deploy a trained data science model to serve intelligent applications. Your model is deployed with an endpoint that allows applications to send requests to the model.</p>\n</dd>\n<dt class=\"hdlist1\">Bias metrics for models</dt>\n<dd>\n<p>Creating bias metrics allows you to monitor your machine learning models for bias.</p>\n</dd>\n</dl>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you create an OpenShift project outside of the Open Data Hub user interface, the project is not shown on the <strong>Data Science Projects</strong> page. In addition, you cannot use features exclusive to Open Data Hub, such as workbenches and model serving, with a standard OpenShift project.</p>\n</div>\n<div class=\"paragraph\">\n<p>To classify your OpenShift project as a data science project, and to make available features exclusive to Open Data Hub, you must add the label <code>opendatahub.io/dashboard: 'true'</code> to the project namespace. After you add this label, your project is subsequently shown on the <strong>Data Science Projects</strong> page.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_using_data_science_projects\">Using data science projects</h3>\n<div class=\"sect3\">\n<h4 id=\"creating-a-data-science-project_nb-server\">Creating a data science project</h4>\n<div class=\"paragraph _abstract\">\n<p>To start your data science work, create a data science project. Creating a project helps you organize your work in one place. You can also enhance your data science project by adding the following functionality:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Workbenches</p>\n</li>\n<li>\n<p>Storage for your project&#8217;s cluster</p>\n</li>\n<li>\n<p>Data connections</p>\n</li>\n<li>\n<p>Data science pipelines</p>\n</li>\n<li>\n<p>Model servers</p>\n</li>\n<li>\n<p>Bias monitoring for your models</p>\n</li>\n</ul>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click <strong>Create data science project</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Create a data science project</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter a <strong>name</strong> for your data science project.</p>\n</li>\n<li>\n<p>Optional: Edit the <strong>resource name</strong> for your data science project. The resource name must consist of lowercase alphanumeric characters, <em>-</em>, and must start and end with an alphanumeric character.</p>\n</li>\n<li>\n<p>Enter a <strong>description</strong> for your data science project.</p>\n</li>\n<li>\n<p>Click <strong>Create</strong>.</p>\n<div class=\"paragraph\">\n<p>A project details page opens. From this page, you can create workbenches, add cluster storage and data connections, import pipelines, and deploy models.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The project that you created is displayed on the <strong>Data Science Projects</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"updating-a-data-science-project_nb-server\">Updating a data science project</h4>\n<div class=\"paragraph _abstract\">\n<p>You can update your data science project&#8217;s details by changing your project&#8217;s name and description text.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the project whose details you want to update and click <strong>Edit project</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Edit data science project</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Optional: Update the <strong>name</strong> for your data science project.</p>\n</li>\n<li>\n<p>Optional: Update the <strong>description</strong> for your data science project.</p>\n</li>\n<li>\n<p>Click <strong>Update</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The data science project that you updated is displayed on the <strong>Data Science Projects</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deleting-a-data-science-project_nb-server\">Deleting a data science project</h4>\n<div class=\"paragraph _abstract\">\n<p>You can delete data science projects so that they do not appear on the Open Data Hub <strong>Data Science Projects</strong> page when you no longer want to use them.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the project that you want to delete and then click <strong>Delete project</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Delete project</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter the project name in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete project</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The data science project that you deleted is no longer displayed on the <strong>Data Science Projects</strong> page.</p>\n</li>\n<li>\n<p>Deleting a data science project deletes any associated workbenches, data science pipelines, cluster storage, and data connections. This data is permanently deleted and is not recoverable.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_using_project_workbenches\">Using project workbenches</h3>\n<div class=\"sect3\">\n<h4 id=\"creating-a-project-workbench_nb-server\">Creating a project workbench</h4>\n<div class=\"paragraph _abstract\">\n<p>To examine and work with models in an isolated area, you can create a workbench. You can use this workbench to create a Jupyter notebook from an existing notebook container image to access its resources and properties. For data science projects that require data retention, you can add container storage to the workbench you are creating. If you require extra power for use with large datasets, you can assign accelerators to your workbench to optimize performance.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you use specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that you can add a workbench to.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to add the workbench to.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Workbenches</strong> tab.</p>\n</li>\n<li>\n<p>Click <strong>Create workbench</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Create workbench</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Configure the properties of the workbench you are creating.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Name</strong> field, enter a name for your workbench.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Description</strong> field, enter a description to define your workbench.</p>\n</li>\n<li>\n<p>In the <strong>Notebook image</strong> section, complete the fields to specify the notebook image to use with your workbench.</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>From the <strong>Image selection</strong> list, select a notebook image.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>In the <strong>Deployment size</strong> section, specify the size of your deployment instance.</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>From the <strong>Container size</strong> list, select a container size for your server.</p>\n</li>\n<li>\n<p>Optional: From the <strong>Accelerator</strong> list, select an accelerator.</p>\n</li>\n<li>\n<p>If you selected an accelerator in the preceding step, specify the number of accelerators to use.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Optional: Select and specify values for any new <strong>environment variables</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Configure the storage for your Open Data Hub cluster.</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>Select <strong>Create new persistent storage</strong> to create storage that is retained after you log out of Open Data Hub. Complete the relevant fields to define the storage.</p>\n</li>\n<li>\n<p>Select <strong>Use existing persistent storage</strong> to reuse existing storage and select the storage from the <strong>Persistent storage</strong> list.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>To use a data connection, in the <strong>Data connections</strong> section, select the <strong>Use a data connection</strong> checkbox.</p>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Create a new data connection as follows:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>Select <strong>Create new data connection</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Name</strong> field, enter a unique name for the data connection.</p>\n</li>\n<li>\n<p>In the <strong>Access key</strong> field, enter the access key ID for the S3-compatible object storage provider.</p>\n</li>\n<li>\n<p>In the <strong>Secret key</strong> field, enter the secret access key for the S3-compatible object storage account that you specified.</p>\n</li>\n<li>\n<p>In the <strong>Endpoint</strong> field, enter the endpoint of your S3-compatible object storage bucket.</p>\n</li>\n<li>\n<p>In the <strong>Region</strong> field, enter the default region of your S3-compatible object storage account.</p>\n</li>\n<li>\n<p>In the <strong>Bucket</strong> field, enter the name of your S3-compatible object storage bucket.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Use an existing data connection as follows:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>Select <strong>Use existing data connection</strong>.</p>\n</li>\n<li>\n<p>From the <strong>Data connection</strong> list, select a data connection that you previously defined.</p>\n</li>\n</ol>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>Click <strong>Create workbench</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The workbench that you created appears on the <strong>Workbenches</strong> tab for the project.</p>\n</li>\n<li>\n<p>Any cluster storage that you associated with the workbench during the creation process appears on the <strong>Cluster storage</strong> tab for the project.</p>\n</li>\n<li>\n<p>The <strong>Status</strong> column on the <strong>Workbenches</strong> tab displays a status of <strong>Starting</strong> when the workbench server is starting, and <strong>Running</strong> when the workbench has successfully started.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"starting-a-workbench_nb-server\">Starting a workbench</h4>\n<div class=\"paragraph _abstract\">\n<p>You can manually start a data science project&#8217;s workbench from the <strong>Workbenches</strong> tab on the project details page. By default, workbenches start immediately after you create them.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project whose workbench you want to start.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Workbenches</strong> tab.</p>\n</li>\n<li>\n<p>Click the toggle in the <strong>Status</strong> column for the relevant workbench to start a workbench that is not running.</p>\n<div class=\"paragraph\">\n<p>The status of the workbench that you started changes from <strong>Stopped</strong> to <strong>Running</strong>. After the workbench has started, click <strong>Open</strong> to open the workbench&#8217;s notebook.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The workbench that you started appears on the <strong>Workbenches</strong> tab for the project, with the status of <strong>Running</strong>.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"updating-a-project-workbench_nb-server\">Updating a project workbench</h4>\n<div class=\"paragraph _abstract\">\n<p>If your data science work requires you to change your workbench&#8217;s notebook image, container size, or identifying information, you can update the properties of your project&#8217;s workbench. If you require extra power for use with large datasets, you can assign accelerators to your workbench to optimize performance.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you use specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that has a workbench.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project whose workbench you want to update.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Workbenches</strong> tab.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the workbench that you want to update and then click <strong>Edit workbench</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Edit workbench</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Update any of the workbench properties and then click <strong>Update workbench</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The workbench that you updated appears on the <strong>Workbenches</strong> tab for the project.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deleting-a-workbench-from-a-data-science-project_nb-server\">Deleting a workbench from a data science project</h4>\n<div class=\"paragraph _abstract\">\n<p>You can delete workbenches from your data science projects to help you remove Jupyter notebooks that are no longer relevant to your work.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project with a workbench.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to delete the workbench from.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Workbenches</strong> tab.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the workbench that you want to delete and then click <strong>Delete workbench</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Delete workbench</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter the name of the workbench in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete workbench</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The workbench that you deleted is no longer displayed in the <strong>Workbenches</strong> tab for the project.</p>\n</li>\n<li>\n<p>The custom resource (CR) associated with the workbench&#8217;s Jupyter notebook is deleted.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_using_data_connections\">Using data connections</h3>\n<div class=\"sect3\">\n<h4 id=\"adding-a-data-connection-to-your-data-science-project_nb-server\">Adding a data connection to your data science project</h4>\n<div class=\"paragraph _abstract\">\n<p>You can enhance your data science project by adding a connection to a data source. When you want to work with a very large data sets, you can store your data in an S3-compatible object storage bucket, so that you do not fill up your local storage. You also have the option of associating the data connection with an existing workbench that does not already have a connection.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that you can add a data connection to.</p>\n</li>\n<li>\n<p>You have access to S3-compatible object storage.</p>\n</li>\n<li>\n<p>If you intend to add the data connection to an existing workbench, you have saved any data in the workbench to avoid losing work.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to add a data connection to.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Data connections</strong> tab.</p>\n</li>\n<li>\n<p>Click <strong>Add data connection</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Add data connection</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter a <strong>name</strong> for the data connection.</p>\n</li>\n<li>\n<p>In the <strong>Access key</strong> field, enter the access key ID for your S3-compatible object storage provider.</p>\n</li>\n<li>\n<p>In the <strong>Secret key</strong> field, enter the secret access key for the S3-compatible object storage account you specified.</p>\n</li>\n<li>\n<p>In the <strong>Endpoint</strong> field, enter the endpoint of your S3-compatible object storage bucket.</p>\n</li>\n<li>\n<p>In the <strong>Region</strong> field, enter the default region of your S3-compatible object storage account.</p>\n</li>\n<li>\n<p>In the <strong>Bucket</strong> field, enter the name of your S3-compatible object storage bucket.</p>\n</li>\n<li>\n<p>Optional: From the <strong>Connected workbench</strong> list, select a workbench to connect.</p>\n</li>\n<li>\n<p>Click <strong>Add data connection</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The data connection that you added appears in the <strong>Data connections</strong> tab for the project.</p>\n</li>\n<li>\n<p>If you selected a workbench, the workbench is visible in the <strong>Connected workbenches</strong> column in the <strong>Data connections</strong> tab for the project.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deleting-a-data-connection_nb-server\">Deleting a data connection</h4>\n<div class=\"paragraph _abstract\">\n<p>You can delete data connections from your data science projects to help you remove connections that are no longer relevant to your work.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project with a data connection.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to delete the data connection from.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Data connections</strong> tab.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the data connection that you want to delete and then click <strong>Delete data connection</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Delete data connection</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter the name of the data connection in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete data connection</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The data connection that you deleted is no longer displayed in the <strong>Data connections</strong> tab for the project.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"updating-a-connected-data-source_nb-server\">Updating a connected data source</h4>\n<div class=\"paragraph _abstract\">\n<p>To use an existing data source with a different workbench, you can change the data source that is connected to your project&#8217;s workbench.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project, created a workbench, and you have defined a data connection.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project whose data source you want to change.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Data connections</strong> tab.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the data source that you want to change and then click <strong>Edit data connection</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Edit data connection</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>In the <strong>Connected workbench</strong> section, select an existing workbench from the list.</p>\n</li>\n<li>\n<p>Click <strong>Update data connection</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The updated data connection is displayed in the <strong>Data connections</strong> tab for the project.</p>\n</li>\n<li>\n<p>You can access your S3 data source using environment variables in the connected workbench.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_configuring_cluster_storage\">Configuring cluster storage</h3>\n<div class=\"sect3\">\n<h4 id=\"adding-cluster-storage-to-your-data-science-project_nb-server\">Adding cluster storage to your data science project</h4>\n<div class=\"paragraph _abstract\">\n<p>For data science projects that require data to be retained, you can add cluster storage to the project. Additionally, you can also connect cluster storage to a specific project&#8217;s workbench.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that you can add cluster storage to.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to add the cluster storage to.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Cluster storage</strong> tab.</p>\n</li>\n<li>\n<p>Click <strong>Add cluster storage</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Add storage</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter a <strong>name</strong> for the cluster storage.</p>\n</li>\n<li>\n<p>Enter a <strong>description</strong> for the cluster storage.</p>\n</li>\n<li>\n<p>Under <strong>Persistent storage size</strong>, enter a new size in gibibytes. The minimum size is 1 GiB, and the maximum size is 16384 GiB.</p>\n</li>\n<li>\n<p>Optional: Select a <strong>workbench</strong> from the list to connect the cluster storage to an existing workbench.</p>\n</li>\n<li>\n<p>If you selected a workbench to connect the storage to, enter the storage directory in the <strong>Mount folder</strong> field.</p>\n</li>\n<li>\n<p>Click <strong>Add storage</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The cluster storage that you added appears in the <strong>Cluster storage</strong> tab for the project.</p>\n</li>\n<li>\n<p>A new persistent volume claim (PVC) is created with the storage size that you defined.</p>\n</li>\n<li>\n<p>The persistent volume claim (PVC) is visible as an attached storage in the <strong>Workbenches</strong> tab for the project.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"updating-cluster-storage_nb-server\">Updating cluster storage</h4>\n<div class=\"paragraph _abstract\">\n<p>If your data science work requires you to change the identifying information of a project&#8217;s cluster storage or the workbench that the storage is connected to, you can update your project&#8217;s cluster storage to change these properties.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that contains cluster storage.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project whose storage you want to update.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Cluster storage</strong> tab.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the storage that you want to update and then click <strong>Edit storage</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Edit storage</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Update the storage&#8217;s properties.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Update the <strong>name</strong> for the storage, if applicable.</p>\n</li>\n<li>\n<p>Update the <strong>description</strong> for the storage, if applicable.</p>\n</li>\n<li>\n<p>Increase the <strong>Persistent storage size</strong> for the storage, if applicable.</p>\n<div class=\"paragraph\">\n<p>Note that you can only increase the storage size. Updating the storage size restarts the workbench and makes it unavailable for a period of time that is usually proportional to the size change.</p>\n</div>\n</li>\n<li>\n<p>Update the <strong>workbench</strong> that the storage is connected to, if applicable.</p>\n</li>\n<li>\n<p>If you selected a new workbench to connect the storage to, enter the storage directory in the <strong>Mount folder</strong> field.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Update storage</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<p>If you increased the storage size, the workbench restarts and is unavailable for a period of time that is usually proportional to the size change.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The storage that you updated appears in the <strong>Cluster storage</strong> tab for the project.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deleting-cluster-storage-from-a-data-science-project_nb-server\">Deleting cluster storage from a data science project</h4>\n<div class=\"paragraph _abstract\">\n<p>You can delete cluster storage from your data science projects to help you free up resources and delete unwanted storage space.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project with cluster storage.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to delete the storage from.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Cluster storage</strong> tab.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the storage that you want to delete and then click <strong>Delete storage</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Delete storage</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter the name of the storage in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete storage</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The storage that you deleted is no longer displayed in the <strong>Cluster storage</strong> tab for the project.</p>\n</li>\n<li>\n<p>The persistent volume (PV) and persistent volume claim (PVC) associated with the cluster storage are both permanently deleted. This data is not recoverable.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_configuring_data_science_pipelines\">Configuring data science pipelines</h3>\n<div class=\"sect3\">\n<h4 id=\"configuring-a-pipeline-server_nb-server\">Configuring a pipeline server</h4>\n<div class=\"paragraph _abstract\">\n<p>Before you can successfully create a pipeline in Open Data Hub, you must configure a pipeline server. This task includes configuring where your pipeline artifacts and data are stored.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>You are not required to specify any storage directories when configuring a data connection for your pipeline server. When you import a pipeline, the <code>/pipelines</code> folder is created in the <code>root</code> folder of the bucket, containing a YAML file for the pipeline. If you upload a new version of the same pipeline, a new YAML file with a different ID is added to the <code>/pipelines</code> folder.</p>\n</div>\n<div class=\"paragraph\">\n<p>When you run a pipeline, the artifacts are stored in the <code>/pipeline-name</code> folder in the <code>root</code> folder of the bucket.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you use an external MySQL database and upgrade to Open Data Hub 2.10.0, the database is migrated to DSP 2.0 format, making it incompatible with earlier versions of Open Data Hub.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that you can add a pipeline server to.</p>\n</li>\n<li>\n<p>You have an existing S3-compatible object storage bucket and you have configured write access to your S3 bucket on your storage account.</p>\n</li>\n<li>\n<p>If you are configuring a pipeline server with an external MySQL database, your database must use MySQL version 5.x.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to configure a pipeline server for.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Pipelines</strong> tab.</p>\n</li>\n<li>\n<p>Click <strong>Configure pipeline server</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Configure pipeline server</strong> dialog appears.</p>\n</div>\n</li>\n<li>\n<p>In the <strong>Object storage connection</strong> section, provide values for the mandatory fields:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Access key</strong> field, enter the access key ID for the S3-compatible object storage provider.</p>\n</li>\n<li>\n<p>In the <strong>Secret key</strong> field, enter the secret access key for the S3-compatible object storage account that you specified.</p>\n</li>\n<li>\n<p>In the <strong>Endpoint</strong> field, enter the endpoint of your S3-compatible object storage bucket.</p>\n</li>\n<li>\n<p>In the <strong>Region</strong> field, enter the default region of your S3-compatible object storage account.</p>\n</li>\n<li>\n<p>In the <strong>Bucket</strong> field, enter the name of your S3-compatible object storage bucket.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you specify incorrect data connection settings, you cannot update these settings on the same pipeline server. Therefore, you must delete the pipeline server and configure another one.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>In the <strong>Database</strong> section, click <strong>Show advanced database options</strong> to specify the database to store your pipeline data and select one of the following sets of actions:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Select <strong>Use default database stored on your cluster</strong> to deploy a MariaDB database in your project.</p>\n</li>\n<li>\n<p>Select <strong>Connect to external MySQL database</strong> to add a new connection to an external database that your pipeline server can access.</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Host</strong> field, enter the database&#8217;s host name.</p>\n</li>\n<li>\n<p>In the <strong>Port</strong> field, enter the database&#8217;s port.</p>\n</li>\n<li>\n<p>In the <strong>Username</strong> field, enter the default user name that is connected to the database.</p>\n</li>\n<li>\n<p>In the <strong>Password</strong> field, enter the password for the default user account.</p>\n</li>\n<li>\n<p>In the <strong>Database</strong> field, enter the database name.</p>\n</li>\n</ol>\n</div>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Click <strong>Configure pipeline server</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>In the <strong>Pipelines</strong> tab for the project:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>The <strong>Import pipeline</strong> button is available.</p>\n</li>\n<li>\n<p>When you click the action menu (<strong>&#8942;</strong>) and then click <strong>View pipeline server configuration</strong>, the pipeline server details are displayed.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"defining-a-pipeline_nb-server\">Defining a pipeline</h4>\n<div class=\"paragraph _abstract\">\n<p>The Kubeflow Pipelines SDK enables you to define end-to-end machine learning and data pipelines. Use the latest Kubeflow Pipelines 2.0 SDK to build your data science pipeline in Python code. After you have built your pipeline, use the SDK to compile it into an Intermediate Representation (IR) YAML file. After defining the pipeline, you can import the YAML file to the Open Data Hub dashboard to enable you to configure its execution settings.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can also use the Elyra JupyterLab extension to create and run data science pipelines within JupyterLab. For more information about the Elyra JupyterLab extension, see <a href=\"https://elyra.readthedocs.io/en/stable/getting_started/overview.html\">Elyra Documentation</a>.</p>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://www.kubeflow.org/docs/components/pipelines/v2/\">Kubeflow Pipelines 2.0 Documentation</a></p>\n</li>\n<li>\n<p><a href=\"https://elyra.readthedocs.io/en/stable/getting_started/overview.html\">Elyra Documentation</a></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"importing-a-data-science-pipeline_nb-server\">Importing a data science pipeline</h4>\n<div class=\"paragraph _abstract\">\n<p>To help you begin working with data science pipelines in Open Data Hub, you can import a YAML file containing your pipeline&#8217;s code to an active pipeline server, or you can import the YAML file from a URL. This file contains a Kubeflow pipeline compiled by using the Kubeflow compiler. After you have imported the pipeline to a pipeline server, you can execute the pipeline by creating a pipeline run.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have compiled your pipeline with the Kubeflow compiler and you have access to the resulting YAML file.</p>\n</li>\n<li>\n<p>If you are uploading your pipeline from a URL, the URL is publicly accessible.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Pipelines</strong> page, select the <strong>project</strong> that you want to import a pipeline to.</p>\n</li>\n<li>\n<p>Click <strong>Import pipeline</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Import pipeline</strong> dialog, enter the details for the pipeline that you are importing.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Pipeline name</strong> field, enter a name for the pipeline that you are importing.</p>\n</li>\n<li>\n<p>In the <strong>Pipeline description</strong> field, enter a description for the pipeline that you are importing.</p>\n</li>\n<li>\n<p>Select where you want to import your pipeline from by performing one of the following actions:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Select <strong>Upload a file</strong> to upload your pipeline from your local machine&#8217;s file system. Import your pipeline by clicking <strong>upload</strong> or by dragging and dropping a file.</p>\n</li>\n<li>\n<p>Select <strong>Import by url</strong> to upload your pipeline from a URL and then enter the URL into the text box.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Click <strong>Import pipeline</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline that you imported appears on the <strong>Pipelines</strong> page and on the <strong>Pipelines</strong> tab on the project details page.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_configuring_access_to_data_science_projects\">Configuring access to data science projects</h3>\n<div class=\"sect3\">\n<h4 id=\"configuring-access-to-data-science-projects_nb-server\">Configuring access to data science projects</h4>\n<div class=\"paragraph _abstract\">\n<p>To enable you to work collaboratively on your data science projects with other users, you can share access to your project. After creating your project, you can then set the appropriate access permissions from the Open Data Hub user interface.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can assign the following access permission levels to your data science projects:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Admin - Users can modify all areas of a project, including its details (project name and description), components, and access permissions.</p>\n</li>\n<li>\n<p>Edit - Users can modify a project&#8217;s components, such as its workbench, but they cannot edit a project&#8217;s access permissions or its details (project name and description).</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"sharing-access-to-a-data-science-project_nb-server\">Sharing access to a data science project</h4>\n<div class=\"paragraph _abstract\">\n<p>To enable your organization to work collaboratively, you can share access to your data science project with other users and groups.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the list of data science projects, click the name of the data science project that you want to share access to.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Permissions</strong> tab.</p>\n<div class=\"paragraph\">\n<p>The <strong>Permissions</strong> page for the project opens.</p>\n</div>\n</li>\n<li>\n<p>Provide one or more users with access to the project.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Users</strong> section, click <strong>Add user</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Name</strong> field, enter the user name of the user whom you want to provide access to the project.</p>\n</li>\n<li>\n<p>From the <strong>Permissions</strong> list, select one of the following access permission levels:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Admin: Users with this access level can edit project details and manage access to the project.</p>\n</li>\n<li>\n<p>Edit: Users with this access level can view and edit project components, such as its workbenches, data connections, and storage.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>To confirm your entry, click <strong>Confirm</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhoai-confirm-entry-icon.png\" alt=\"The Confirm icon\"></span>).</p>\n</li>\n<li>\n<p>Optional: To add an additional user, click <strong>Add user</strong> and repeat the process.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Provide one or more OpenShift groups with access to the project.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Groups</strong> section, click <strong>Add group</strong>.</p>\n</li>\n<li>\n<p>From the <strong>Name</strong> list, select a group to provide access to the project.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you do not have <code>cluster-admin</code> permissions, the <strong>Name</strong> list is not visible. Instead, an input field is displayed enabling you to configure group permissions.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>From the <strong>Permissions</strong> list, select one of the following access permission levels:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Admin: Groups with this access permission level can edit project details and manage access to the project.</p>\n</li>\n<li>\n<p>Edit: Groups with this access permission level can view and edit project components, such as its workbenches, data connections, and storage.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>To confirm your entry, click <strong>Confirm</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhoai-confirm-entry-icon.png\" alt=\"The Confirm icon\"></span>).</p>\n</li>\n<li>\n<p>Optional: To add an additional group, click <strong>Add group</strong> and repeat the process.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Users to whom you provided access to the project can perform only the actions permitted by their access permission level.</p>\n</li>\n<li>\n<p>The <strong>Users</strong> and <strong>Groups</strong> sections on the <strong>Permissions</strong> tab show the respective users and groups that you provided with access to the project.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"updating-access-to-a-data-science-project_nb-server\">Updating access to a data science project</h4>\n<div class=\"paragraph _abstract\">\n<p>To change the level of collaboration on your data science project, you can update the access permissions of users and groups who have access to your project.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project.</p>\n</li>\n<li>\n<p>You have previously shared access to your project with other users or groups.</p>\n</li>\n<li>\n<p>You have administrator permissions or you are the project owner.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to change the access permissions of.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Permissions</strong> tab.</p>\n<div class=\"paragraph\">\n<p>The <strong>Permissions</strong> page for the project opens.</p>\n</div>\n</li>\n<li>\n<p>Update the user access permissions to the project.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Name</strong> field, update the user name of the user whom you want to provide access to the project.</p>\n</li>\n<li>\n<p>From the <strong>Permissions</strong> list, update the user access permissions by selecting one of the following:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Admin: Users with this access level can edit project details and manage access to the project.</p>\n</li>\n<li>\n<p>Edit: Users with this access level can view and edit project components, such as its workbenches, data connections, and storage.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>To confirm the update to the entry, click <strong>Confirm</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhoai-confirm-entry-icon.png\" alt=\"The Confirm icon\"></span>).</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Update the OpenShift groups access permissions to the project.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>From the <strong>Name</strong> list, update the group that has access to the project by selecting another group from the list.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you do not have <code>cluster-admin</code> permissions, the <strong>Name</strong> list is not visible. Instead, you can configure group permissions in the input field that appears.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>From the <strong>Permissions</strong> list, update the group access permissions by selecting one of the following:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Admin: Groups with this access permission level can edit project details and manage access to the project.</p>\n</li>\n<li>\n<p>Edit: Groups with this access permission level can view and edit project components, such as its workbenches, data connections, and storage.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>To confirm the update to the entry, click <strong>Confirm</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhoai-confirm-entry-icon.png\" alt=\"The Confirm icon\"></span>).</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The <strong>Users</strong> and <strong>Groups</strong> sections on the <strong>Permissions</strong> tab show the respective users and groups whose project access permissions you changed.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"removing-access-to-a-data-science-project_nb-server\">Removing access to a data science project</h4>\n<div class=\"paragraph _abstract\">\n<p>If you no longer want to work collaboratively on your data science project, you can restrict access to your project by removing users and groups that you previously provided access to your project.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project.</p>\n</li>\n<li>\n<p>You have previously shared access to your project with other users or groups.</p>\n</li>\n<li>\n<p>You have administrator permissions or you are the project owner.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to change the access permissions of.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Permissions</strong> tab.</p>\n<div class=\"paragraph\">\n<p>The <strong>Permissions</strong> page for the project opens.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the user or group whose access permissions you want to revoke and click <strong>Delete</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Users whose access you have revoked can no longer perform the actions that were permitted by their access permission level.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-python-packages-installed-on-your-notebook-server_nb-server\">Viewing Python packages installed on your notebook server</h3>\n<div class=\"paragraph _abstract\">\n<p>You can check which Python packages are installed on your notebook server and which version of the package you have by running the <code>pip</code> tool in a notebook cell.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>Log in to Jupyter and open a notebook.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Enter the following in a new cell in your notebook:</p>\n<div class=\"listingblock execute\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>!pip list</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Run the cell.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The output shows an alphabetical list of all installed Python packages and their versions. For example, if you use this command immediately after creating a notebook server that uses the <strong>Minimal</strong> image, the first packages shown are similar to the following:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>Package                           Version\n--------------------------------- ----------\naiohttp                           3.7.3\nalembic                           1.5.2\nappdirs                           1.4.4\nargo-workflows                    3.6.1\nargon2-cffi                       20.1.0\nasync-generator                   1.10\nasync-timeout                     3.0.1\nattrdict                          2.0.1\nattrs                             20.3.0\nbackcall                          0.2.0</code></pre>\n</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"installing-python-packages-on-your-notebook-server_nb-server\">Installing Python packages on your notebook server</h3>\n<div class=\"paragraph _abstract\">\n<p>You can install Python packages that are not part of the default notebook server image by adding the package and the version to a <code>requirements.txt</code> file and then running the <code>pip install</code> command in a notebook cell.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nYou can also install packages directly, but using a <code>requirements.txt</code> file so that the packages stated in the file can be easily re-used across different notebooks is recommended. In addition, using a <code>requirements.txt</code> file is also useful when using a S2I build to deploy a model.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>Log in to Jupyter and open a notebook.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Create a new text file using one of the following methods:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Click <strong>+</strong> to open a new launcher and click <strong>Text file</strong>.</p>\n</li>\n<li>\n<p>Click <strong>File</strong> &#8594; <strong>New</strong> &#8594; <strong>Text File</strong>.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Rename the text file to <code>requirements.txt</code>.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Right-click on the name of the file and click <strong>Rename Text</strong>. The <strong>Rename File</strong> dialog opens.</p>\n</li>\n<li>\n<p>Enter <code>requirements.txt</code> in the <strong>New Name</strong> field and click <strong>Rename</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Add the packages to install to the <code>requirements.txt</code> file.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>altair</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>You can specify the exact version to install by using the <code>==</code> (equal to) operator, for example:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>altair==4.1.0</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Specifying exact package versions to enhance the stability of your notebook server over time is recommended. New package versions can introduce undesirable or unexpected changes in your environment&#8217;s behavior.\nTo install multiple packages at the same time, place each package on a separate line.</p>\n</div>\n</li>\n<li>\n<p>Install the packages in <code>requirements.txt</code> to your server using a notebook cell.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Create a new cell in your notebook and enter the following command:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>!pip install -r requirements.txt</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Run the cell by pressing Shift and Enter.</p>\n</li>\n</ol>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>This command installs the package on your notebook server, but you must still run the <code>import</code> directive in a code cell to use the package in your code.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>import altair</pre>\n</div>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Confirm that the packages in <code>requirements.txt</code> appear in the list of packages installed on the notebook server.\nSee <a href=\"https://opendatahub.io/docs/working-on-data-science-projects#viewing-python-packages-installed-on-your-notebook-server_nb-server\">Viewing Python packages installed on your notebook server</a> for details.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"updating-notebook-server-settings-by-restarting-your-server_nb-server\">Updating notebook server settings by restarting your server</h3>\n<div class=\"paragraph _abstract\">\n<p>You can update the settings on your notebook server by stopping and relaunching the notebook server. For example, if your server runs out of memory, you can restart the server to make the container size larger.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>A running notebook server.</p>\n</li>\n<li>\n<p>Log in to Jupyter.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Click <strong>File</strong> &#8594; <strong>Hub Control Panel</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Notebook server control panel</strong> opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Stop notebook server</strong> button.</p>\n<div class=\"paragraph\">\n<p>The <strong>Stop server</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Click <strong>Stop server</strong> to confirm your decision.</p>\n<div class=\"paragraph\">\n<p>The <strong>Start a notebook server</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Update the relevant notebook server settings and click <strong>Start server</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The notebook server starts and contains your updated settings.</p>\n</li>\n</ul>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://opendatahub.io/docs/getting-started-with-open-data-hub/#launching-jupyter-and-starting-a-notebook-server_get-started\">Launching Jupyter and starting a notebook server</a></p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"working-with-data-science-pipelines_ds-pipelines\">Working with data science pipelines</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>As a data scientist, you can enhance your data science projects on Open Data Hub by building portable machine learning (ML) workflows with data science pipelines, using Docker containers. This enables you to standardize and automate machine learning workflows to enable you to develop and deploy your data science models.</p>\n</div>\n<div class=\"paragraph\">\n<p>For example, the steps in a machine learning workflow might include items such as data extraction, data processing, feature extraction, model training, model validation, and model serving. Automating these activities enables your organization to develop a continuous process of retraining and updating a model based on newly received data. This can help resolve challenges related to building an integrated machine learning deployment and continuously operating it in production.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can also use the Elyra JupyterLab extension to create and run data science pipelines within JupyterLab. For more information, see <a href=\"https://opendatahub.io/docs/working-on-data-science-projects/#_working_with_pipelines_in_jupyterlab\">Working with pipelines in JupyterLab</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>From Open Data Hub version 2.10.0, data science pipelines are based on <a href=\"https://www.kubeflow.org/docs/components/pipelines/v2/\">KubeFlow Pipelines (KFP) version 2.0</a>. For more information, see <a href=\"https://opendatahub.io/docs/working-on-data-science-projects/#enabling-data-science-pipelines-2_ds-pipelines\">Enabling Data Science Pipelines 2.0</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>A data science pipeline in Open Data Hub consists of the following components:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Pipeline server: A server that is attached to your data science project and hosts your data science pipeline.</p>\n</li>\n<li>\n<p>Pipeline: A pipeline defines the configuration of your machine learning workflow and the relationship between each component in the workflow.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Pipeline code: A definition of your pipeline in a YAML file.</p>\n</li>\n<li>\n<p>Pipeline graph: A graphical illustration of the steps executed in a pipeline run and the relationship between them.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Pipeline run: An execution of your pipeline.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Active run: A pipeline run that is in its execution phase, or is stopped.</p>\n</li>\n<li>\n<p>Scheduled run: A pipeline run scheduled to execute at least once.</p>\n</li>\n<li>\n<p>Archived run: A pipeline run that resides in the run archive and is no longer required.</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>This feature is based on Kubeflow Pipelines 2.0. Use the latest Kubeflow Pipelines 2.0 SDK to build your data science pipeline in Python code. After you have built your pipeline, use the SDK to compile it into an Intermediate Representation (IR) YAML file. The Open Data Hub user interface enables you to track and manage pipelines and pipeline runs.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can store your pipeline artifacts in an S3-compatible object storage bucket so that you do not consume local storage. To do this, you must first configure write access to your S3 bucket on your storage account.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"enabling-data-science-pipelines-2_ds-pipelines\">Enabling Data Science Pipelines 2.0</h3>\n<div class=\"paragraph\">\n<p>From Open Data Hub version 2.10.0, data science pipelines are based on <a href=\"https://www.kubeflow.org/docs/components/pipelines/v2/\">KubeFlow Pipelines (KFP) version 2.0</a>. DSP 2.0 is enabled and deployed by default in Open Data Hub.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>The <code>PipelineConf</code> class is deprecated, and there is no KFP 2.0 equivalent.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Data Science Pipelines (DSP) 2.0 contains an installation of Argo Workflows. Open Data Hub does not support direct customer usage of this installation of Argo Workflows.</p>\n</div>\n<div class=\"paragraph\">\n<p>To install or upgrade to Open Data Hub 2.10.0 with DSP, ensure that your cluster does not have an existing installation of Argo Workflows that is not installed by Open Data Hub.</p>\n</div>\n<div class=\"paragraph\">\n<p>Argo Workflows resources that are created by Open Data Hub have the following labels in the OpenShift Console under <strong>Administration &gt; CustomResourceDefinitions</strong>, in the <code>argoproj.io</code> group:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code> labels:\n    app.kubernetes.io/part-of: data-science-pipelines-operator\n    app.opendatahub.io/data-science-pipelines-operator: 'true'</code></pre>\n</div>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_installing_open_data_hub_with_dsp_2_0\">Installing Open Data Hub with DSP 2.0</h4>\n<div class=\"paragraph\">\n<p>To install Open Data Hub 2.10.0, ensure that there is no installation of Argo Workflows that is not installed by DSP on your cluster, and follow the installation steps described in <a href=\"https://opendatahub.io/docs/installing-open-data-hub/\">Installing Open Data Hub</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>If there is an existing installation of Argo Workflows that is not installed by DSP on your cluster, DSP will be disabled after you install Open Data Hub 2.10.0 with DSP.</p>\n</div>\n<div class=\"paragraph\">\n<p>To enable data science pipelines, remove the separate installation of Argo Workflows from your cluster. Data Science Pipelines will be enabled automatically.</p>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_upgrading_to_dsp_2_0\">Upgrading to DSP 2.0</h4>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>After you upgrade to Open Data Hub 2.10.0, pipelines created with DSP 1.0 will continue to run, but will be inaccessible from the Open Data Hub dashboard. We recommend that current DSP users do not upgrade to Open Data Hub 2.10.0 until you are ready to migrate to the new pipelines solution.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>To upgrade to Open Data Hub 2.10.0 with DSP 2.0, ensure that there is no installation of Argo Workflows that is not installed by DSP on your cluster, and follow the upgrade steps described in <a href=\"https://opendatahub.io/docs/upgrading-open-data-hub/\">Upgrading Open Data Hub</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>If you upgrade to Open Data Hub 2.10.0 with DSP enabled and an Argo Workflows installation that is not installed by DSP exists on your cluster, Open Data Hub components will not be upgraded. To complete the component upgrade, disable DSP or remove the separate installation of Argo Workflows. The component upgrade will complete automatically.</p>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_managing_data_science_pipelines\">Managing data science pipelines</h3>\n<div class=\"sect3\">\n<h4 id=\"configuring-a-pipeline-server_ds-pipelines\">Configuring a pipeline server</h4>\n<div class=\"paragraph _abstract\">\n<p>Before you can successfully create a pipeline in Open Data Hub, you must configure a pipeline server. This task includes configuring where your pipeline artifacts and data are stored.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>You are not required to specify any storage directories when configuring a data connection for your pipeline server. When you import a pipeline, the <code>/pipelines</code> folder is created in the <code>root</code> folder of the bucket, containing a YAML file for the pipeline. If you upload a new version of the same pipeline, a new YAML file with a different ID is added to the <code>/pipelines</code> folder.</p>\n</div>\n<div class=\"paragraph\">\n<p>When you run a pipeline, the artifacts are stored in the <code>/pipeline-name</code> folder in the <code>root</code> folder of the bucket.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you use an external MySQL database and upgrade to Open Data Hub 2.10.0, the database is migrated to DSP 2.0 format, making it incompatible with earlier versions of Open Data Hub.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that you can add a pipeline server to.</p>\n</li>\n<li>\n<p>You have an existing S3-compatible object storage bucket and you have configured write access to your S3 bucket on your storage account.</p>\n</li>\n<li>\n<p>If you are configuring a pipeline server with an external MySQL database, your database must use MySQL version 5.x.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to configure a pipeline server for.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Pipelines</strong> tab.</p>\n</li>\n<li>\n<p>Click <strong>Configure pipeline server</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Configure pipeline server</strong> dialog appears.</p>\n</div>\n</li>\n<li>\n<p>In the <strong>Object storage connection</strong> section, provide values for the mandatory fields:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Access key</strong> field, enter the access key ID for the S3-compatible object storage provider.</p>\n</li>\n<li>\n<p>In the <strong>Secret key</strong> field, enter the secret access key for the S3-compatible object storage account that you specified.</p>\n</li>\n<li>\n<p>In the <strong>Endpoint</strong> field, enter the endpoint of your S3-compatible object storage bucket.</p>\n</li>\n<li>\n<p>In the <strong>Region</strong> field, enter the default region of your S3-compatible object storage account.</p>\n</li>\n<li>\n<p>In the <strong>Bucket</strong> field, enter the name of your S3-compatible object storage bucket.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you specify incorrect data connection settings, you cannot update these settings on the same pipeline server. Therefore, you must delete the pipeline server and configure another one.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>In the <strong>Database</strong> section, click <strong>Show advanced database options</strong> to specify the database to store your pipeline data and select one of the following sets of actions:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Select <strong>Use default database stored on your cluster</strong> to deploy a MariaDB database in your project.</p>\n</li>\n<li>\n<p>Select <strong>Connect to external MySQL database</strong> to add a new connection to an external database that your pipeline server can access.</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Host</strong> field, enter the database&#8217;s host name.</p>\n</li>\n<li>\n<p>In the <strong>Port</strong> field, enter the database&#8217;s port.</p>\n</li>\n<li>\n<p>In the <strong>Username</strong> field, enter the default user name that is connected to the database.</p>\n</li>\n<li>\n<p>In the <strong>Password</strong> field, enter the password for the default user account.</p>\n</li>\n<li>\n<p>In the <strong>Database</strong> field, enter the database name.</p>\n</li>\n</ol>\n</div>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Click <strong>Configure pipeline server</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>In the <strong>Pipelines</strong> tab for the project:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>The <strong>Import pipeline</strong> button is available.</p>\n</li>\n<li>\n<p>When you click the action menu (<strong>&#8942;</strong>) and then click <strong>View pipeline server configuration</strong>, the pipeline server details are displayed.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"defining-a-pipeline_ds-pipelines\">Defining a pipeline</h4>\n<div class=\"paragraph _abstract\">\n<p>The Kubeflow Pipelines SDK enables you to define end-to-end machine learning and data pipelines. Use the latest Kubeflow Pipelines 2.0 SDK to build your data science pipeline in Python code. After you have built your pipeline, use the SDK to compile it into an Intermediate Representation (IR) YAML file. After defining the pipeline, you can import the YAML file to the Open Data Hub dashboard to enable you to configure its execution settings.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can also use the Elyra JupyterLab extension to create and run data science pipelines within JupyterLab. For more information about the Elyra JupyterLab extension, see <a href=\"https://elyra.readthedocs.io/en/stable/getting_started/overview.html\">Elyra Documentation</a>.</p>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://www.kubeflow.org/docs/components/pipelines/v2/\">Kubeflow Pipelines 2.0 Documentation</a></p>\n</li>\n<li>\n<p><a href=\"https://elyra.readthedocs.io/en/stable/getting_started/overview.html\">Elyra Documentation</a></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"importing-a-data-science-pipeline_ds-pipelines\">Importing a data science pipeline</h4>\n<div class=\"paragraph _abstract\">\n<p>To help you begin working with data science pipelines in Open Data Hub, you can import a YAML file containing your pipeline&#8217;s code to an active pipeline server, or you can import the YAML file from a URL. This file contains a Kubeflow pipeline compiled by using the Kubeflow compiler. After you have imported the pipeline to a pipeline server, you can execute the pipeline by creating a pipeline run.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have compiled your pipeline with the Kubeflow compiler and you have access to the resulting YAML file.</p>\n</li>\n<li>\n<p>If you are uploading your pipeline from a URL, the URL is publicly accessible.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Pipelines</strong> page, select the <strong>project</strong> that you want to import a pipeline to.</p>\n</li>\n<li>\n<p>Click <strong>Import pipeline</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Import pipeline</strong> dialog, enter the details for the pipeline that you are importing.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Pipeline name</strong> field, enter a name for the pipeline that you are importing.</p>\n</li>\n<li>\n<p>In the <strong>Pipeline description</strong> field, enter a description for the pipeline that you are importing.</p>\n</li>\n<li>\n<p>Select where you want to import your pipeline from by performing one of the following actions:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Select <strong>Upload a file</strong> to upload your pipeline from your local machine&#8217;s file system. Import your pipeline by clicking <strong>upload</strong> or by dragging and dropping a file.</p>\n</li>\n<li>\n<p>Select <strong>Import by url</strong> to upload your pipeline from a URL and then enter the URL into the text box.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Click <strong>Import pipeline</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline that you imported appears on the <strong>Pipelines</strong> page and on the <strong>Pipelines</strong> tab on the project details page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"downloading-a-data-science-pipeline-version_ds-pipelines\">Downloading a data science pipeline version</h4>\n<div class=\"paragraph _abstract\">\n<p>To make further changes to a data science pipeline version that you previously uploaded to Open Data Hub, you can download pipeline version code from the user interface.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have created and imported a pipeline to an active pipeline server that is available to download.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Pipelines</strong> page, select the <strong>project</strong> that contains the version that you want to download.</p>\n</li>\n<li>\n<p>For a pipeline that contains the version that you want to download, click <strong>Expand</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhoai-expand-icon.png\" alt=\"rhoai expand icon\"></span>).</p>\n</li>\n<li>\n<p>Click the pipeline version that you want to download.</p>\n</li>\n<li>\n<p>On the <strong>Pipeline details</strong> page, click the <strong>YAML</strong> tab.</p>\n</li>\n<li>\n<p>Click the <strong>Download</strong> button (<span class=\"image\"><img src=\"/static/docs/images/rhoai-download-icon.png\" alt=\"rhoai download icon\"></span>) to download the YAML file containing your pipeline version code to your local machine.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline version code downloads to your browser&#8217;s default directory for downloaded files.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deleting-a-data-science-pipeline_ds-pipelines\">Deleting a data science pipeline</h4>\n<div class=\"paragraph _abstract\">\n<p>If you no longer require access to your data science pipeline on the dashboard, you can delete it so that it does not appear on the <strong>Data Science Pipelines</strong> page.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>There are active pipelines available on the <strong>Pipelines</strong> page.</p>\n</li>\n<li>\n<p>The pipeline that you want to delete does not contain any pipeline versions.</p>\n</li>\n<li>\n<p>The pipeline that you want to delete does not contain any pipeline versions. For more information, see <a href=\"https://opendatahub.io/docs/working-on-data-science-projects/#_managing_data_science_pipelines\">Deleting a pipeline version</a>.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Pipelines</strong> page, select the project that contains the pipeline that you want to delete from the <strong>Project</strong> list.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the pipeline that you want to delete and click <strong>Delete pipeline</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Delete pipeline</strong> dialog, enter the pipeline name in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete pipeline</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The data science pipeline that you deleted no longer appears on the <strong>Pipelines</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deleting-a-pipeline-server_ds-pipelines\">Deleting a pipeline server</h4>\n<div class=\"paragraph _abstract\">\n<p>After you have finished running your data science pipelines, you can delete the pipeline server. Deleting a pipeline server automatically deletes all of its associated pipelines, pipeline versions, and runs. If your pipeline data is stored in a database, the database is also deleted along with its meta-data. In addition, after deleting a pipeline server, you cannot create new pipelines or pipeline runs until you create another pipeline server.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Pipelines</strong> page, select the <strong>project</strong> for the pipeline server that you want to delete.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline server actions</strong> list, select <strong>Delete pipeline server</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Delete pipeline server</strong> dialog, enter the pipeline server&#8217;s name in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Pipelines previously assigned to the deleted pipeline server no longer appears on the <strong>Pipelines</strong> page for the relevant data science project.</p>\n</li>\n<li>\n<p>Pipeline runs previously assigned to the deleted pipeline server no longer appears on the <strong>Runs</strong> page for the relevant data science project.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"viewing-the-details-of-a-pipeline-server_ds-pipelines\">Viewing the details of a pipeline server</h4>\n<div class=\"paragraph _abstract\">\n<p>You can view the details of pipeline servers configured in Open Data Hub, such as the pipeline&#8217;s data connection details and where its data is stored.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>You have previously created a data science project that contains an active and available pipeline server.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Pipelines</strong> page opens, select the <strong>project</strong> whose pipeline server you want to view.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline server actions</strong> list, select <strong>View pipeline server configuration</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>You can view the relevant pipeline server details in the <strong>View pipeline server</strong> dialog.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"viewing-existing-pipelines_ds-pipelines\">Viewing existing pipelines</h4>\n<div class=\"paragraph _abstract\">\n<p>You can view the details of pipelines that you have imported to Open Data Hub, such as the pipeline&#8217;s last run, when it was created, the pipeline&#8217;s executed runs, and details of any associated pipeline versions.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>Existing pipelines are available.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Pipelines</strong> page, select the relevant <strong>project</strong> for the pipelines you want to view.</p>\n</li>\n<li>\n<p>Study the pipelines on the list.</p>\n</li>\n<li>\n<p>Optional: Click <strong>Expand</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhoai-expand-icon.png\" alt=\"rhoai expand icon\"></span>) on the relevant row to view details of any pipeline versions associated with the pipeline.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>A list of previously created data science pipelines appears on the <strong>Pipelines</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_managing_pipeline_runs\">Managing pipeline runs</h3>\n<div class=\"sect3\">\n<h4 id=\"overview-of-pipeline-runs_ds-pipelines\">Overview of pipeline runs</h4>\n<div class=\"paragraph _abstract\">\n<p>A pipeline run is a single execution of a data science pipeline. As data scientist, you can use Open Data Hub to define, manage, and track executions of a data science pipeline. You can view a record of previously executed, scheduled, and archived runs from the <strong>Runs</strong> page in the Open Data Hub user interface.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can optimize your use of pipeline runs for portability. You can clone your pipeline runs to reproduce and scale them accordingly, or archive them when you want to retain a record of their execution, but no longer require them. You can delete archived runs that you no longer want to retain, or you can restore them to their former state.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can execute a run once, that is, immediately after its creation, or on a recurring basis. Recurring runs consist of a copy of a pipeline with all of its parameter values and a run trigger. A run trigger indicates when a recurring run executes. You can define the following run triggers:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Periodic: used for scheduling runs to execute in intervals.</p>\n</li>\n<li>\n<p>Cron: used for scheduling runs as a cron job.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>You can also configure multiple instances of the same run to execute concurrently, from a range of one to ten. When executed, you can track the run&#8217;s progress from the run <strong>Details</strong> page on the Open Data Hub user interface. From here, you can view the run&#8217;s graph, and output artifacts. A pipeline run can be in one of the following states:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Scheduled run: A pipeline run scheduled to execute at least once.</p>\n</li>\n<li>\n<p>Active run: A pipeline run that is in its execution phase, or is stopped.</p>\n</li>\n<li>\n<p>Archived run: A pipeline run that resides in the run archive and is no longer required.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>You can use catch up runs to ensure your pipeline runs do not permanently fall behind schedule when paused. For example, if you re-enable a paused recurring run, the run scheduler backfills each missed run interval. If you disable catch up runs, and you have a scheduled run interval ready to execute, the run scheduler only schedules the run execution for the latest run interval. Catch up runs are enabled by default. However, if your pipeline handles backfill internally, Red Hat recommends that you disable catch up runs to avoid duplicate backfill.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can review and analyze logs for each step in an active pipeline run. With the log viewer, you can search for specific log messages, view the log for each step, and download the step logs to your local machine.</p>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"viewing-active-pipeline-runs_ds-pipelines\">Viewing active pipeline runs</h4>\n<div class=\"paragraph _abstract\">\n<p>You can view a list of pipeline runs that were previously executed in Open Data Hub. From this list, you can view details relating to your pipeline runs, such as the pipeline version that the run belongs to, along with the run status, duration, and execution start time.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and has a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have previously executed a pipeline run.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, select the <strong>project</strong> for the active pipeline runs that you want to view.</p>\n</li>\n<li>\n<p>On the <strong>Run details</strong> page, click the <strong>Active</strong> tab.</p>\n<div class=\"paragraph\">\n<p>After a run has completed its execution, the run&#8217;s status appears in the <strong>Status</strong> column in the table, indicating whether the run has succeeded or failed.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>A list of active runs appears in the <strong>Active</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"executing-a-pipeline-run_ds-pipelines\">Executing a pipeline run</h4>\n<div class=\"paragraph _abstract\">\n<p>You can instantiate a single execution of a pipeline by creating an active pipeline run that executes immediately after creation.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, select the <strong>project</strong> that you want to create a run for.</p>\n</li>\n<li>\n<p>Click <strong>Create run</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Create run</strong> page, configure the run:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Name</strong> field, enter a name for the run.</p>\n</li>\n<li>\n<p>In the <strong>Description</strong> field, enter a description for the run.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline</strong> list, select the pipeline that you want to create a run for. Alternatively, to create a new pipeline, click <strong>Create new pipeline</strong> and complete the relevant fields in the <strong>Import pipeline</strong> dialog.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline version</strong> list, select the pipeline version to create a run for. Alternatively, to upload a new version, click <strong>Upload new version</strong> and complete the relevant fields in the <strong>Upload new version</strong> dialog.</p>\n</li>\n<li>\n<p>Configure the input parameters for the run by selecting the parameters from the list.</p>\n</li>\n<li>\n<p>Click <strong>Create run</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline run that you created appears in the <strong>Active</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"stopping-an-active-pipeline-run_ds-pipelines\">Stopping an active pipeline run</h4>\n<div class=\"paragraph _abstract\">\n<p>If you no longer require an active pipeline run to continue executing, you can stop the run before its defined end date.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>There is a previously created data science project available that contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>An active pipeline run is currently executing.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, select the <strong>project</strong> that contains the pipeline whose active run you want to stop.</p>\n</li>\n<li>\n<p>In the <strong>Active</strong> tab, click the action menu (<strong>&#8942;</strong>) beside the active run that you want to delete and click <strong>Stop</strong>.</p>\n<div class=\"paragraph\">\n<p>There might be a short delay while the run stops.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>In the list of active runs, the status of the run is \"stopped\".</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"duplicating-an-active-pipeline-run_ds-pipelines\">Duplicating an active pipeline run</h4>\n<div class=\"paragraph _abstract\">\n<p>To make it easier to quickly execute pipeline runs with the same configuration, you can duplicate them.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>An active run is available to duplicate in the <strong>Active</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, select the <strong>project</strong> that has the pipeline run that you want to duplicate.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the relevant active run and click <strong>Duplicate</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Duplicate run</strong> page, configure the duplicate run:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Name</strong> field, enter a name for the duplicate run.</p>\n</li>\n<li>\n<p>In the <strong>Description</strong> field, enter a description for the duplicate run.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline</strong> list, select the pipeline to contain the duplicate run.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline version</strong> list, select the pipeline version to contain the duplicate run.</p>\n</li>\n<li>\n<p>In the <strong>Parameters</strong> section, configure the input parameters for the run that you are duplicating by selecting the appropriate parameters from the list.</p>\n</li>\n<li>\n<p>Click <strong>Create run</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The duplicate pipeline run appears in the <strong>Active</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"viewing-scheduled-pipeline-runs_ds-pipelines\">Viewing scheduled pipeline runs</h4>\n<div class=\"paragraph _abstract\">\n<p>You can view a list of pipeline runs that are scheduled for execution in Open Data Hub. From this list, you can view details relating to your pipeline runs, such as the pipeline version that the run belongs to. You can also view the run status, execution frequency, and schedule.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have scheduled a pipeline run that is available to view.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Runs</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project whose scheduled pipeline runs you want to view.</p>\n</li>\n<li>\n<p>Click the <strong>Schedules</strong> tab.</p>\n</li>\n<li>\n<p>Study the table showing a list of scheduled runs.</p>\n<div class=\"paragraph\">\n<p>After a run has been scheduled, the run&#8217;s status indicates whether the run is ready for execution or unavailable for execution. To change its execution availability, click the run&#8217;s Status icon.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>A list of scheduled runs appears in the <strong>Schedules</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"scheduling-a-pipeline-run-using-a-cron-job_ds-pipelines\">Scheduling a pipeline run using a cron job</h4>\n<div class=\"paragraph _abstract\">\n<p>You can use a cron job to schedule a pipeline run to execute at a specific time. Cron jobs are useful for creating periodic and recurring tasks, and can also schedule individual tasks for a specific time, such as if you want to schedule a run for a low activity period. To successfully execute runs in Open Data Hub, you must use the supported format. See <a href=\"https://pkg.go.dev/github.com/robfig/cron#hdr-CRON_Expression_Format\">Cron Expression Format</a> for more information.</p>\n</div>\n<div class=\"paragraph\">\n<p>The following examples show the correct format:</p>\n</div>\n<table class=\"tableblock frame-all grid-all stretch\">\n<colgroup>\n<col style=\"width: 50%;\">\n<col style=\"width: 50%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Run occurrence</th>\n<th class=\"tableblock halign-left valign-top\">Cron format</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Every five minutes</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">@every 5m</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Every 10 minutes</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">0 */10 * * * *</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Daily at 16:16 UTC</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">0 16 16 * * *</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Daily every quarter of the hour</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">0 0,15,30,45 * * * *</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">On Monday and Tuesday at 15:40 UTC</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">0 40 15 * * MON,TUE</p></td>\n</tr>\n</tbody>\n</table>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://pkg.go.dev/github.com/robfig/cron#hdr-CRON_Expression_Format\">Cron Expression Format</a></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"scheduling-a-pipeline-run_ds-pipelines\">Scheduling a pipeline run</h4>\n<div class=\"paragraph _abstract\">\n<p>To repeatedly run a pipeline, you can create a scheduled pipeline run.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Runs</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project that you want to schedule a run for.</p>\n</li>\n<li>\n<p>Click the <strong>Schedules</strong> tab.</p>\n</li>\n<li>\n<p>Click <strong>Schedule run</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Schedule run</strong> page, configure the run that you are scheduling:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Name</strong> field, enter a name for the run.</p>\n</li>\n<li>\n<p>In the <strong>Description</strong> field, enter a description for the run.</p>\n</li>\n<li>\n<p>From the <strong>Trigger type</strong> list, select one of the following options:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Select <strong>Periodic</strong> to specify an execution frequency. In the <strong>Run every</strong> field, enter a numerical value and select an execution frequency from the list.</p>\n</li>\n<li>\n<p>Select <strong>Cron</strong> to specify the execution schedule in <code>cron</code> format. This creates a cron job to execute the run. Click the <strong>Copy</strong> button (<span class=\"image\"><img src=\"/static/docs/images/osd-copy.png\" alt=\"osd copy\"></span>) to copy the cron job schedule to the clipboard. The field furthest to the left represents seconds. For more information about scheduling tasks using the supported <code>cron</code> format, see <a href=\"https://pkg.go.dev/github.com/robfig/cron#hdr-CRON_Expression_Format\">Cron Expression Format</a>.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>In the <strong>Maximum concurrent runs</strong> field, specify the number of runs that can execute concurrently, from a range of one to ten.</p>\n</li>\n<li>\n<p>For <strong>Start date</strong>, specify a start date for the run. Select a start date using the <strong>Calendar</strong> and the start time from the list of times.</p>\n</li>\n<li>\n<p>For <strong>End date</strong>, specify an end date for the run. Select an end date using the <strong>Calendar</strong> and the end time from the list of times.</p>\n</li>\n<li>\n<p>For <strong>Catch up</strong>, enable or disable catch up runs. You can use catch up runs to ensure your pipeline runs do not permanently fall behind schedule when paused. For example, if you re-enable a paused recurring run, the run scheduler backfills each missed run interval.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline</strong> list, select the pipeline that you want to create a run for. Alternatively, to create a new pipeline, click <strong>Create new pipeline</strong> and complete the relevant fields in the <strong>Import pipeline</strong> dialog.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline version</strong> list, select the pipeline version to create a run for. Alternatively, to upload a new version, click <strong>Upload new version</strong> and complete the relevant fields in the <strong>Upload new version</strong> dialog.</p>\n</li>\n<li>\n<p>Configure the input parameters for the run by selecting the parameters from the list.</p>\n</li>\n<li>\n<p>Click <strong>Schedule run</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline run that you created appears in the <strong>Schedules</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"duplicating-a-scheduled-pipeline-run_ds-pipelines\">Duplicating a scheduled pipeline run</h4>\n<div class=\"paragraph _abstract\">\n<p>To make it easier to schedule runs to execute as part of your pipeline configuration, you can duplicate existing scheduled runs.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>A scheduled run is available to duplicate in the <strong>Schedules</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, select the <strong>project</strong> that has the pipeline run that you want to duplicate.</p>\n</li>\n<li>\n<p>Click the <strong>Schedules</strong> tab.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the run that you want to duplicate and click <strong>Duplicate</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Duplicate schedule</strong> page, configure the duplicate run:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Name</strong> field, enter a name for the duplicate run.</p>\n</li>\n<li>\n<p>In the <strong>Description</strong> field, enter a description for the duplicate run.</p>\n</li>\n<li>\n<p>From the <strong>Trigger type</strong> list, select one of the following options:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Select <strong>Periodic</strong> to specify an execution frequency. In the <strong>Run every</strong> field, enter a numerical value and select an execution frequency from the list.</p>\n</li>\n<li>\n<p>Select <strong>Cron</strong> to specify the execution schedule in <code>cron</code> format. This creates a cron job to execute the run. Click the <strong>Copy</strong> button (<span class=\"image\"><img src=\"/static/docs/images/osd-copy.png\" alt=\"osd copy\"></span>) to copy the cron job schedule to the clipboard. The field furthest to the left represents seconds. For more information about scheduling tasks using the supported <code>cron</code> format, see <a href=\"https://pkg.go.dev/github.com/robfig/cron#hdr-CRON_Expression_Format\">Cron Expression Format</a>.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>For <strong>Maximum concurrent runs</strong>, specify the number of runs that can execute concurrently, from a range of one to ten.</p>\n</li>\n<li>\n<p>For <strong>Start date</strong>, specify a start date for the duplicate run. Select a start date using the <strong>Calendar</strong> and the start time from the list of times.</p>\n</li>\n<li>\n<p>For <strong>End date</strong>, specify an end date for the duplicate run. Select an end date using the <strong>Calendar</strong> and the end time from the list of times.</p>\n</li>\n<li>\n<p>For <strong>Catch up</strong>, enable or disable catch up runs. You can use catch up runs to ensure your pipeline runs do not permanently fall behind schedule when paused. For example, if you re-enable a paused recurring run, the run scheduler backfills each missed run interval.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline</strong> list, select the pipeline that you want to create a duplicate run for. Alternatively, to create a new pipeline, click <strong>Create new pipeline</strong> and complete the relevant fields in the <strong>Import pipeline</strong> dialog.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline version</strong> list, select the pipeline version to create a duplicate run for. Alternatively, to upload a new version, click <strong>Upload new version</strong> and complete the relevant fields in the <strong>Upload new version</strong> dialog.</p>\n</li>\n<li>\n<p>Configure the input parameters for the run by selecting the parameters from the list.</p>\n</li>\n<li>\n<p>Click <strong>Schedule run</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline run that you duplicated appears in the <strong>Schedules</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deleting-a-scheduled-pipeline-run_ds-pipelines\">Deleting a scheduled pipeline run</h4>\n<div class=\"paragraph _abstract\">\n<p>To discard pipeline runs that you previously scheduled, but no longer require, you can delete them so that they do not appear on the <strong>Runs</strong> page.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have previously scheduled a run that is available to delete.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Runs</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project that contains the pipeline whose scheduled run you want to delete.</p>\n<div class=\"paragraph\">\n<p>The page refreshes to show the pipeline&#8217;s scheduled runs on the <strong>Schedules</strong> tab.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the scheduled run that you want to delete and click <strong>Delete</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Delete schedule</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter the run&#8217;s name in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The run that you deleted no longer appears on the <strong>Schedules</strong> tab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"viewing-the-details-of-a-pipeline-run_ds-pipelines\">Viewing the details of a pipeline run</h4>\n<div class=\"paragraph _abstract\">\n<p>To gain a clearer understanding of your pipeline runs, you can view the details of a previously triggered pipeline run, such as its graph, execution details, and run output.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have previously triggered a pipeline run.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Pipelines</strong> page, select the <strong>project</strong> that you want to view run details for.</p>\n</li>\n<li>\n<p>For a pipeline that you want to view run details for, click <strong>Expand</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhoai-expand-icon.png\" alt=\"rhoai expand icon\"></span>).</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) for the pipeline version and then click <strong>View runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, click the name of the run that you want to view the details of.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>On the <strong>Run details</strong> page, you can view the run&#8217;s graph, execution details, input parameters, step logs, and run output.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"viewing-archived-pipeline-runs_ds-pipelines\">Viewing archived pipeline runs</h4>\n<div class=\"paragraph _abstract\">\n<p>You can view a list of pipeline runs that you have archived. You can view details for your archived pipeline runs, such as the pipeline version, run status, duration, and execution start date.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and has a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>An archived pipeline run exists.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, select the <strong>project</strong> for the archived pipeline runs that you want to view.</p>\n</li>\n<li>\n<p>Click the <strong>Archived</strong> tab.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>A list of archived runs appears in the <strong>Archived</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"archiving-a-pipeline-run_ds-pipelines\">Archiving a pipeline run</h4>\n<div class=\"paragraph _abstract\">\n<p>You can retain records of your pipeline runs by archiving them. If required, you can restore runs from your archive to reuse, or delete runs that are no longer required.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and has a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have previously executed a pipeline run that is available.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, select the project for the pipeline run that you want to archive from the <strong>Project</strong> list.</p>\n</li>\n<li>\n<p>On the <strong>Run details</strong> page, click the action menu (<strong>&#8942;</strong>) beside the run that you want to archive and then click <strong>Archive</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Archiving run</strong> dialog, enter the run name in the text field to confirm that you intend to archive it.</p>\n</li>\n<li>\n<p>Click <strong>Archive</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The archived run does not appear in the <strong>Active</strong> tab and instead appears in the <strong>Archived</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"restoring-an-archived-pipeline-run_ds-pipelines\">Restoring an archived pipeline run</h4>\n<div class=\"paragraph _abstract\">\n<p>You can restore an archived run to the active state.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and has a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>An archived run exists in your project.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, select the <strong>project</strong> for the archived pipeline run that you want to restore.</p>\n</li>\n<li>\n<p>On the <strong>Run details</strong> page, click the <strong>Archived</strong> tab.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the run that you want to restore and click <strong>Restore</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Restore run</strong> dialog, enter the run name in the text field to confirm that you intend to restore it.</p>\n</li>\n<li>\n<p>Click <strong>Restore</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The restored run appears in the <strong>Active</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deleting-an-archived-pipeline-run_ds-pipelines\">Deleting an archived pipeline run</h4>\n<div class=\"paragraph _abstract\">\n<p>You can delete pipeline runs from the Open Data Hub run archive.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and has a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have previously archived a pipeline run.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, select the <strong>project</strong> for the archived pipeline run you want to delete.</p>\n</li>\n<li>\n<p>In the <strong>Run details</strong> page, click <strong>Archived</strong>.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the run that you want to delete and click <strong>Delete</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Delete run</strong> dialog, enter the run name in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The archived run that you deleted no longer appears in the <strong>Archived</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"duplicating-an-archived-pipeline-run_ds-pipelines\">Duplicating an archived pipeline run</h4>\n<div class=\"paragraph _abstract\">\n<p>To make it easier to reproduce runs with the same configuration as runs in your archive, you can duplicate them.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>An archived run is available to duplicate in the <strong>Archived</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, select the <strong>project</strong> that has the pipeline run that you want to duplicate.</p>\n</li>\n<li>\n<p>Click the <strong>Archived</strong> tab.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the relevant archived run and click <strong>Duplicate</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Duplicate run</strong> page, configure the duplicate run:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Name</strong> field, enter a name for the duplicate run.</p>\n</li>\n<li>\n<p>In the <strong>Description</strong> field, enter a description for the duplicate run.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline</strong> list, select the pipeline to contain the duplicate run.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline version</strong> list, select the pipeline version to contain the duplicate run.</p>\n</li>\n<li>\n<p>In the <strong>Parameters</strong> section, configure the input parameters for the run that you are duplicating by selecting the appropriate parameters from the list.</p>\n</li>\n<li>\n<p>Click <strong>Create run</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The duplicate pipeline run appears in the <strong>Active</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_working_with_pipeline_logs\">Working with pipeline logs</h3>\n<div class=\"sect3\">\n<h4 id=\"about-pipeline-logs_ds-pipelines\">About pipeline logs</h4>\n<div class=\"paragraph _abstract\">\n<p>You can review and analyze step logs for each step in a triggered pipeline run.</p>\n</div>\n<div class=\"paragraph\">\n<p>To help you troubleshoot and audit your pipelines, you can review and analyze these step logs by using the log viewer in the Open Data Hub dashboard. From here, you can search for specific log messages, view the log for each step, and download the step logs to your local machine.</p>\n</div>\n<div class=\"paragraph\">\n<p>If the step log file exceeds its capacity, a warning appears above the log viewer stating that the log window displays partial content. Expanding the warning displays further information, such as how the log viewer refreshes every three seconds, and that each step log displays the last 500 lines of log messages received. In addition, you can click <strong>download all step logs</strong> to download all step logs to your local machine.</p>\n</div>\n<div class=\"paragraph\">\n<p>Each step has a set of container logs. You can view these container logs by selecting a container from the <strong>Steps</strong> list in the log viewer. The <code>Step-main</code> container log consists of the log output for the step. The <code>step-copy-artifact</code> container log consists of output relating to artifact data sent to s3-compatible storage. If the data transferred between the steps in your pipeline is larger than 3 KB, five container logs are typically available. These logs contain output relating to data transferred between your persistent volume claims (PVCs).</p>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"viewing-pipeline-step-logs_ds-pipelines\">Viewing pipeline step logs</h4>\n<div class=\"paragraph _abstract\">\n<p>To help you troubleshoot and audit your pipelines, you can review and analyze the log of each pipeline step using the log viewer. From here, you can search for specific log messages and download the logs for each step in your pipeline. If the pipeline is running, you can also pause and resume the log from the log viewer.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have previously triggered a pipeline run.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Pipelines</strong> page, select the <strong>project</strong> that you want to view logs for.</p>\n</li>\n<li>\n<p>For the pipeline that you want to view logs for, click <strong>Expand</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhoai-expand-icon.png\" alt=\"rhoai expand icon\"></span>).</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) on the row containing the project version that you want to view pipeline logs for and click <strong>View runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, click the name of the run that you want to view logs for.</p>\n</li>\n<li>\n<p>On the graph on the <strong>Run details</strong> page , click the pipeline step that you want to view logs for.</p>\n</li>\n<li>\n<p>Click the <strong>Logs</strong> tab.</p>\n</li>\n<li>\n<p>To view the logs of another pipeline step, from the <strong>Steps</strong> list, select the step that you want to view logs for.</p>\n</li>\n<li>\n<p>Analyze the log using the log viewer.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>To search for a specific log message, enter at least part of the message in the search bar.</p>\n</li>\n<li>\n<p>To view the full log in a separate browser window, click the action menu (&#8942;) and select <strong>View raw logs</strong>. Alternatively, to expand the size of the log viewer, click the action menu (&#8942;) and select <strong>Expand</strong>.</p>\n</li>\n</ul>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>You can view the logs for each step in your pipeline.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"downloading-pipeline-step-logs_ds-pipelines\">Downloading pipeline step logs</h4>\n<div class=\"paragraph _abstract\">\n<p>Instead of viewing the step logs of a pipeline run using the log viewer on the Open Data Hub dashboard, you can download them for further analysis. You can choose to download the logs belonging to all steps in your pipeline, or you can download the log only for the step log displayed in the log viewer.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have previously triggered a pipeline run.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Pipelines</strong> page, select the <strong>project</strong> that you want to download logs for.</p>\n</li>\n<li>\n<p>For the pipeline that you want to download logs for, click <strong>Expand</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhoai-expand-icon.png\" alt=\"rhoai expand icon\"></span>).</p>\n</li>\n<li>\n<p>Click <strong>View runs</strong> on the row containing the pipeline version that you want to download logs for.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, click the name of the run that you want to download logs for.</p>\n</li>\n<li>\n<p>On the graph on the <strong>Run details</strong> page, click the pipeline step that you want to download logs for.</p>\n</li>\n<li>\n<p>Click the <strong>Logs</strong> tab.</p>\n</li>\n<li>\n<p>In the log viewer, click the <strong>Download</strong> button (<span class=\"image\"><img src=\"/static/docs/images/rhoai-download-icon.png\" alt=\"rhoai download icon\"></span>).</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Select <strong>Download current stop log</strong> to download the log for the current pipeline step.</p>\n</li>\n<li>\n<p>Select <strong>Download all step logs</strong> to download the logs for all steps in your pipeline run.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The step logs download to your browser&#8217;s default directory for downloaded files.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_working_with_pipelines_in_jupyterlab\">Working with pipelines in JupyterLab</h3>\n<div class=\"sect3\">\n<h4 id=\"overview-of-pipelines-in-jupyterlab_ds-pipelines\">Overview of pipelines in JupyterLab</h4>\n<div class=\"paragraph _abstract\">\n<p>You can use Elyra to create visual end-to-end pipeline workflows in JupyterLab. Elyra is an extension for JupyterLab that provides you with a Pipeline Editor to create pipeline workflows that can be executed in Open Data Hub.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can access the Elyra extension within JupyterLab when you create the most recent version of one of the following notebook images:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Standard Data Science</p>\n</li>\n<li>\n<p>PyTorch</p>\n</li>\n<li>\n<p>TensorFlow</p>\n</li>\n<li>\n<p>TrustyAI</p>\n</li>\n<li>\n<p>HabanaAI</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>When you use the Pipeline Editor to visually design your pipelines, minimal coding is required to create and run pipelines. For more information about Elyra, see <a href=\"https://elyra.readthedocs.io/en/stable/getting_started/overview.html\">Elyra Documentation</a>. For more information about the Pipeline Editor, see <a href=\"https://elyra.readthedocs.io/en/stable/user_guide/jupyterlab-interface.html#visual-pipeline-editor\">Visual Pipeline Editor</a>. After you have created your pipeline, you can run it locally in JupyterLab, or remotely using data science pipelines in Open Data Hub.</p>\n</div>\n<div class=\"paragraph\">\n<p>The pipeline creation process consists of the following tasks:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Create a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>Create a pipeline server.</p>\n</li>\n<li>\n<p>Create a new pipeline in the Pipeline Editor in JupyterLab.</p>\n</li>\n<li>\n<p>Develop your pipeline by adding Python notebooks or Python scripts and defining their runtime properties.</p>\n</li>\n<li>\n<p>Define execution dependencies.</p>\n</li>\n<li>\n<p>Run or export your pipeline.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>Before you can run a pipeline in JupyterLab, your pipeline instance must contain a runtime configuration. A runtime configuration defines connectivity information for your pipeline instance and S3-compatible cloud storage.</p>\n</div>\n<div class=\"paragraph\">\n<p>If you create a workbench as part of a data science project, a default runtime configuration is created automatically. However, if you create a notebook from the Jupyter tile in the Open Data Hub dashboard, you must create a runtime configuration before you can run your pipeline in JupyterLab. For more information about runtime configurations, see <a href=\"https://elyra.readthedocs.io/en/stable/user_guide/runtime-conf.html\">Runtime Configuration</a>. As a prerequisite, before you create a workbench, ensure that you have created and configured a pipeline server within the same data science project as your workbench.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can use S3-compatible cloud storage to make data available to your notebooks and scripts while they are executed. Your cloud storage must be accessible from the machine in your deployment that runs JupyterLab and from the cluster that hosts Data Science Pipelines. Before you create and run pipelines in JupyterLab, ensure that you have your s3-compatible storage credentials readily available.</p>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://elyra.readthedocs.io/en/stable/getting_started/overview.html\">Elyra Documentation</a></p>\n</li>\n<li>\n<p><a href=\"https://elyra.readthedocs.io/en/stable/user_guide/jupyterlab-interface.html#visual-pipeline-editor\">Visual Pipeline Editor</a></p>\n</li>\n<li>\n<p><a href=\"https://elyra.readthedocs.io/en/stable/user_guide/runtime-conf.html\">Runtime Configuration</a>.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"accessing-the-pipeline-editor_ds-pipelines\">Accessing the pipeline editor</h4>\n<div class=\"paragraph _abstract\">\n<p>You can use Elyra to create visual end-to-end pipeline workflows in JupyterLab. Elyra is an extension for JupyterLab that provides you with a Pipeline Editor to create pipeline workflows that can execute in Open Data Hub.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project.</p>\n</li>\n<li>\n<p>You have created a workbench with the <strong>Standard Data Science</strong> notebook image.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, PyTorch, or HabanaAI).</p>\n</li>\n<li>\n<p>You have access to S3-compatible storage.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>After you open JupyterLab, confirm that the JupyterLab launcher is automatically displayed.</p>\n</li>\n<li>\n<p>In the <strong>Elyra</strong> section of the JupyterLab launcher, click the <strong>Pipeline Editor</strong> tile.</p>\n<div class=\"paragraph\">\n<p>The Pipeline Editor opens.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>You can view the Pipeline Editor in JupyterLab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"creating-a-runtime-configuration_ds-pipelines\">Creating a runtime configuration</h4>\n<div class=\"paragraph _abstract\">\n<p>If you create a workbench as part of a data science project, a default runtime configuration is created automatically. However, if you create a notebook from the Jupyter tile in the Open Data Hub dashboard, you must create a runtime configuration before you can run your pipeline in JupyterLab. This enables you to specify connectivity information for your pipeline instance and S3-compatible cloud storage.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have access to S3-compatible cloud storage.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, PyTorch, or HabanaAI).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left sidebar of JupyterLab, click <strong>Runtimes</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>).</p>\n</li>\n<li>\n<p>Click the <strong>Create new runtime configuration</strong> button (<span class=\"image\"><img src=\"/static/docs/images/jupyter-create-runtime.png\" alt=\"Create new runtime configuration\"></span>).</p>\n<div class=\"paragraph\">\n<p>The <strong>Add new Data Science Pipelines runtime configuration</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Complete the relevant fields to define your runtime configuration.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Display Name</strong> field, enter a name for your runtime configuration.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Description</strong> field, enter a description to define your runtime configuration.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Tags</strong> field, click <strong>Add Tag</strong> to define a category for your pipeline instance. Enter a name for the tag and press Enter.</p>\n</li>\n<li>\n<p>Define the credentials of your data science pipeline:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint</strong> field, enter the API endpoint of your data science pipeline. Do not specify the pipelines namespace in this field.</p>\n</li>\n<li>\n<p>In the <strong>Public Data Science Pipelines API Endpoint</strong> field, enter the public API endpoint of your data science pipeline.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>You can obtain the Data Science Pipelines API endpoint from the <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong> page in the dashboard. Copy the relevant end point and enter it in the <strong>Public Data Science Pipelines API Endpoint</strong> field.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Optional: In the <strong>Data Science Pipelines User Namespace</strong> field, enter the relevant user namespace to run pipelines.</p>\n</li>\n<li>\n<p>From the <strong>Authentication Type</strong> list, select the authentication type required to authenticate your pipeline.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you created a notebook directly from the Jupyter tile on the dashboard, select <code>EXISTING_BEARER_TOKEN</code> from the <strong>Authentication Type</strong> list.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint Username</strong> field, enter the user name required for the authentication type.</p>\n</li>\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint Password Or Token</strong>, enter the password or token required for the authentication type.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>To obtain the Data Science Pipelines API endpoint token, in the upper-right corner of the OpenShift web console, click your user name and select <strong>Copy login command</strong>. After you have logged in, click <strong>Display token</strong> and copy the value of <code>--token=</code> from the <strong>Log in with this token</strong> command.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Define the connectivity information of your S3-compatible storage:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Cloud Object Storage Endpoint</strong> field, enter the endpoint of your S3-compatible storage. For more information about Amazon s3 endpoints, see <a href=\"https://docs.aws.amazon.com/general/latest/gr/s3.html\">Amazon Simple Storage Service endpoints and quotas</a>.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Public Cloud Object Storage Endpoint</strong> field, enter the URL of your S3-compatible storage.</p>\n</li>\n<li>\n<p>In the <strong>Cloud Object Storage Bucket Name</strong> field, enter the name of the bucket where your pipeline artifacts are stored. If the bucket name does not exist, it is created automatically.</p>\n</li>\n<li>\n<p>From the <strong>Cloud Object Storage Authentication Type</strong> list, select the authentication type required to access to your S3-compatible cloud storage. If you use AWS S3 buckets, select <code>KUBERNETES_SECRET</code> from the list.</p>\n</li>\n<li>\n<p>In the <strong>Cloud Object Storage Credentials Secret</strong> field, enter the secret that contains the storage user name and password. This secret is defined in the relevant user namespace, if applicable. In addition, it must be stored on the cluster that hosts your pipeline runtime.</p>\n</li>\n<li>\n<p>In the <strong>Cloud Object Storage Username</strong> field, enter the user name to connect to your S3-compatible cloud storage, if applicable. If you use AWS S3 buckets, enter your AWS Secret Access Key ID.</p>\n</li>\n<li>\n<p>In the <strong>Cloud Object Storage Password</strong> field, enter the password to connect to your S3-compatible cloud storage, if applicable. If you use AWS S3 buckets, enter your AWS Secret Access Key.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Save &amp; Close</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The runtime configuration that you created is shown in the <strong>Runtimes</strong> tab (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>) in the left sidebar of JupyterLab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"updating-a-runtime-configuration_ds-pipelines\">Updating a runtime configuration</h4>\n<div class=\"paragraph _abstract\">\n<p>To ensure that your runtime configuration is accurate and updated, you can change the settings of an existing runtime configuration.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have access to S3-compatible storage.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>A previously created runtime configuration is available in the JupyterLab interface.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, PyTorch, or HabanaAI).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left sidebar of JupyterLab, click <strong>Runtimes</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>).</p>\n</li>\n<li>\n<p>Hover the cursor over the runtime configuration that you want to update and click the <strong>Edit</strong> button (<span class=\"image\"><img src=\"/static/docs/images/rhoai-edit-icon.png\" alt=\"Edit runtime configuration\"></span>).</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Pipelines runtime configuration</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Fill in the relevant fields to update your runtime configuration.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Display Name</strong> field, update name for your runtime configuration, if applicable.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Description</strong> field, update the description of your runtime configuration, if applicable.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Tags</strong> field, click <strong>Add Tag</strong> to define a category for your pipeline instance. Enter a name for the tag and press Enter.</p>\n</li>\n<li>\n<p>Define the credentials of your data science pipeline:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint</strong> field, update the API endpoint of your data science pipeline, if applicable. Do not specify the pipelines namespace in this field.</p>\n</li>\n<li>\n<p>In the <strong>Public Data Science Pipelines API Endpoint</strong> field, update the API endpoint of your data science pipeline, if applicable.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Data Science Pipelines User Namespace</strong> field, update the relevant user namespace to run pipelines, if applicable.</p>\n</li>\n<li>\n<p>From the <strong>Authentication Type</strong> list, select a new authentication type required to authenticate your pipeline, if applicable.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you created a notebook directly from the Jupyter tile on the dashboard, select <code>EXISTING_BEARER_TOKEN</code> from the <strong>Authentication Type</strong> list.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint Username</strong> field, update the user name required for the authentication type, if applicable.</p>\n</li>\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint Password Or Token</strong>, update the password or token required for the authentication type, if applicable.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>To obtain the Data Science Pipelines API endpoint token, in the upper-right corner of the OpenShift web console, click your user name and select <strong>Copy login command</strong>. After you have logged in, click <strong>Display token</strong> and copy the value of <code>--token=</code> from the <strong>Log in with this token</strong> command.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Define the connectivity information of your S3-compatible storage:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Cloud Object Storage Endpoint</strong> field, update the endpoint of your S3-compatible storage, if applicable. For more information about Amazon s3 endpoints, see <a href=\"https://docs.aws.amazon.com/general/latest/gr/s3.html\">Amazon Simple Storage Service endpoints and quotas</a>.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Public Cloud Object Storage Endpoint</strong> field, update the URL of your S3-compatible storage, if applicable.</p>\n</li>\n<li>\n<p>In the <strong>Cloud Object Storage Bucket Name</strong> field, update the name of the bucket where your pipeline artifacts are stored, if applicable. If the bucket name does not exist, it is created automatically.</p>\n</li>\n<li>\n<p>From the <strong>Cloud Object Storage Authentication Type</strong> list, update the authentication type required to access to your S3-compatible cloud storage, if applicable. If you use AWS S3 buckets, you must select <code>USER_CREDENTIALS</code> from the list.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Cloud Object Storage Credentials Secret</strong> field, update the secret that contains the storage user name and password, if applicable. This secret is defined in the relevant user namespace. You must save the secret on the cluster that hosts your pipeline runtime.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Cloud Object Storage Username</strong> field, update the user name to connect to your S3-compatible cloud storage, if applicable. If you use AWS S3 buckets, update your AWS Secret Access Key ID.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Cloud Object Storage Password</strong> field, update the password to connect to your S3-compatible cloud storage, if applicable. If you use AWS S3 buckets, update your AWS Secret Access Key.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Save &amp; Close</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The runtime configuration that you updated is shown in the <strong>Runtimes</strong> tab (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>) in the left sidebar of JupyterLab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deleting-a-runtime-configuration_ds-pipelines\">Deleting a runtime configuration</h4>\n<div class=\"paragraph _abstract\">\n<p>After you have finished using your runtime configuration, you can delete it from the JupyterLab interface. After deleting a runtime configuration, you cannot run pipelines in JupyterLab until you create another runtime configuration.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>A previously created runtime configuration is visible in the JupyterLab interface.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, PyTorch, or HabanaAI).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left sidebar of JupyterLab, click <strong>Runtimes</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>).</p>\n</li>\n<li>\n<p>Hover the cursor over the runtime configuration that you want to delete and click the <strong>Delete Item</strong> button (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-trash-button.png\" alt=\"Delete item\"></span>).</p>\n<div class=\"paragraph\">\n<p>A dialog box appears prompting you to confirm the deletion of your runtime configuration.</p>\n</div>\n</li>\n<li>\n<p>Click <strong>OK</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The runtime configuration that you deleted is no longer shown in the <strong>Runtimes</strong> tab (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>) in the left sidebar of JupyterLab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"duplicating-a-runtime-configuration_ds-pipelines\">Duplicating a runtime configuration</h4>\n<div class=\"paragraph _abstract\">\n<p>To prevent you from re-creating runtime configurations with similar values in their entirety, you can duplicate an existing runtime configuration in the JupyterLab interface.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>A previously created runtime configuration is visible in the JupyterLab interface.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, PyTorch, or HabanaAI).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left sidebar of JupyterLab, click <strong>Runtimes</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>).</p>\n</li>\n<li>\n<p>Hover the cursor over the runtime configuration that you want to duplicate and click the <strong>Duplicate</strong> button (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-duplicate.png\" alt=\"Duplicate\"></span>).</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The runtime configuration that you duplicated is shown in the <strong>Runtimes</strong> tab (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>) in the left sidebar of JupyterLab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"running-a-pipeline-in-jupyterlab_ds-pipelines\">Running a pipeline in JupyterLab</h4>\n<div class=\"paragraph _abstract\">\n<p>You can run pipelines that you have created in JupyterLab from the Pipeline Editor user interface. Before you can run a pipeline, you must create a data science project and a pipeline server. After you create a pipeline server, you must create a workbench within the same project as your pipeline server.\nYour pipeline instance in JupyterLab must contain a runtime configuration. If you create a workbench as part of a data science project, a default runtime configuration is created automatically. However, if you create a notebook from the Jupyter tile in the Open Data Hub dashboard, you must create a runtime configuration before you can run your pipeline in JupyterLab. A runtime configuration defines connectivity information for your pipeline instance and S3-compatible cloud storage.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have access to S3-compatible storage.</p>\n</li>\n<li>\n<p>You have created a pipeline in JupyterLab.</p>\n</li>\n<li>\n<p>You have opened your pipeline in the Pipeline Editor in JupyterLab.</p>\n</li>\n<li>\n<p>Your pipeline instance contains a runtime configuration.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, PyTorch, or HabanaAI).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the Pipeline Editor user interface, click <strong>Run Pipeline</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-run-pipeline-button.png\" alt=\"The Runtimes icon\"></span>).</p>\n<div class=\"paragraph\">\n<p>The <strong>Run Pipeline</strong> dialog appears. The <strong>Pipeline Name</strong> field is automatically populated with the pipeline file name.</p>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>You must enter a unique pipeline name. The pipeline name that you enter must not match the name of any previously executed pipelines.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Define the settings for your pipeline run.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>From the <strong>Runtime Configuration</strong> list, select the relevant runtime configuration to run your pipeline.</p>\n</li>\n<li>\n<p>Optional: Configure your pipeline parameters, if applicable. If your pipeline contains nodes that reference pipeline parameters, you can change the default parameter values. If a parameter is required and has no default value, you must enter a value.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>OK</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>You can view the output artifacts of your pipeline run. The artifacts are stored in your designated object storage bucket.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"exporting-a-pipeline-in-jupyterlab_ds-pipelines\">Exporting a pipeline in JupyterLab</h4>\n<div class=\"paragraph _abstract\">\n<p>You can export pipelines that you have created in JupyterLab. When you export a pipeline, the pipeline is prepared for later execution, but is not uploaded or executed immediately. During the export process, any package dependencies are uploaded to S3-compatible storage. Also, pipeline code is generated for the target runtime.</p>\n</div>\n<div class=\"paragraph\">\n<p>Before you can export a pipeline, you must create a data science project and a pipeline server. After you create a pipeline server, you must create a workbench within the same project as your pipeline server. In addition, your pipeline instance in JupyterLab must contain a runtime configuration. If you create a workbench as part of a data science project, a default runtime configuration is created automatically. However, if you create a notebook from the Jupyter tile in the Open Data Hub dashboard, you must create a runtime configuration before you can export your pipeline in JupyterLab. A runtime configuration defines connectivity information for your pipeline instance and S3-compatible cloud storage.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you are using specialized Open Data Hub groups, you are part of the user group or admin group (for example, <code>odh-users</code> or <code>odh-admins</code>) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>You have access to S3-compatible storage.</p>\n</li>\n<li>\n<p>You have a created a pipeline in JupyterLab.</p>\n</li>\n<li>\n<p>You have opened your pipeline in the Pipeline Editor in JupyterLab.</p>\n</li>\n<li>\n<p>Your pipeline instance contains a runtime configuration.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, PyTorch, or HabanaAI).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the Pipeline Editor user interface, click <strong>Export Pipeline</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-export-pipeline-button.png\" alt=\"Export pipeline\"></span>).</p>\n<div class=\"paragraph\">\n<p>The <strong>Export Pipeline</strong> dialog appears. The <strong>Pipeline Name</strong> field is automatically populated with the pipeline file name.</p>\n</div>\n</li>\n<li>\n<p>Define the settings to export your pipeline.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>From the <strong>Runtime Configuration</strong> list, select the relevant runtime configuration to export your pipeline.</p>\n</li>\n<li>\n<p>From the <strong>Export Pipeline as</strong> select an appropriate file format</p>\n</li>\n<li>\n<p>In the <strong>Export Filename</strong> field, enter a file name for the exported pipeline.</p>\n</li>\n<li>\n<p>Select the <strong>Replace if file already exists</strong> check box to replace an existing file of the same name as the pipeline you are exporting.</p>\n</li>\n<li>\n<p>Optional: Configure your pipeline parameters, if applicable. If your pipeline contains nodes that reference pipeline parameters, you can change the default parameter values. If a parameter is required and has no default value, you must enter a value.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>OK</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>You can view the file containing the pipeline that you exported in your designated object storage bucket.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2 _additional-resources\">\n<h3 id=\"_additional_resources\">Additional resources</h3>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><a href=\"https://pypi.org/project/kfp/\">KubeFlow Pipelines SDK</a></p>\n</li>\n<li>\n<p><a href=\"https://www.kubeflow.org/docs/components/pipelines/v2/\">Kubeflow Pipelines 2.0 Documentation</a>\n<a href=\"https://opendatahub.io/docs/working-on-data-science-projects/working-with-data-science-pipelines_ds-pipelines#working_with_pipelines_in_jupyterlab\">Working with pipelines in JupyterLab</a>.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"working-with-accelerators_accelerators\">Working with accelerators</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>Use accelerators, such as NVIDIA GPUs and Habana Gaudi devices, to optimize the performance of your end-to-end data science workflows.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"overview-of-accelerators_accelerators\">Overview of accelerators</h3>\n<div class=\"paragraph _abstract\">\n<p>If you work with large data sets, you can use accelerators to optimize the performance of your data science models in Open Data Hub. With accelerators, you can scale your work, reduce latency, and increase productivity. You can use accelerators in Open Data Hub to assist your data scientists in the following tasks:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Natural language processing (NLP)</p>\n</li>\n<li>\n<p>Inference</p>\n</li>\n<li>\n<p>Training deep neural networks</p>\n</li>\n<li>\n<p>Data cleansing and data processing</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>Open Data Hub supports the following accelerators:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>NVIDIA graphics processing units (GPUs)</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>To use compute-heavy workloads in your models, you can enable NVIDIA graphics processing units (GPUs) in Open Data Hub.</p>\n</li>\n<li>\n<p>To enable GPUs on OpenShift, you must install the <a href=\"https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html\">NVIDIA GPU Operator</a>.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Habana Gaudi devices (HPUs)</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Habana, an Intel company, provides hardware accelerators intended for deep learning workloads. You can use the Habana libraries and software associated with Habana Gaudi devices available from your notebook.</p>\n</li>\n<li>\n<p>Before you can enable Habana Gaudi devices in Open Data Hub, you must install the necessary dependencies and the version of the HabanaAI Operator that matches the Habana version of the HabanaAI workbench image in your deployment. For more information about how to enable your OpenShift environment for Habana Gaudi devices, see <a href=\"https://docs.habana.ai/en/v1.10.0/Orchestration/HabanaAI_Operator/index.html\">HabanaAI Operator v1.10 for OpenShift</a> and <a href=\"https://docs.habana.ai/en/v1.13.0/Orchestration/HabanaAI_Operator/index.html\">HabanaAI Operator v1.13 for OpenShift</a>.</p>\n</li>\n<li>\n<p>You can enable Habana Gaudi devices on-premises or with AWS DL1 compute nodes on an AWS instance.</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>Before you can use an accelerator in Open Data Hub, your OpenShift instance must contain an associated accelerator profile. For accelerators that are new to your deployment, you must configure an accelerator profile for the accelerator in context. You can create an accelerator profile from the <strong>Settings</strong> &#8594; <strong>Accelerator profiles</strong> page on the Open Data Hub dashboard. If your deployment contains existing accelerators that had associated accelerator profiles already configured, an accelerator profile is automatically created after you upgrade to the latest version of Open Data Hub.</p>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://docs.habana.ai/en/v1.10.0/Orchestration/HabanaAI_Operator/index.html\">HabanaAI Operator v1.10 for OpenShift</a></p>\n</li>\n<li>\n<p><a href=\"https://docs.habana.ai/en/v1.13.0/Orchestration/HabanaAI_Operator/index.html\">HabanaAI Operator v1.13 for OpenShift</a></p>\n</li>\n<li>\n<p><a href=\"https://habana.ai/\">Habana, an Intel Company</a></p>\n</li>\n<li>\n<p><a href=\"https://aws.amazon.com/ec2/instance-types/dl1/\">Amazon EC2 DL1 Instances</a></p>\n</li>\n<li>\n<p><a href=\"https://linux.die.net/man/8/lspci\">lspci(8) - Linux man page</a></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"working-with-accelerator-profiles_accelerators\">Working with accelerator profiles</h3>\n<div class=\"paragraph _abstract\">\n<p>To configure accelerators for your data scientists to use in Open Data Hub, you must create an associated accelerator profile. An accelerator profile is a custom resource definition (CRD) on OpenShift that has an AcceleratorProfile resource, and defines the specification of the accelerator. You can create and manage accelerator profiles by selecting <strong>Settings</strong> &#8594; <strong>Accelerator profiles</strong> on the Open Data Hub dashboard.</p>\n</div>\n<div class=\"paragraph\">\n<p>For accelerators that are new to your deployment, you must manually configure an accelerator profile for each accelerator. If your deployment contains an accelerator before you upgrade, the associated accelerator profile remains after the upgrade. You can manage the accelerators that appear to your data scientists by assigning specific accelerator profiles to your custom notebook images. This example shows the code for a Habana Gaudi 1 accelerator profile:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-yaml\" data-lang=\"yaml\">---\napiVersion: dashboard.opendatahub.io/v1alpha\nkind: AcceleratorProfile\nmetadata:\n  name: hpu-profile-first-gen-gaudi\nspec:\n  displayName: Habana HPU - 1st Gen Gaudi\n  description: First Generation Habana Gaudi device\n  enabled: true\n  identifier: habana.ai/gaudi\n  tolerations:\n    - effect: NoSchedule\n      key: habana.ai/gaudi\n      operator: Exists\n---</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The accelerator profile code appears on the <strong>Instances</strong> tab on the details page for the <code>AcceleratorProfile</code> custom resource definition (CRD). For more information about accelerator profile attributes, see the following table:</p>\n</div>\n<table id=\"table-accelerator-profile-attributes_accelerators\" class=\"tableblock frame-all grid-all stretch\">\n<caption class=\"title\">Table 2. Accelerator profile attributes</caption>\n<colgroup>\n<col style=\"width: 12.5%;\">\n<col style=\"width: 12.5%;\">\n<col style=\"width: 12.5%;\">\n<col style=\"width: 62.5%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Attribute</th>\n<th class=\"tableblock halign-left valign-top\">Type</th>\n<th class=\"tableblock halign-left valign-top\">Required</th>\n<th class=\"tableblock halign-left valign-top\">Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">displayName</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">String</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Required</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">The display name of the accelerator profile.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">description</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">String</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Optional</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Descriptive text defining the accelerator profile.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">identifier</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">String</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Required</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">A unique identifier defining the accelerator resource.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">enabled</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Boolean</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Required</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Determines if the accelerator is visible in Open Data Hub.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">tolerations</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Array</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Optional</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">The tolerations that can apply to notebooks and serving runtimes that use the accelerator. For more information about the toleration attributes that Open Data Hub supports, see <a href=\"https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#toleration-v1-core\">Toleration v1 core</a>.</p></td>\n</tr>\n</tbody>\n</table>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#toleration-v1-core\">Toleration v1 core</a></p>\n</li>\n<li>\n<p><a href=\"https://docs.openshift.com/container-platform/4.14/nodes/scheduling/nodes-scheduler-taints-tolerations.html\">Understanding taints and tolerations</a></p>\n</li>\n<li>\n<p><a href=\"https://docs.openshift.com/container-platform/4.14/operators/understanding/crds/crd-managing-resources-from-crds.html\">Managing resources from custom resource definitions</a></p>\n</li>\n</ul>\n</div>\n<div class=\"sect3\">\n<h4 id=\"creating-an-accelerator-profile_accelerators\">Creating an accelerator profile</h4>\n<div class=\"paragraph _abstract\">\n<p>To configure accelerators for your data scientists to use in Open Data Hub, you must create an associated accelerator profile.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Settings</strong> &#8594; <strong>Accelerator profiles</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Accelerator profiles</strong> page appears, displaying existing accelerator profiles. To enable or disable an existing accelerator profile, on the row containing the relevant accelerator profile, click the toggle in the <strong>Enable</strong> column.</p>\n</div>\n</li>\n<li>\n<p>Click <strong>Create accelerator profile</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Create accelerator profile</strong> dialog appears.</p>\n</div>\n</li>\n<li>\n<p>In the <strong>Name</strong> field, enter a name for the accelerator profile.</p>\n</li>\n<li>\n<p>In the <strong>Identifier</strong> field, enter a unique string that identifies the hardware accelerator associated with the accelerator profile.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Description</strong> field, enter a description for the accelerator profile.</p>\n</li>\n<li>\n<p>To enable or disable the accelerator profile immediately after creation, click the toggle in the <strong>Enable</strong> column.</p>\n</li>\n<li>\n<p>Optional: Add a toleration to schedule pods with matching taints.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click <strong>Add toleration</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Add toleration</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Operator</strong> list, select one of the following options:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>Equal</strong> - The <strong>key/value/effect</strong> parameters must match. This is the default.</p>\n</li>\n<li>\n<p><strong>Exists</strong> - The <strong>key/effect</strong> parameters must match. You must leave a blank value parameter, which matches any.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>From the <strong>Effect</strong> list, select one of the following options:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>None</strong></p>\n</li>\n<li>\n<p><strong>NoSchedule</strong> - New pods that do not match the taint are not scheduled onto that node. Existing pods on the node remain.</p>\n</li>\n<li>\n<p><strong>PreferNoSchedule</strong> - New pods that do not match the taint might be scheduled onto that node, but the scheduler tries not to. Existing pods on the node remain.</p>\n</li>\n<li>\n<p><strong>NoExecute</strong> - New pods that do not match the taint cannot be scheduled onto that node. Existing pods on the node that do not have a matching toleration are removed.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>In the <strong>Key</strong> field, enter a toleration key. The key is any string, up to 253 characters. The key must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores.</p>\n</li>\n<li>\n<p>In the <strong>Value</strong> field, enter a toleration value. The value is any string, up to 63 characters. The value must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores.</p>\n</li>\n<li>\n<p>In the <strong>Toleration Seconds</strong> section, select one of the following options to specify how long a pod stays bound to a node that has a node condition.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>Forever</strong> - Pods stays permanently bound to a node.</p>\n</li>\n<li>\n<p><strong>Custom value</strong> - Enter a value, in seconds, to define how long pods stay bound to a node that has a node condition.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Click <strong>Add</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Create accelerator profile</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The accelerator profile appears on the <strong>Accelerator profiles</strong> page.</p>\n</li>\n<li>\n<p>The <strong>Accelerator</strong> list appears on the <strong>Start a notebook server</strong> page. After you select an accelerator, the <strong>Number of accelerators</strong> field appears, which you can use to choose the number of accelerators for your notebook server.</p>\n</li>\n<li>\n<p>The accelerator profile appears on the <strong>Instances</strong> tab on the details page for the <code>AcceleratorProfile</code> custom resource definition (CRD).</p>\n</li>\n</ul>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#toleration-v1-core\">Toleration v1 core</a></p>\n</li>\n<li>\n<p><a href=\"https://docs.openshift.com/container-platform/4.14/nodes/scheduling/nodes-scheduler-taints-tolerations.html\">Understanding taints and tolerations</a></p>\n</li>\n<li>\n<p><a href=\"https://docs.openshift.com/container-platform/4.14/operators/understanding/crds/crd-managing-resources-from-crds.html\">Managing resources from custom resource definitions</a></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"updating-an-accelerator-profile_accelerators\">Updating an accelerator profile</h4>\n<div class=\"paragraph _abstract\">\n<p>You can update the existing accelerator profiles in your deployment. You might want to change important identifying information, such as the display name, the identifier, or the description.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>The accelerator profile exists in your deployment.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Settings</strong> &#8594; <strong>Notebook images</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Notebook images</strong> page appears. Previously imported notebook images are displayed. To enable or disable a previously imported notebook image, on the row containing the relevant notebook image, click the toggle in the <strong>Enable</strong> column.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (&#8942;) and select <strong>Edit</strong> from the list.</p>\n<div class=\"paragraph\">\n<p>The <strong>Edit accelerator profile</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>In the <strong>Name</strong> field, update the accelerator profile name.</p>\n</li>\n<li>\n<p>In the <strong>Identifier</strong> field, update the unique string that identifies the hardware accelerator associated with the accelerator profile, if applicable.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Description</strong> field, update the accelerator profile.</p>\n</li>\n<li>\n<p>To enable or disable the accelerator profile immediately after creation, click the toggle in the <strong>Enable</strong> column.</p>\n</li>\n<li>\n<p>Optional: Add a toleration to schedule pods with matching taints.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click <strong>Add toleration</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Add toleration</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Operator</strong> list, select one of the following options:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>Equal</strong> - The <strong>key/value/effect</strong> parameters must match. This is the default.</p>\n</li>\n<li>\n<p><strong>Exists</strong> - The <strong>key/effect</strong> parameters must match. You must leave a blank value parameter, which matches any.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>From the <strong>Effect</strong> list, select one of the following options:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>None</strong></p>\n</li>\n<li>\n<p><strong>NoSchedule</strong> - New pods that do not match the taint are not scheduled onto that node. Existing pods on the node remain.</p>\n</li>\n<li>\n<p><strong>PreferNoSchedule</strong> - New pods that do not match the taint might be scheduled onto that node, but the scheduler tries not to. Existing pods on the node remain.</p>\n</li>\n<li>\n<p><strong>NoExecute</strong> - New pods that do not match the taint cannot be scheduled onto that node. Existing pods on the node that do not have a matching toleration are removed.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>In the <strong>Key</strong> field, enter a toleration key. The key is any string, up to 253 characters. The key must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores.</p>\n</li>\n<li>\n<p>In the <strong>Value</strong> field, enter a toleration value. The value is any string, up to 63 characters. The value must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores.</p>\n</li>\n<li>\n<p>In the <strong>Toleration Seconds</strong> section, select one of the following options to specify how long a pod stays bound to a node that has a node condition.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>Forever</strong> - Pods stays permanently bound to a node.</p>\n</li>\n<li>\n<p><strong>Custom value</strong> - Enter a value, in seconds, to define how long pods stay bound to a node that has a node condition.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Click <strong>Add</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>If your accelerator profile contains existing tolerations, you can edit them.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click the action menu (&#8942;) on the row containing the toleration that you want to edit and select <strong>Edit</strong> from the list.</p>\n</li>\n<li>\n<p>Complete the applicable fields to update the details of the toleration.</p>\n</li>\n<li>\n<p>Click <strong>Update</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Update accelerator profile</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>If your accelerator profile has new identifying information, this information appears in the <strong>Accelerator</strong> list on the <strong>Start a notebook server</strong> page.</p>\n</li>\n</ul>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#toleration-v1-core\">Toleration v1 core</a></p>\n</li>\n<li>\n<p><a href=\"https://docs.openshift.com/container-platform/4.14/nodes/scheduling/nodes-scheduler-taints-tolerations.html\">Understanding taints and tolerations</a></p>\n</li>\n<li>\n<p><a href=\"https://docs.openshift.com/container-platform/4.14/operators/understanding/crds/crd-managing-resources-from-crds.html\">Managing resources from custom resource definitions</a></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"deleting-an-accelerator-profile_accelerators\">Deleting an accelerator profile</h4>\n<div class=\"paragraph _abstract\">\n<p>To discard accelerator profiles that you no longer require, you can delete them so that they do not appear on the dashboard.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>The accelerator profile that you want to delete exists in your deployment.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Settings</strong> &#8594; <strong>Accelerator profiles</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Accelerator profiles</strong> page appears, displaying existing accelerator profiles.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the accelerator profile that you want to delete and click <strong>Delete</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Delete accelerator profile</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter the name of the accelerator profile in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The accelerator profile no longer appears on the <strong>Accelerator profiles</strong> page.</p>\n</li>\n</ul>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#toleration-v1-core\">Toleration v1 core</a></p>\n</li>\n<li>\n<p><a href=\"https://docs.openshift.com/container-platform/4.14/nodes/scheduling/nodes-scheduler-taints-tolerations.html\">Understanding taints and tolerations</a></p>\n</li>\n<li>\n<p><a href=\"https://docs.openshift.com/container-platform/4.14/operators/understanding/crds/crd-managing-resources-from-crds.html\">Managing resources from custom resource definitions</a></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"viewing-accelerator-profiles_accelerators\">Viewing accelerator profiles</h4>\n<div class=\"paragraph\">\n<p>If you have defined accelerator profiles for Open Data Hub, you can view, enable, and disable them from the <strong>Accelerator profiles</strong> page.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>Your deployment contains existing accelerator profiles.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Settings</strong> &#8594; <strong>Accelerator profiles</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Accelerator profiles</strong> page appears, displaying existing accelerator profiles.</p>\n</div>\n</li>\n<li>\n<p>Inspect the list of accelerator profiles. To enable or disable an accelerator profile, on the row containing the accelerator profile, click the toggle in the <strong>Enable</strong> column.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The <strong>Accelerator profiles</strong> page appears appears, displaying existing accelerator profiles.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"configuring-a-recommended-accelerator-for-notebook-images_accelerators\">Configuring a recommended accelerator for notebook images</h4>\n<div class=\"paragraph _abstract\">\n<p>To help you indicate the most suitable accelerators to your data scientists, you can configure a recommended tag to appear on the dashboard.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to OpenShift Container Platform.</p>\n</li>\n<li>\n<p>You have the <code>cluster-admin</code> role in OpenShift Container Platform.</p>\n</li>\n<li>\n<p>You have existing notebook images in your deployment.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Settings</strong> &#8594; <strong>Notebook images</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Notebook images</strong> page appears. Previously imported notebook images are displayed.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (&#8942;) and select <strong>Edit</strong> from the list.</p>\n<div class=\"paragraph\">\n<p>The <strong>Update notebook image</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Accelerator identifier</strong> list, select an identifier to set its accelerator as recommended with the notebook image. If the notebook image contains only one accelerator identifier, the identifier name displays by default.</p>\n</li>\n<li>\n<p>Click <strong>Update</strong>.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you have already configured an accelerator identifier for a notebook image, you can specify a recommended accelerator for the notebook image by creating an associated accelerator profile. To do this, click <strong>Create profile</strong> on the row containing the notebook image and complete the relevant fields. If the notebook image does not contain an accelerator identifier, you must manually configure one before creating an associated accelerator profile.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>When your data scientists select an accelerator with a specific notebook image, a tag appears next to the corresponding accelerator indicating its compatibility.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"configuring-a-recommended-accelerator-for-serving-runtimes_accelerators\">Configuring a recommended accelerator for serving runtimes</h4>\n<div class=\"paragraph _abstract\">\n<p>To help you indicate the most suitable accelerators to your data scientists, you can configure a recommended accelerator tag for your serving runtimes.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>If you use specialized Open Data Hub groups, you are part of the admin group (for example, <code>odh-admins</code>) in OpenShift.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Settings</strong> &gt; <strong>Serving runtimes</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Serving runtimes</strong> page opens and shows the model-serving runtimes that are already installed and enabled in your Open Data Hub deployment. By default, the OpenVINO Model Server runtime is pre-installed and enabled in Open Data Hub.</p>\n</div>\n</li>\n<li>\n<p>Edit your custom runtime that you want to add the recommended accelerator tag to, click the action menu (&#8942;) and select <strong>Edit</strong>.</p>\n<div class=\"paragraph\">\n<p>A page with an embedded YAML editor opens.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nYou cannot directly edit the OpenVINO Model Server runtime that is included in Open Data Hub by default. However, you can <em>clone</em> this runtime and edit the cloned version. You can then add the edited clone as a new, custom runtime. To do this, click the action menu beside the OpenVINO Model Server and select <strong>Duplicate</strong>.\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>In the editor, enter the YAML code to apply the annotation <code>opendatahub.io/recommended-accelerators</code>. The excerpt in this example shows the annotation to set a recommended tag for an NVIDIA GPU accelerator:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-yaml\" data-lang=\"yaml\">metadata:\n\tannotations:\n\t\topendatahub.io/recommended-accelerators: '[\"nvidia.com/gpu\"]'</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Click <strong>Update</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>When your data scientists select an accelerator with a specific serving runtime, a tag appears next to the corresponding accelerator indicating its compatibility.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"habana-gaudi-integration_accelerators\">Habana Gaudi integration</h3>\n<div class=\"paragraph _abstract\">\n<p>To accelerate your high-performance deep learning (DL) models, you can integrate Habana Gaudi devices in Open Data Hub. Open Data Hub also includes the HabanaAI workbench image, which is pre-built and ready for your data scientists to use after you install or upgrade Open Data Hub.</p>\n</div>\n<div class=\"paragraph\">\n<p>Before you can enable Habana Gaudi devices in Open Data Hub, you must install the necessary dependencies and the version of the HabanaAI Operator that matches the Habana version of the HabanaAI workbench image in your deployment. This allows your data scientists to use Habana libraries and software associated with Habana Gaudi devices from their workbench.</p>\n</div>\n<div class=\"paragraph\">\n<p>For more information about how to enable your OpenShift environment for Habana Gaudi devices, see <a href=\"https://docs.habana.ai/en/v1.10.0/Orchestration/HabanaAI_Operator/index.html\">HabanaAI Operator v1.10 for OpenShift</a> and <a href=\"https://docs.habana.ai/en/v1.13.0/Orchestration/HabanaAI_Operator/index.html\">HabanaAI Operator v1.13 for OpenShift</a>.</p>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Currently, Habana Gaudi integration is only supported in OpenShift 4.12.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can use Habana Gaudi accelerators on Open Data Hub with versions 1.10.0 and 1.13.0 of the Habana Gaudi Operator. The version of the HabanaAI Operator that you install must match the Habana version of the HabanaAI workbench image in your deployment. This means that only one version of HabanaAI workbench image will work for you at a time.</p>\n</div>\n<div class=\"paragraph\">\n<p>For information about the supported configurations for versions 1.10 and 1.13 of the Habana Gaudi Operator, see <a href=\"https://docs.habana.ai/en/latest/Support_Matrix/Support_Matrix_v1.10.0.html#support-matrix-1-10-0\">Support Matrix v1.10.0</a> and <a href=\"https://docs.habana.ai/en/latest/Support_Matrix/Support_Matrix_v1.13.0.html#support-matrix-1-13-0\">Support Matrix v1.13.0</a>.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>You can use Habana Gaudi devices in an Amazon EC2 DL1 instance on OpenShift. Therefore, your OpenShift platform must support EC2 DL1 instances. Habana Gaudi accelerators are available to your data scientists when they create a workbench instance or serve a model.</p>\n</div>\n<div class=\"paragraph\">\n<p>To identify the Habana Gaudi devices present in your deployment, use the <code>lspci</code> utility. For more information, see <a href=\"https://linux.die.net/man/8/lspci\">lspci(8) - Linux man page</a>.</p>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If the <code>lspci</code> utility indicates that Habana Gaudi devices are present in your deployment, it does not necessarily mean that the devices are ready to use.</p>\n</div>\n<div class=\"paragraph\">\n<p>Before you can use your Habana Gaudi devices, you must enable them in your OpenShift environment and configure an accelerator profile for each device. For more information about how to enable your OpenShift environment for Habana Gaudi devices, see <a href=\"https://docs.habana.ai/en/v1.10.0/Orchestration/HabanaAI_Operator/index.html\">HabanaAI Operator for OpenShift</a>.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://docs.habana.ai/en/v1.10.0/Orchestration/HabanaAI_Operator/index.html\">HabanaAI Operator v1.10 for OpenShift</a></p>\n</li>\n<li>\n<p><a href=\"https://docs.habana.ai/en/v1.13.0/Orchestration/HabanaAI_Operator/index.html\">HabanaAI Operator v1.13 for OpenShift</a></p>\n</li>\n<li>\n<p><a href=\"https://linux.die.net/man/8/lspci\">lspci(8) - Linux man page</a></p>\n</li>\n<li>\n<p><a href=\"https://aws.amazon.com/ec2/instance-types/dl1/\">Amazon EC2 DL1 Instances</a></p>\n</li>\n<li>\n<p><a href=\"https://docs.habana.ai/en/latest/Support_Matrix/Support_Matrix_v1.10.0.html#support-matrix-1-10-0\">Support Matrix v1.10.0</a></p>\n</li>\n<li>\n<p><a href=\"https://docs.habana.ai/en/latest/Support_Matrix/Support_Matrix_v1.13.0.html#support-matrix-1-13-0\">Support Matrix v1.13.0</a></p>\n</li>\n<li>\n<p><a href=\"https://access.redhat.com/solutions/4870701\">What version of the Kubernetes API is included with each OpenShift 4.x release?</a></p>\n</li>\n</ul>\n</div>\n<div class=\"sect3\">\n<h4 id=\"enabling-habana-gaudi-devices_accelerators\">Enabling Habana Gaudi devices</h4>\n<div class=\"paragraph _abstract\">\n<p>Before you can use Habana Gaudi devices in Open Data Hub, you must install the necessary dependencies and deploy the HabanaAI Operator.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to OpenShift Container Platform.</p>\n</li>\n<li>\n<p>You have the <code>cluster-admin</code> role in OpenShift Container Platform.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>To enable Habana Gaudi devices in Open Data Hub, follow the instructions at <a href=\"https://docs.habana.ai/en/latest/Orchestration/HabanaAI_Operator/index.html\">HabanaAI Operator for OpenShift</a>.</p>\n</li>\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Settings</strong> &#8594; <strong>Accelerator profiles</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Accelerator profiles</strong> page appears, displaying existing accelerator profiles. To enable or disable an existing accelerator profile, on the row containing the relevant accelerator profile, click the toggle in the <strong>Enable</strong> column.</p>\n</div>\n</li>\n<li>\n<p>Click <strong>Create accelerator profile</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Create accelerator profile</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>In the <strong>Name</strong> field, enter a name for the Habana Gaudi device.</p>\n</li>\n<li>\n<p>In the <strong>Identifier</strong> field, enter a unique string that identifies the Habana Gaudi device, for example, <code>habana.ai/gaudi</code>.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Description</strong> field, enter a description for the Habana Gaudi device.</p>\n</li>\n<li>\n<p>To enable or disable the accelerator profile for the Habana Gaudi device immediately after creation, click the toggle in the <strong>Enable</strong> column.</p>\n</li>\n<li>\n<p>Optional: Add a toleration to schedule pods with matching taints.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click <strong>Add toleration</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Add toleration</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Operator</strong> list, select one of the following options:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>Equal</strong> - The <strong>key/value/effect</strong> parameters must match. This is the default.</p>\n</li>\n<li>\n<p><strong>Exists</strong> - The <strong>key/effect</strong> parameters must match. You must leave a blank value parameter, which matches any.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>From the <strong>Effect</strong> list, select one of the following options:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>None</strong></p>\n</li>\n<li>\n<p><strong>NoSchedule</strong> - New pods that do not match the taint are not scheduled onto that node. Existing pods on the node remain.</p>\n</li>\n<li>\n<p><strong>PreferNoSchedule</strong> - New pods that do not match the taint might be scheduled onto that node, but the scheduler tries not to. Existing pods on the node remain.</p>\n</li>\n<li>\n<p><strong>NoExecute</strong> - New pods that do not match the taint cannot be scheduled onto that node. Existing pods on the node that do not have a matching toleration are removed.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>In the <strong>Key</strong> field, enter the toleration key <code>habana.ai/gaudi</code>. The key is any string, up to 253 characters. The key must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores.</p>\n</li>\n<li>\n<p>In the <strong>Value</strong> field, enter a toleration value. The value is any string, up to 63 characters. The value must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores.</p>\n</li>\n<li>\n<p>In the <strong>Toleration Seconds</strong> section, select one of the following options to specify how long a pod stays bound to a node that has a node condition.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>Forever</strong> - Pods stays permanently bound to a node.</p>\n</li>\n<li>\n<p><strong>Custom value</strong> - Enter a value, in seconds, to define how long pods stay bound to a node that has a node condition.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Click <strong>Add</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Create accelerator profile</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>From the <strong>Administrator</strong> perspective, the following Operators appear on the <strong>Operators</strong> &#8594; <strong>Installed Operators</strong> page.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>HabanaAI</p>\n</li>\n<li>\n<p>Node Feature Discovery (NFD)</p>\n</li>\n<li>\n<p>Kernel Module Management (KMM)</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>The <strong>Accelerator</strong> list displays the Habana Gaudi accelerator on the <strong>Start a notebook server</strong> page. After you select an accelerator, the <strong>Number of accelerators</strong> field appears, which you can use to choose the number of accelerators for your notebook server.</p>\n</li>\n<li>\n<p>The accelerator profile appears on the <strong>Accelerator profiles</strong> page</p>\n</li>\n<li>\n<p>The accelerator profile appears on the <strong>Instances</strong> tab on the details page for the <code>AcceleratorProfile</code> custom resource definition (CRD).</p>\n</li>\n</ul>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://docs.habana.ai/en/v1.10.0/Orchestration/HabanaAI_Operator/index.html\">HabanaAI Operator v1.10 for OpenShift</a>.</p>\n</li>\n<li>\n<p><a href=\"https://docs.habana.ai/en/v1.13.0/Orchestration/HabanaAI_Operator/index.html\">HabanaAI Operator v1.13 for OpenShift</a>.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"troubleshooting-common-problems-in-jupyter-for-administrators_accelerators\">Troubleshooting common problems in Jupyter for administrators</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>If your users are experiencing errors in Open Data Hub relating to Jupyter, their notebooks, or their notebook server, read this section to understand what could be causing the problem, and how to resolve the problem.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter\">A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter</h3>\n<div class=\"paragraph\">\n<div class=\"title\">Problem</div>\n<p>If you have configured specialized user groups for Open Data Hub, the user name might not be added to the default user group for Open Data Hub.</p>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Diagnosis</div>\n<p>Check whether the user is part of the default user group.</p>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>Find the names of groups allowed access to Jupyter.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Log in to the OpenShift Container Platform web console.</p>\n</li>\n<li>\n<p>Click <strong>User Management</strong> &#8594; <strong>Groups</strong>.</p>\n</li>\n<li>\n<p>Click the name of your user group, for example, {oai-user-group}.</p>\n<div class=\"paragraph\">\n<p>The <strong>Group details</strong> page for that group appears.</p>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click the <strong>Details</strong> tab for the group and confirm that the <strong>Users</strong> section for the relevant group contains the users who have permission to access Jupyter.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Resolution</div>\n<ul>\n<li>\n<p>If the user is not added to any of the groups allowed access to Jupyter, add them.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_a_users_notebook_server_does_not_start\">A user&#8217;s notebook server does not start</h3>\n<div class=\"paragraph\">\n<div class=\"title\">Problem</div>\n<p>The OpenShift Container Platform cluster that hosts the user&#8217;s notebook server might not have access to enough resources, or the Jupyter pod may have failed.</p>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Diagnosis</div>\n<ol class=\"arabic\">\n<li>\n<p>Log in to the OpenShift Container Platform web console.</p>\n</li>\n<li>\n<p>Delete and restart the notebook server pod for this user.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click <strong>Workloads</strong> &#8594; <strong>Pods</strong> and set the <strong>Project</strong> to <code>rhods-notebooks</code>.</p>\n</li>\n<li>\n<p>Search for the notebook server pod that belongs to this user, for example, <code>jupyter-nb-&lt;username&gt;-*</code>.</p>\n<div class=\"paragraph\">\n<p>If the notebook server pod exists, an intermittent failure may have occurred in the notebook server pod.</p>\n</div>\n<div class=\"paragraph\">\n<p>If the notebook server pod for the user does not exist, continue with diagnosis.</p>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Check the resources currently available in the OpenShift Container Platform cluster against the resources required by the selected notebook server image.</p>\n<div class=\"paragraph\">\n<p>If worker nodes with sufficient CPU and RAM are available for scheduling in the cluster, continue with diagnosis.</p>\n</div>\n</li>\n<li>\n<p>Check the state of the Jupyter pod.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Resolution</div>\n<ul>\n<li>\n<p>If there was an intermittent failure of the notebook server pod:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Delete the notebook server pod that belongs to the user.</p>\n</li>\n<li>\n<p>Ask the user to start their notebook server again.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>If the notebook server does not have sufficient resources to run the selected notebook server image, either add more resources to the OpenShift Container Platform cluster, or choose a smaller image size.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells\">The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells</h3>\n<div class=\"paragraph\">\n<div class=\"title\">Problem</div>\n<p>The user might have run out of storage space on their notebook server.</p>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Diagnosis</div>\n<ol class=\"arabic\">\n<li>\n<p>Log in to Jupyter and start the notebook server that belongs to the user having problems. If the notebook server does not start, follow these steps to check whether the user has run out of storage space:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Log in to the OpenShift Container Platform web console.</p>\n</li>\n<li>\n<p>Click <strong>Workloads</strong> &#8594; <strong>Pods</strong> and set the <strong>Project</strong> to <code>rhods-notebooks</code>.</p>\n</li>\n<li>\n<p>Click the notebook server pod that belongs to this user, for example, <code>jupyter-nb-&lt;idp&gt;-&lt;username&gt;-*</code>.</p>\n</li>\n<li>\n<p>Click <strong>Logs</strong>. The user has exceeded their available capacity if you see lines similar to the following:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>Unexpected error while saving file: XXXX database or disk is full</pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Resolution</div>\n<ul>\n<li>\n<p>Increase the user&#8217;s available storage by expanding their persistent volume.</p>\n</li>\n<li>\n<p>Work with the user to identify files that can be deleted from the <code>/opt/app-root/src</code> directory on their notebook server to free up their existing storage space.</p>\n</li>\n</ul>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>When you delete files using the JupyterLab file explorer, the files move to the hidden <code>/opt/app-root/src/.local/share/Trash/files</code> folder in the persistent storage for the notebook. To free up storage space for notebooks, you must permanently delete these files.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"troubleshooting-common-problems-in-jupyter-for-users_accelerators\">Troubleshooting common problems in Jupyter for users</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>If you are seeing errors in Open Data Hub related to Jupyter, your notebooks, or your notebook server, read this section to understand what could be causing the problem.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_i_see_a_403_forbidden_error_when_i_log_in_to_jupyter\">I see a <strong>403: Forbidden</strong> error when I log in to Jupyter</h3>\n<div class=\"paragraph\">\n<div class=\"title\">Problem</div>\n<p>If your administrator has configured specialized user groups for Open Data Hub, your user name might not be added to the default user group or the default administrator group for Open Data Hub.</p>\n</div>\n<div class=\"literalblock\">\n<div class=\"title\">Resolution</div>\n<div class=\"content\">\n<pre>Contact your administrator so that they can add you to the correct group/s.</pre>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_my_notebook_server_does_not_start\">My notebook server does not start</h3>\n<div class=\"paragraph\">\n<div class=\"title\">Problem</div>\n<p>The OpenShift Container Platform cluster that hosts your notebook server might not have access to enough resources, or the Jupyter pod may have failed.</p>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Resolution</div>\n<p>Check the logs in the <strong>Events</strong> section in OpenShift for error messages associated with the problem. For example:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>Server requested\n2021-10-28T13:31:29.830991Z [Warning] 0/7 nodes are available: 2 Insufficient memory,\n2 node(s) had taint {node-role.kubernetes.io/infra: }, that the pod didn't tolerate, 3 node(s) had taint {node-role.kubernetes.io/master: },\nthat the pod didn't tolerate.</pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Contact your administrator with details of any relevant error messages so that they can perform further checks.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_i_see_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_i_run_my_notebook_cells\">I see a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when I run my notebook cells</h3>\n<div class=\"paragraph\">\n<div class=\"title\">Problem</div>\n<p>You might have run out of storage space on your notebook server.</p>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Resolution</div>\n<p>Contact your administrator so that they can perform further checks.</p>\n</div>\n</div>\n</div>\n</div>","id":"47af8a52-b33d-554d-995f-8b56bac3948f","document":{"title":"Working on data science projects"}},"markdownRemark":null},"pageContext":{"id":"47af8a52-b33d-554d-995f-8b56bac3948f"}},"staticQueryHashes":["2604506565"],"slicesMap":{}}