{"componentChunkName":"component---src-templates-docs-page-tsx","path":"/docs/assemblies/working-with-data-science-pipelines/","result":{"data":{"allFile":{"edges":[{"node":{"childAsciidoc":{"fields":{"slug":"/docs/README/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/installing-open-data-hub/"},"sections":[{"parentId":null,"name":"Installing Open Data Hub version 2","level":1,"index":0,"id":"installing-odh-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing the Open Data Hub Operator version 2","level":2,"index":0,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing Open Data Hub components","level":2,"index":1,"id":"installing-odh-components_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Accessing the Open Data Hub dashboard","level":2,"index":2,"id":"accessing-the-odh-dashboard_installv2"},{"parentId":null,"name":"Installing Open Data Hub version 1","level":1,"index":1,"id":"installing-odh-v1_installv1"},{"parentId":"installing-odh-v1_installv1","name":"Installing the Open Data Hub Operator version 1","level":2,"index":0,"id":"installing-the-odh-operator-v1_installv1"},{"parentId":"installing-odh-v1_installv1","name":"Creating a new project for your Open Data Hub instance","level":2,"index":1,"id":"creating-a-new-project-for-your-odh-instance_installv1"},{"parentId":"installing-odh-v1_installv1","name":"Adding an Open Data Hub instance","level":2,"index":2,"id":"adding-an-odh-instance_installv1"},{"parentId":"installing-odh-v1_installv1","name":"Accessing the Open Data Hub dashboard","level":2,"index":3,"id":"accessing-the-odh-dashboard_installv1"},{"parentId":null,"name":"Working with certificates","level":1,"index":2,"id":"working-with-certificates_certs"},{"parentId":"working-with-certificates_certs","name":"Understanding certificates in Open Data Hub","level":2,"index":0,"id":"understanding-certificates_certs"},{"parentId":"understanding-certificates_certs","name":"How CA bundles are injected","level":3,"index":0,"id":"_how_ca_bundles_are_injected"},{"parentId":"understanding-certificates_certs","name":"How the ConfigMap is managed","level":3,"index":1,"id":"_how_the_configmap_is_managed"},{"parentId":"working-with-certificates_certs","name":"Adding a CA bundle","level":2,"index":1,"id":"adding-a-ca-bundle_certs"},{"parentId":"working-with-certificates_certs","name":"Removing a CA bundle","level":2,"index":2,"id":"removing-a-ca-bundle_certs"},{"parentId":"working-with-certificates_certs","name":"Removing a CA bundle from a namespace","level":2,"index":3,"id":"removing-a-ca-bundle-from-a-namespace_certs"},{"parentId":"working-with-certificates_certs","name":"Managing certificates","level":2,"index":4,"id":"managing-certificates_certs"},{"parentId":"working-with-certificates_certs","name":"Using self-signed certificates with Open Data Hub components","level":2,"index":5,"id":"_using_self_signed_certificates_with_open_data_hub_components"},{"parentId":"_using_self_signed_certificates_with_open_data_hub_components","name":"Using certificates with data science pipelines","level":3,"index":0,"id":"using-certificates-with-data-science-pipelines_certs"},{"parentId":"using-certificates-with-data-science-pipelines_certs","name":"Providing a CA bundle only for data science pipelines","level":4,"index":0,"id":"_providing_a_ca_bundle_only_for_data_science_pipelines"},{"parentId":"_using_self_signed_certificates_with_open_data_hub_components","name":"Using certificates with workbenches","level":3,"index":1,"id":"using-certificates-with-workbenches_certs"},{"parentId":"using-certificates-with-workbenches_certs","name":"Creating data science pipelines with Elyra and self-signed certificates","level":4,"index":0,"id":"_creating_data_science_pipelines_with_elyra_and_self_signed_certificates"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/getting-started-with-open-data-hub/"},"sections":[{"parentId":null,"name":"Logging in to Open Data Hub","level":1,"index":0,"id":"logging-in_get-started"},{"parentId":null,"name":"The Open Data Hub user interface","level":1,"index":1,"id":"user-interface_get-started"},{"parentId":"user-interface_get-started","name":"Global navigation","level":2,"index":0,"id":"_global_navigation"},{"parentId":"user-interface_get-started","name":"Side navigation","level":2,"index":1,"id":"_side_navigation"},{"parentId":null,"name":"Notifications in Open Data Hub","level":1,"index":2,"id":"notifications_get-started"},{"parentId":null,"name":"Creating a data science project","level":1,"index":3,"id":"creating-a-data-science-project_get-started"},{"parentId":null,"name":"Creating a project workbench","level":1,"index":4,"id":"creating-a-project-workbench_get-started"},{"parentId":"creating-a-project-workbench_get-started","name":"Launching Jupyter and starting a notebook server","level":2,"index":0,"id":"launching-jupyter-and-starting-a-notebook-server_get-started"},{"parentId":"creating-a-project-workbench_get-started","name":"Options for notebook server environments","level":2,"index":1,"id":"options-for-notebook-server-environments_get-started"},{"parentId":null,"name":"Tutorials for data scientists","level":1,"index":5,"id":"tutorials-for-data-scientists_get-started"},{"parentId":"tutorials-for-data-scientists_get-started","name":"Accessing tutorials","level":2,"index":0,"id":"accessing-tutorials_get-started"},{"parentId":null,"name":"Configuring your IDE","level":1,"index":6,"id":"configuring-your-ide_get-started"},{"parentId":"configuring-your-ide_get-started","name":"Configuring your code-server workbench","level":2,"index":0,"id":"_configuring_your_code_server_workbench"},{"parentId":"_configuring_your_code_server_workbench","name":"Installing extensions with code-server","level":3,"index":0,"id":"_installing_extensions_with_code_server"},{"parentId":"_installing_extensions_with_code_server","name":"Extensions","level":4,"index":0,"id":"_extensions"},{"parentId":null,"name":"Enabling services connected to Open Data Hub","level":1,"index":7,"id":"enabling-services_get-started"},{"parentId":null,"name":"Disabling applications connected to Open Data Hub","level":1,"index":8,"id":"disabling-applications-connected_get-started"},{"parentId":"disabling-applications-connected_get-started","name":"Removing disabled applications from Open Data Hub","level":2,"index":0,"id":"removing-disabled-applications_get-started"},{"parentId":null,"name":"Support requirements and limitations","level":1,"index":9,"id":"support-requirements-and-limitations_requirements"},{"parentId":"support-requirements-and-limitations_requirements","name":"Supported browsers","level":2,"index":0,"id":"supported-browsers_requirements"},{"parentId":"support-requirements-and-limitations_requirements","name":"Supported services","level":2,"index":1,"id":"supported-services_requirements"},{"parentId":"support-requirements-and-limitations_requirements","name":"Supported packages","level":2,"index":2,"id":"supported-packages_requirements"},{"parentId":null,"name":"Common questions","level":1,"index":10,"id":"common-questions_get-started"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/managing-resources/"},"sections":[{"parentId":null,"name":"Customizing the dashboard","level":1,"index":0,"id":"customizing-the-dashboard"},{"parentId":"customizing-the-dashboard","name":"Editing the dashboard configuration file","level":2,"index":0,"id":"editing-the-dashboard-configuration-file_dashboard"},{"parentId":"customizing-the-dashboard","name":"Dashboard configuration options","level":2,"index":1,"id":"ref-dashboard-configuration-options_dashboard"},{"parentId":null,"name":"Managing applications that show in the dashboard","level":1,"index":1,"id":"managing-applications-that-show-in-the-dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Adding an application to the dashboard","level":2,"index":0,"id":"adding-an-application-to-the-dashboard_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Preventing users from adding applications to the dashboard","level":2,"index":1,"id":"preventing-users-from-adding-applications-to-the-dashboard_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Showing or hiding information about enabled applications","level":2,"index":2,"id":"showing-hiding-information-about-enabled-applications_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Hiding the default Jupyter application","level":2,"index":3,"id":"hiding-the-default-jupyter-application_dashboard"},{"parentId":null,"name":"Managing cluster resources","level":1,"index":2,"id":"managing-cluster-resources"},{"parentId":"managing-cluster-resources","name":"Configuring the default PVC size for your cluster","level":2,"index":0,"id":"configuring-the-default-pvc-size-for-your-cluster_managing-resources"},{"parentId":"managing-cluster-resources","name":"Restoring the default PVC size for your cluster","level":2,"index":1,"id":"restoring-the-default-pvc-size-for-your-cluster_managing-resources"},{"parentId":"managing-cluster-resources","name":"Overview of accelerators","level":2,"index":2,"id":"overview-of-accelerators_managing-resources"},{"parentId":"overview-of-accelerators_managing-resources","name":"Enabling GPU support in Open Data Hub","level":3,"index":0,"id":"enabling-gpu-support_managing-resources"},{"parentId":"overview-of-accelerators_managing-resources","name":"Enabling Habana Gaudi devices","level":3,"index":1,"id":"enabling-habana-gaudi-devices_managing-resources"},{"parentId":"managing-cluster-resources","name":"Allocating additional resources to Open Data Hub users","level":2,"index":3,"id":"allocating-additional-resources-to-data-science-users_managing-resources"},{"parentId":null,"name":"Managing Jupyter notebook servers","level":1,"index":3,"id":"managing-notebook-servers"},{"parentId":"managing-notebook-servers","name":"Accessing the Jupyter administration interface","level":2,"index":0,"id":"accessing-the-jupyter-administration-interface_managing-resources"},{"parentId":"managing-notebook-servers","name":"Starting notebook servers owned by other users","level":2,"index":1,"id":"starting-notebook-servers-owned-by-other-users_managing-resources"},{"parentId":"managing-notebook-servers","name":"Accessing notebook servers owned by other users","level":2,"index":2,"id":"accessing-notebook-servers-owned-by-other-users_managing-resources"},{"parentId":"managing-notebook-servers","name":"Stopping notebook servers owned by other users","level":2,"index":3,"id":"stopping-notebook-servers-owned-by-other-users_managing-resources"},{"parentId":"managing-notebook-servers","name":"Stopping idle notebooks","level":2,"index":4,"id":"stopping-idle-notebooks_managing-resources"},{"parentId":"managing-notebook-servers","name":"Adding notebook pod tolerations","level":2,"index":5,"id":"adding-notebook-pod-tolerations_managing-resources"},{"parentId":"managing-notebook-servers","name":"Configuring a custom notebook image","level":2,"index":6,"id":"configuring-a-custom-notebook-image_managing-resources"},{"parentId":null,"name":"Backing up storage data","level":1,"index":4,"id":"backing-up-storage-data_managing-resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/managing-users/"},"sections":[{"parentId":null,"name":"Adding users","level":1,"index":0,"id":"adding-users"},{"parentId":"adding-users","name":"Overview of user types and permissions","level":2,"index":0,"id":"overview-of-user-types-and-permissions_managing-users"},{"parentId":"adding-users","name":"Defining Open Data Hub administrator and user groups","level":2,"index":1,"id":"defining-data-science-admin-and-user-groups_managing-users"},{"parentId":"adding-users","name":"Adding users to specialized Open Data Hub user groups","level":2,"index":2,"id":"adding-users-to-specialized-data-science-user-groups_managing-users"},{"parentId":"adding-users","name":"Viewing Open Data Hub users","level":2,"index":3,"id":"viewing-data-science-users_managing-users"},{"parentId":null,"name":"Deleting users and their resources","level":1,"index":1,"id":"deleting-users"},{"parentId":"deleting-users","name":"About deleting users and their resources","level":2,"index":0,"id":"about-deleting-users-and-resources_managing-users"},{"parentId":"deleting-users","name":"Backing up storage data","level":2,"index":1,"id":"backing-up-storage-data_managing-users"},{"parentId":"deleting-users","name":"Stopping notebook servers owned by other users","level":2,"index":2,"id":"stopping-notebook-servers-owned-by-other-users_managing-users"},{"parentId":"deleting-users","name":"Revoking user access to Jupyter","level":2,"index":3,"id":"revoking-user-access-to-jupyter_managing-users"},{"parentId":"deleting-users","name":"Cleaning up after deleting users","level":2,"index":4,"id":"cleaning-up-after-deleting-users_managing-users"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/serving-models/"},"sections":[{"parentId":null,"name":"About model serving","level":1,"index":0,"id":"about-model-serving_about-model-serving"},{"parentId":null,"name":"Serving small and medium-sized models","level":1,"index":1,"id":"serving-small-and-medium-sized-models_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Configuring model servers","level":2,"index":0,"id":"_configuring_model_servers"},{"parentId":"_configuring_model_servers","name":"Enabling the multi-model serving platform","level":3,"index":0,"id":"enabling-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a custom model-serving runtime for the multi-model serving platform","level":3,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a model server for the multi-model serving platform","level":3,"index":2,"id":"adding-a-model-server-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Deleting a model server","level":3,"index":3,"id":"deleting-a-model-server_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Working with deployed models","level":2,"index":1,"id":"_working_with_deployed_models"},{"parentId":"_working_with_deployed_models","name":"Deploying a model by using the multi-model serving platform","level":3,"index":0,"id":"deploying-a-model-using-the-multi-model-serving-platform_model-serving"},{"parentId":"_working_with_deployed_models","name":"Viewing a deployed model","level":3,"index":1,"id":"viewing-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Updating the deployment properties of a deployed model","level":3,"index":2,"id":"updating-the-deployment-properties-of-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Deleting a deployed model","level":3,"index":3,"id":"deleting-a-deployed-model_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Configuring monitoring for the multi-model serving platform","level":2,"index":2,"id":"configuring-monitoring-for-the-multi-model-serving-platform_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Viewing model-serving runtime metrics for the multi-model serving platform","level":2,"index":3,"id":"viewing-metrics-for-the-multi-model-serving-platform_model-serving"},{"parentId":"serving-small-and-medium-sized-models_model-serving","name":"Monitoring model performance","level":2,"index":4,"id":"_monitoring_model_performance"},{"parentId":"_monitoring_model_performance","name":"Viewing performance metrics for all models on a model server","level":3,"index":0,"id":"viewing-performance-metrics-for-model-server_model-serving"},{"parentId":"_monitoring_model_performance","name":"Viewing HTTP request metrics for a deployed model","level":3,"index":1,"id":"viewing-http-request-metrics-for-a-deployed-model_model-serving"},{"parentId":null,"name":"Serving large models","level":1,"index":2,"id":"serving-large-models_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"About the single-model serving platform","level":2,"index":0,"id":"about-the-single-model-serving-platform_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Installing KServe","level":2,"index":1,"id":"installing-kserve_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Deploying models by using the single-model serving platform","level":2,"index":2,"id":"deploying-models-using-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Enabling the single-model serving platform","level":3,"index":0,"id":"enabling-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Adding a custom model-serving runtime for the single-model serving platform","level":3,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Deploying models on the single-model serving platform","level":3,"index":2,"id":"deploying-models-on-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Accessing the inference endpoint for a deployed model","level":3,"index":3,"id":"accessing-inference-endpoint-for-deployed-model_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Configuring monitoring for the single-model serving platform","level":2,"index":3,"id":"configuring-monitoring-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Viewing model-serving runtime metrics for the single-model serving platform","level":2,"index":4,"id":"viewing-metrics-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"serving-large-models_serving-large-models","name":"Performance tuning on the single-model serving platform","level":2,"index":5,"id":"_performance_tuning_on_the_single_model_serving_platform"},{"parentId":"_performance_tuning_on_the_single_model_serving_platform","name":"Resolving CUDA out-of-memory errors","level":3,"index":0,"id":"resolving-cuda-oom-errors-for-the-single-model-serving-platform_serving-large-models"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/monitoring-data-science-models/"},"sections":[{"parentId":null,"name":"Enabling the TrustyAI Service for a data science project","level":1,"index":0,"id":"enabling-trustyai-service_monitor"},{"parentId":"enabling-trustyai-service_monitor","name":"Enabling the TrustyAI Service by using the dashboard","level":2,"index":0,"id":"enabling-trustyai-service-using-dashboard_monitor"},{"parentId":"enabling-trustyai-service_monitor","name":"Enabling the TrustyAI Service by using the CLI","level":2,"index":1,"id":"enabling-trustyai-service-using-cli_monitor"},{"parentId":null,"name":"Authenticating the TrustyAI service","level":1,"index":1,"id":"authenticating-trustyai-service_monitor"},{"parentId":null,"name":"Sending training data to a model","level":1,"index":2,"id":"sending-training-data-to-a-model_monitor"},{"parentId":null,"name":"Configuring bias metrics for a model","level":1,"index":3,"id":"configuring-bias-metrics-for-a-model_bias-monitoring"},{"parentId":"configuring-bias-metrics-for-a-model_bias-monitoring","name":"Creating a bias metric","level":2,"index":0,"id":"creating-a-bias-metric_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the dashboard","level":3,"index":0,"id":"creating-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the CLI","level":3,"index":1,"id":"creating-a-bias-metric-using-cli_bias-monitoring"},{"parentId":"configuring-bias-metrics-for-a-model_bias-monitoring","name":"Duplicating a bias metric","level":2,"index":1,"id":"duplicating-a-bias-metric_bias-monitoring"},{"parentId":"configuring-bias-metrics-for-a-model_bias-monitoring","name":"Deleting a bias metric","level":2,"index":2,"id":"deleting-a-bias-metric_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the dashboard","level":3,"index":0,"id":"deleting-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the CLI","level":3,"index":1,"id":"deleting-a-bias-metric-using-cli_bias-monitoring"},{"parentId":null,"name":"Viewing bias metrics for a model","level":1,"index":4,"id":"viewing-bias-metrics_monitor"},{"parentId":null,"name":"Supported bias metrics","level":1,"index":5,"id":"supported-bias-metrics_monitor"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/upgrading-open-data-hub/"},"sections":[{"parentId":null,"name":"Overview of upgrading Open Data Hub","level":1,"index":0,"id":"overview-of-upgrading-odh_upgrade"},{"parentId":null,"name":"Requirements for upgrading Open Data Hub","level":1,"index":1,"id":"requirements-for-upgrading-odh_upgrade"},{"parentId":null,"name":"Upgrading Open Data Hub version 1 to version 2","level":1,"index":2,"id":"upgrading-odh-v1-to-v2_upgradev1"},{"parentId":"upgrading-odh-v1-to-v2_upgradev1","name":"Upgrading the Open Data Hub Operator version 1","level":2,"index":0,"id":"upgrading-the-odh-operator-v1_upgradev1"},{"parentId":"upgrading-odh-v1-to-v2_upgradev1","name":"Installing Open Data Hub components","level":2,"index":1,"id":"installing-odh-components_upgradev1"},{"parentId":"upgrading-odh-v1-to-v2_upgradev1","name":"Accessing the Open Data Hub dashboard","level":2,"index":2,"id":"accessing-the-odh-dashboard_upgradev1"},{"parentId":null,"name":"Upgrading Open Data Hub version 2.0 to version 2.2","level":1,"index":3,"id":"upgrading-odh-v2_upgradev2"},{"parentId":"upgrading-odh-v2_upgradev2","name":"Cleaning up resources","level":2,"index":0,"id":"cleaning-up-resources_upgradev2"},{"parentId":"upgrading-odh-v2_upgradev2","name":"Installing Open Data Hub version 2","level":2,"index":1,"id":"installing-odh-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing the Open Data Hub Operator version 2","level":3,"index":0,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing Open Data Hub components","level":3,"index":1,"id":"installing-odh-components_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Accessing the Open Data Hub dashboard","level":3,"index":2,"id":"accessing-the-odh-dashboard_installv2"},{"parentId":"upgrading-odh-v2_upgradev2","name":"Adding a CA bundle after upgrading","level":2,"index":2,"id":"adding-a-ca-bundle-after-upgrading_upgradev2"},{"parentId":null,"name":"Adding a CA bundle after upgrading","level":1,"index":4,"id":"adding-a-ca-bundle-after-upgrading_upgradev2"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-on-data-science-projects/"},"sections":[{"parentId":null,"name":"Creating and importing notebooks","level":1,"index":0,"id":"creating-and-importing-notebooks_notebooks"},{"parentId":"creating-and-importing-notebooks_notebooks","name":"Creating a new notebook","level":2,"index":0,"id":"creating-a-new-notebook_notebooks"},{"parentId":"creating-a-new-notebook_notebooks","name":"Notebook images for data scientists","level":3,"index":0,"id":"notebook-images-for-data-scientists_notebooks"},{"parentId":"creating-and-importing-notebooks_notebooks","name":"Uploading an existing notebook file from local storage","level":2,"index":1,"id":"uploading-an-existing-notebook-file-from-local-storage_notebooks"},{"parentId":"creating-and-importing-notebooks_notebooks","name":"Uploading an existing notebook file from a Git repository using JupyterLab","level":2,"index":2,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_notebooks"},{"parentId":"creating-and-importing-notebooks_notebooks","name":"Uploading an existing notebook file from a Git repository using the command line interface","level":2,"index":3,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_notebooks"},{"parentId":null,"name":"Collaborating on notebooks using Git","level":1,"index":1,"id":"collaborating-on-notebooks-using-git_git-collab"},{"parentId":"collaborating-on-notebooks-using-git_git-collab","name":"Uploading an existing notebook file from a Git repository using JupyterLab","level":2,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_git-collab"},{"parentId":"collaborating-on-notebooks-using-git_git-collab","name":"Uploading an existing notebook file from a Git repository using the command line interface","level":2,"index":1,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_git-collab"},{"parentId":"collaborating-on-notebooks-using-git_git-collab","name":"Updating your project with changes from a remote Git repository","level":2,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_git-collab"},{"parentId":"collaborating-on-notebooks-using-git_git-collab","name":"Pushing project changes to a Git repository","level":2,"index":3,"id":"pushing-project-changes-to-a-git-repository_git-collab"},{"parentId":null,"name":"Working on data science projects","level":1,"index":2,"id":"working-on-data-science-projects_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Using data science projects","level":2,"index":0,"id":"_using_data_science_projects"},{"parentId":"_using_data_science_projects","name":"Creating a data science project","level":3,"index":0,"id":"creating-a-data-science-project_nb-server"},{"parentId":"_using_data_science_projects","name":"Updating a data science project","level":3,"index":1,"id":"updating-a-data-science-project_nb-server"},{"parentId":"_using_data_science_projects","name":"Deleting a data science project","level":3,"index":2,"id":"deleting-a-data-science-project_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Using project workbenches","level":2,"index":1,"id":"_using_project_workbenches"},{"parentId":"_using_project_workbenches","name":"Creating a project workbench","level":3,"index":0,"id":"creating-a-project-workbench_nb-server"},{"parentId":"_using_project_workbenches","name":"Starting a workbench","level":3,"index":1,"id":"starting-a-workbench_nb-server"},{"parentId":"_using_project_workbenches","name":"Updating a project workbench","level":3,"index":2,"id":"updating-a-project-workbench_nb-server"},{"parentId":"_using_project_workbenches","name":"Deleting a workbench from a data science project","level":3,"index":3,"id":"deleting-a-workbench-from-a-data-science-project_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Using data connections","level":2,"index":2,"id":"_using_data_connections"},{"parentId":"_using_data_connections","name":"Adding a data connection to your data science project","level":3,"index":0,"id":"adding-a-data-connection-to-your-data-science-project_nb-server"},{"parentId":"_using_data_connections","name":"Deleting a data connection","level":3,"index":1,"id":"deleting-a-data-connection_nb-server"},{"parentId":"_using_data_connections","name":"Updating a connected data source","level":3,"index":2,"id":"updating-a-connected-data-source_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Configuring cluster storage","level":2,"index":3,"id":"_configuring_cluster_storage"},{"parentId":"_configuring_cluster_storage","name":"Adding cluster storage to your data science project","level":3,"index":0,"id":"adding-cluster-storage-to-your-data-science-project_nb-server"},{"parentId":"_configuring_cluster_storage","name":"Updating cluster storage","level":3,"index":1,"id":"updating-cluster-storage_nb-server"},{"parentId":"_configuring_cluster_storage","name":"Deleting cluster storage from a data science project","level":3,"index":2,"id":"deleting-cluster-storage-from-a-data-science-project_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Configuring data science pipelines","level":2,"index":4,"id":"_configuring_data_science_pipelines"},{"parentId":"_configuring_data_science_pipelines","name":"Configuring a pipeline server","level":3,"index":0,"id":"configuring-a-pipeline-server_nb-server"},{"parentId":"_configuring_data_science_pipelines","name":"Defining a pipeline","level":3,"index":1,"id":"defining-a-pipeline_nb-server"},{"parentId":"_configuring_data_science_pipelines","name":"Importing a data science pipeline","level":3,"index":2,"id":"importing-a-data-science-pipeline_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Configuring access to data science projects","level":2,"index":5,"id":"_configuring_access_to_data_science_projects"},{"parentId":"_configuring_access_to_data_science_projects","name":"Configuring access to data science projects","level":3,"index":0,"id":"configuring-access-to-data-science-projects_nb-server"},{"parentId":"_configuring_access_to_data_science_projects","name":"Sharing access to a data science project","level":3,"index":1,"id":"sharing-access-to-a-data-science-project_nb-server"},{"parentId":"_configuring_access_to_data_science_projects","name":"Updating access to a data science project","level":3,"index":2,"id":"updating-access-to-a-data-science-project_nb-server"},{"parentId":"_configuring_access_to_data_science_projects","name":"Removing access to a data science project","level":3,"index":3,"id":"removing-access-to-a-data-science-project_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Viewing Python packages installed on your notebook server","level":2,"index":6,"id":"viewing-python-packages-installed-on-your-notebook-server_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Installing Python packages on your notebook server","level":2,"index":7,"id":"installing-python-packages-on-your-notebook-server_nb-server"},{"parentId":"working-on-data-science-projects_nb-server","name":"Updating notebook server settings by restarting your server","level":2,"index":8,"id":"updating-notebook-server-settings-by-restarting-your-server_nb-server"},{"parentId":null,"name":"Working with data science pipelines","level":1,"index":3,"id":"working-with-data-science-pipelines_ds-pipelines"},{"parentId":"working-with-data-science-pipelines_ds-pipelines","name":"Enabling Data Science Pipelines 2.0","level":2,"index":0,"id":"enabling-data-science-pipelines-2_ds-pipelines"},{"parentId":"enabling-data-science-pipelines-2_ds-pipelines","name":"Installing Open Data Hub with DSP 2.0","level":3,"index":0,"id":"_installing_open_data_hub_with_dsp_2_0"},{"parentId":"enabling-data-science-pipelines-2_ds-pipelines","name":"Upgrading to DSP 2.0","level":3,"index":1,"id":"_upgrading_to_dsp_2_0"},{"parentId":"working-with-data-science-pipelines_ds-pipelines","name":"Managing data science pipelines","level":2,"index":1,"id":"_managing_data_science_pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Configuring a pipeline server","level":3,"index":0,"id":"configuring-a-pipeline-server_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Defining a pipeline","level":3,"index":1,"id":"defining-a-pipeline_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Importing a data science pipeline","level":3,"index":2,"id":"importing-a-data-science-pipeline_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Downloading a data science pipeline version","level":3,"index":3,"id":"downloading-a-data-science-pipeline-version_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Deleting a data science pipeline","level":3,"index":4,"id":"deleting-a-data-science-pipeline_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Deleting a pipeline server","level":3,"index":5,"id":"deleting-a-pipeline-server_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Viewing the details of a pipeline server","level":3,"index":6,"id":"viewing-the-details-of-a-pipeline-server_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Viewing existing pipelines","level":3,"index":7,"id":"viewing-existing-pipelines_ds-pipelines"},{"parentId":"working-with-data-science-pipelines_ds-pipelines","name":"Managing pipeline runs","level":2,"index":2,"id":"_managing_pipeline_runs"},{"parentId":"_managing_pipeline_runs","name":"Overview of pipeline runs","level":3,"index":0,"id":"overview-of-pipeline-runs_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Viewing active pipeline runs","level":3,"index":1,"id":"viewing-active-pipeline-runs_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Executing a pipeline run","level":3,"index":2,"id":"executing-a-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Stopping an active pipeline run","level":3,"index":3,"id":"stopping-an-active-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Duplicating an active pipeline run","level":3,"index":4,"id":"duplicating-an-active-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Viewing scheduled pipeline runs","level":3,"index":5,"id":"viewing-scheduled-pipeline-runs_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Scheduling a pipeline run using a cron job","level":3,"index":6,"id":"scheduling-a-pipeline-run-using-a-cron-job_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Scheduling a pipeline run","level":3,"index":7,"id":"scheduling-a-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Duplicating a scheduled pipeline run","level":3,"index":8,"id":"duplicating-a-scheduled-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Deleting a scheduled pipeline run","level":3,"index":9,"id":"deleting-a-scheduled-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Viewing the details of a pipeline run","level":3,"index":10,"id":"viewing-the-details-of-a-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Viewing archived pipeline runs","level":3,"index":11,"id":"viewing-archived-pipeline-runs_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Archiving a pipeline run","level":3,"index":12,"id":"archiving-a-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Restoring an archived pipeline run","level":3,"index":13,"id":"restoring-an-archived-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Deleting an archived pipeline run","level":3,"index":14,"id":"deleting-an-archived-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Duplicating an archived pipeline run","level":3,"index":15,"id":"duplicating-an-archived-pipeline-run_ds-pipelines"},{"parentId":"working-with-data-science-pipelines_ds-pipelines","name":"Working with pipeline logs","level":2,"index":3,"id":"_working_with_pipeline_logs"},{"parentId":"_working_with_pipeline_logs","name":"About pipeline logs","level":3,"index":0,"id":"about-pipeline-logs_ds-pipelines"},{"parentId":"_working_with_pipeline_logs","name":"Viewing pipeline step logs","level":3,"index":1,"id":"viewing-pipeline-step-logs_ds-pipelines"},{"parentId":"_working_with_pipeline_logs","name":"Downloading pipeline step logs","level":3,"index":2,"id":"downloading-pipeline-step-logs_ds-pipelines"},{"parentId":"working-with-data-science-pipelines_ds-pipelines","name":"Working with pipelines in JupyterLab","level":2,"index":4,"id":"_working_with_pipelines_in_jupyterlab"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Overview of pipelines in JupyterLab","level":3,"index":0,"id":"overview-of-pipelines-in-jupyterlab_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Accessing the pipeline editor","level":3,"index":1,"id":"accessing-the-pipeline-editor_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Creating a runtime configuration","level":3,"index":2,"id":"creating-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Updating a runtime configuration","level":3,"index":3,"id":"updating-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Deleting a runtime configuration","level":3,"index":4,"id":"deleting-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Duplicating a runtime configuration","level":3,"index":5,"id":"duplicating-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Running a pipeline in JupyterLab","level":3,"index":6,"id":"running-a-pipeline-in-jupyterlab_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Exporting a pipeline in JupyterLab","level":3,"index":7,"id":"exporting-a-pipeline-in-jupyterlab_ds-pipelines"},{"parentId":"working-with-data-science-pipelines_ds-pipelines","name":"Additional resources","level":2,"index":5,"id":"_additional_resources"},{"parentId":null,"name":"Working with accelerators","level":1,"index":4,"id":"working-with-accelerators_accelerators"},{"parentId":"working-with-accelerators_accelerators","name":"Overview of accelerators","level":2,"index":0,"id":"overview-of-accelerators_accelerators"},{"parentId":"working-with-accelerators_accelerators","name":"Working with accelerator profiles","level":2,"index":1,"id":"working-with-accelerator-profiles_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Creating an accelerator profile","level":3,"index":0,"id":"creating-an-accelerator-profile_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Updating an accelerator profile","level":3,"index":1,"id":"updating-an-accelerator-profile_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Deleting an accelerator profile","level":3,"index":2,"id":"deleting-an-accelerator-profile_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Viewing accelerator profiles","level":3,"index":3,"id":"viewing-accelerator-profiles_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Configuring a recommended accelerator for notebook images","level":3,"index":4,"id":"configuring-a-recommended-accelerator-for-notebook-images_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Configuring a recommended accelerator for serving runtimes","level":3,"index":5,"id":"configuring-a-recommended-accelerator-for-serving-runtimes_accelerators"},{"parentId":"working-with-accelerators_accelerators","name":"Habana Gaudi integration","level":2,"index":2,"id":"habana-gaudi-integration_accelerators"},{"parentId":"habana-gaudi-integration_accelerators","name":"Enabling Habana Gaudi devices","level":3,"index":0,"id":"enabling-habana-gaudi-devices_accelerators"},{"parentId":null,"name":"Troubleshooting common problems in Jupyter for administrators","level":1,"index":5,"id":"troubleshooting-common-problems-in-jupyter-for-administrators_accelerators"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_accelerators","name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":2,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_accelerators","name":"A user&#8217;s notebook server does not start","level":2,"index":1,"id":"_a_users_notebook_server_does_not_start"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-administrators_accelerators","name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":2,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"},{"parentId":null,"name":"Troubleshooting common problems in Jupyter for users","level":1,"index":6,"id":"troubleshooting-common-problems-in-jupyter-for-users_accelerators"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-users_accelerators","name":"I see a <strong>403: Forbidden</strong> error when I log in to Jupyter","level":2,"index":0,"id":"_i_see_a_403_forbidden_error_when_i_log_in_to_jupyter"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-users_accelerators","name":"My notebook server does not start","level":2,"index":1,"id":"_my_notebook_server_does_not_start"},{"parentId":"troubleshooting-common-problems-in-jupyter-for-users_accelerators","name":"I see a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when I run my notebook cells","level":2,"index":2,"id":"_i_see_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_i_run_my_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/_artifacts/document-attributes-global/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-distributed-workloads/"},"sections":[{"parentId":null,"name":"Overview of distributed workloads","level":1,"index":0,"id":"overview-of-distributed-workloads_distributed-workloads"},{"parentId":null,"name":"Configuring distributed workloads","level":1,"index":1,"id":"configuring-distributed-workloads_distributed-workloads"},{"parentId":"configuring-distributed-workloads_distributed-workloads","name":"Configuring the distributed workloads components","level":2,"index":0,"id":"configuring-the-distributed-workloads-components_distributed-workloads"},{"parentId":"configuring-distributed-workloads_distributed-workloads","name":"Configuring quota management for distributed workloads","level":2,"index":1,"id":"configuring-quota-management-for-distributed-workloads_distributed-workloads"},{"parentId":null,"name":"Running distributed workloads","level":1,"index":2,"id":"running-distributed-workloads_distributed-workloads"},{"parentId":"running-distributed-workloads_distributed-workloads","name":"Running distributed data science workloads from notebooks","level":2,"index":0,"id":"running-distributed-data-science-workloads-from-notebooks_distributed-workloads"},{"parentId":"running-distributed-workloads_distributed-workloads","name":"Running distributed data science workloads from data science pipelines","level":2,"index":1,"id":"running-distributed-data-science-workloads-from-ds-pipelines_distributed-workloads"},{"parentId":null,"name":"Monitoring distributed workloads","level":1,"index":3,"id":"monitoring-distributed-workloads_distributed-workloads"},{"parentId":"monitoring-distributed-workloads_distributed-workloads","name":"Viewing project metrics for distributed workloads","level":2,"index":0,"id":"viewing-project-metrics-for-distributed-workloads_distributed-workloads"},{"parentId":"monitoring-distributed-workloads_distributed-workloads","name":"Viewing the status of distributed workloads","level":2,"index":1,"id":"viewing-the-status-of-distributed-workloads_distributed-workloads"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/adding-users/"},"sections":[{"parentId":null,"name":"Overview of user types and permissions","level":1,"index":0,"id":"overview-of-user-types-and-permissions_{context}"},{"parentId":null,"name":"Defining {productname-short} administrator and user groups","level":1,"index":1,"id":"defining-data-science-admin-and-user-groups_{context}"},{"parentId":null,"name":"Adding users to specialized {productname-short} user groups","level":1,"index":2,"id":"adding-users-to-specialized-data-science-user-groups_{context}"},{"parentId":null,"name":"Viewing {productname-short} users","level":1,"index":3,"id":"viewing-data-science-users_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/collaborating-on-notebooks-using-git/"},"sections":[{"parentId":null,"name":"Uploading an existing notebook file from a Git repository using JupyterLab","level":1,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_git-collab"},{"parentId":null,"name":"Uploading an existing notebook file from a Git repository using the command line interface","level":1,"index":1,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_git-collab"},{"parentId":null,"name":"Updating your project with changes from a remote Git repository","level":1,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_git-collab"},{"parentId":null,"name":"Pushing project changes to a Git repository","level":1,"index":3,"id":"pushing-project-changes-to-a-git-repository_git-collab"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/creating-and-importing-notebooks/"},"sections":[{"parentId":null,"name":"Creating a new notebook","level":1,"index":0,"id":"creating-a-new-notebook_notebooks"},{"parentId":"creating-a-new-notebook_notebooks","name":"Notebook images for data scientists","level":2,"index":0,"id":"notebook-images-for-data-scientists_notebooks"},{"parentId":null,"name":"Uploading an existing notebook file from local storage","level":1,"index":1,"id":"uploading-an-existing-notebook-file-from-local-storage_notebooks"},{"parentId":null,"name":"Uploading an existing notebook file from a Git repository using JupyterLab","level":1,"index":2,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_notebooks"},{"parentId":null,"name":"Uploading an existing notebook file from a Git repository using the command line interface","level":1,"index":3,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface_notebooks"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-bias-metrics-for-a-model/"},"sections":[{"parentId":null,"name":"Creating a bias metric","level":1,"index":0,"id":"creating-a-bias-metric_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the dashboard","level":2,"index":0,"id":"creating-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the CLI","level":2,"index":1,"id":"creating-a-bias-metric-using-cli_bias-monitoring"},{"parentId":null,"name":"Duplicating a bias metric","level":1,"index":1,"id":"duplicating-a-bias-metric_bias-monitoring"},{"parentId":null,"name":"Deleting a bias metric","level":1,"index":2,"id":"deleting-a-bias-metric_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the dashboard","level":2,"index":0,"id":"deleting-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the CLI","level":2,"index":1,"id":"deleting-a-bias-metric-using-cli_bias-monitoring"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/deleting-users/"},"sections":[{"parentId":null,"name":"About deleting users and their resources","level":1,"index":0,"id":"about-deleting-users-and-resources_{context}"},{"parentId":null,"name":"Backing up storage data","level":1,"index":1,"id":"backing-up-storage-data_{context}"},{"parentId":null,"name":"Stopping notebook servers owned by other users","level":1,"index":2,"id":"stopping-notebook-servers-owned-by-other-users_{context}"},{"parentId":null,"name":"Revoking user access to Jupyter","level":1,"index":3,"id":"revoking-user-access-to-jupyter_{context}"},{"parentId":null,"name":"Cleaning up after deleting users","level":1,"index":4,"id":"cleaning-up-after-deleting-users_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/customizing-the-dashboard/"},"sections":[{"parentId":null,"name":"Editing the dashboard configuration file","level":1,"index":0,"id":"editing-the-dashboard-configuration-file_dashboard"},{"parentId":null,"name":"Dashboard configuration options","level":1,"index":1,"id":"ref-dashboard-configuration-options_dashboard"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/installing-odh-v1/"},"sections":[{"parentId":null,"name":"Installing the Open Data Hub Operator version 1","level":1,"index":0,"id":"installing-the-odh-operator-v1_installv1"},{"parentId":null,"name":"Creating a new project for your Open Data Hub instance","level":1,"index":1,"id":"creating-a-new-project-for-your-odh-instance_installv1"},{"parentId":null,"name":"Adding an Open Data Hub instance","level":1,"index":2,"id":"adding-an-odh-instance_installv1"},{"parentId":null,"name":"Accessing the Open Data Hub dashboard","level":1,"index":3,"id":"accessing-the-odh-dashboard_installv1"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/installing-odh-v2/"},"sections":[{"parentId":null,"name":"Installing the Open Data Hub Operator version 2","level":1,"index":0,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":null,"name":"Installing Open Data Hub components","level":1,"index":1,"id":"installing-odh-components_installv2"},{"parentId":null,"name":"Accessing the Open Data Hub dashboard","level":1,"index":2,"id":"accessing-the-odh-dashboard_installv2"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-cluster-resources/"},"sections":[{"parentId":null,"name":"Configuring the default PVC size for your cluster","level":1,"index":0,"id":"configuring-the-default-pvc-size-for-your-cluster_{context}"},{"parentId":null,"name":"Restoring the default PVC size for your cluster","level":1,"index":1,"id":"restoring-the-default-pvc-size-for-your-cluster_{context}"},{"parentId":null,"name":"Overview of accelerators","level":1,"index":2,"id":"overview-of-accelerators_{context}"},{"parentId":"overview-of-accelerators_{context}","name":"Enabling GPU support in {productname-short}","level":2,"index":0,"id":"enabling-gpu-support_{context}"},{"parentId":"overview-of-accelerators_{context}","name":"Enabling Habana Gaudi devices","level":2,"index":1,"id":"enabling-habana-gaudi-devices_{context}"},{"parentId":null,"name":"Allocating additional resources to {productname-short} users","level":1,"index":3,"id":"allocating-additional-resources-to-data-science-users_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-applications-that-show-in-the-dashboard/"},"sections":[{"parentId":null,"name":"Adding an application to the dashboard","level":1,"index":0,"id":"adding-an-application-to-the-dashboard_dashboard"},{"parentId":null,"name":"Preventing users from adding applications to the dashboard","level":1,"index":1,"id":"preventing-users-from-adding-applications-to-the-dashboard_dashboard"},{"parentId":null,"name":"Showing or hiding information about enabled applications","level":1,"index":2,"id":"showing-hiding-information-about-enabled-applications_dashboard"},{"parentId":null,"name":"Hiding the default Jupyter application","level":1,"index":3,"id":"hiding-the-default-jupyter-application_dashboard"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-notebook-servers/"},"sections":[{"parentId":null,"name":"Accessing the Jupyter administration interface","level":1,"index":0,"id":"accessing-the-jupyter-administration-interface_{context}"},{"parentId":null,"name":"Starting notebook servers owned by other users","level":1,"index":1,"id":"starting-notebook-servers-owned-by-other-users_{context}"},{"parentId":null,"name":"Accessing notebook servers owned by other users","level":1,"index":2,"id":"accessing-notebook-servers-owned-by-other-users_{context}"},{"parentId":null,"name":"Stopping notebook servers owned by other users","level":1,"index":3,"id":"stopping-notebook-servers-owned-by-other-users_{context}"},{"parentId":null,"name":"Stopping idle notebooks","level":1,"index":4,"id":"stopping-idle-notebooks_{context}"},{"parentId":null,"name":"Adding notebook pod tolerations","level":1,"index":5,"id":"adding-notebook-pod-tolerations_{context}"},{"parentId":null,"name":"Configuring a custom notebook image","level":1,"index":6,"id":"configuring-a-custom-notebook-image_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-model-performance/"},"sections":[{"parentId":null,"name":"Viewing performance metrics for all models on a model server","level":1,"index":0,"id":"viewing-performance-metrics-for-model-server_monitoring-model-performance"},{"parentId":null,"name":"Viewing HTTP request metrics for a deployed model","level":1,"index":1,"id":"viewing-http-request-metrics-for-a-deployed-model_monitoring-model-performance"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/serving-small-and-medium-sized-models/"},"sections":[{"parentId":null,"name":"Configuring model servers","level":1,"index":0,"id":"_configuring_model_servers"},{"parentId":"_configuring_model_servers","name":"Enabling the multi-model serving platform","level":2,"index":0,"id":"enabling-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a custom model-serving runtime for the multi-model serving platform","level":2,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Adding a model server for the multi-model serving platform","level":2,"index":2,"id":"adding-a-model-server-for-the-multi-model-serving-platform_model-serving"},{"parentId":"_configuring_model_servers","name":"Deleting a model server","level":2,"index":3,"id":"deleting-a-model-server_model-serving"},{"parentId":null,"name":"Working with deployed models","level":1,"index":1,"id":"_working_with_deployed_models"},{"parentId":"_working_with_deployed_models","name":"Deploying a model by using the multi-model serving platform","level":2,"index":0,"id":"deploying-a-model-using-the-multi-model-serving-platform_model-serving"},{"parentId":"_working_with_deployed_models","name":"Viewing a deployed model","level":2,"index":1,"id":"viewing-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Updating the deployment properties of a deployed model","level":2,"index":2,"id":"updating-the-deployment-properties-of-a-deployed-model_model-serving"},{"parentId":"_working_with_deployed_models","name":"Deleting a deployed model","level":2,"index":3,"id":"deleting-a-deployed-model_model-serving"},{"parentId":null,"name":"Configuring monitoring for the multi-model serving platform","level":1,"index":2,"id":"configuring-monitoring-for-the-multi-model-serving-platform_model-serving"},{"parentId":null,"name":"Viewing model-serving runtime metrics for the multi-model serving platform","level":1,"index":3,"id":"viewing-metrics-for-the-multi-model-serving-platform_model-serving"},{"parentId":null,"name":"Monitoring model performance","level":1,"index":4,"id":"_monitoring_model_performance"},{"parentId":"_monitoring_model_performance","name":"Viewing performance metrics for all models on a model server","level":2,"index":0,"id":"viewing-performance-metrics-for-model-server_model-serving"},{"parentId":"_monitoring_model_performance","name":"Viewing HTTP request metrics for a deployed model","level":2,"index":1,"id":"viewing-http-request-metrics-for-a-deployed-model_model-serving"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/serving-large-models/"},"sections":[{"parentId":null,"name":"About the single-model serving platform","level":1,"index":0,"id":"about-the-single-model-serving-platform_serving-large-models"},{"parentId":null,"name":"Installing KServe","level":1,"index":1,"id":"installing-kserve_serving-large-models"},{"parentId":null,"name":"Deploying models by using the single-model serving platform","level":1,"index":2,"id":"deploying-models-using-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Enabling the single-model serving platform","level":2,"index":0,"id":"enabling-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Adding a custom model-serving runtime for the single-model serving platform","level":2,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Deploying models on the single-model serving platform","level":2,"index":2,"id":"deploying-models-on-the-single-model-serving-platform_serving-large-models"},{"parentId":"deploying-models-using-the-single-model-serving-platform_serving-large-models","name":"Accessing the inference endpoint for a deployed model","level":2,"index":3,"id":"accessing-inference-endpoint-for-deployed-model_serving-large-models"},{"parentId":null,"name":"Configuring monitoring for the single-model serving platform","level":1,"index":3,"id":"configuring-monitoring-for-the-single-model-serving-platform_serving-large-models"},{"parentId":null,"name":"Viewing model-serving runtime metrics for the single-model serving platform","level":1,"index":4,"id":"viewing-metrics-for-the-single-model-serving-platform_serving-large-models"},{"parentId":null,"name":"Performance tuning on the single-model serving platform","level":1,"index":5,"id":"_performance_tuning_on_the_single_model_serving_platform"},{"parentId":"_performance_tuning_on_the_single_model_serving_platform","name":"Resolving CUDA out-of-memory errors","level":2,"index":0,"id":"resolving-cuda-oom-errors-for-the-single-model-serving-platform_serving-large-models"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/upgrading-odh-v1-to-v2/"},"sections":[{"parentId":null,"name":"Upgrading the Open Data Hub Operator version 1","level":1,"index":0,"id":"upgrading-the-odh-operator-v1_upgradev1"},{"parentId":null,"name":"Installing Open Data Hub components","level":1,"index":1,"id":"installing-odh-components_upgradev1"},{"parentId":null,"name":"Accessing the Open Data Hub dashboard","level":1,"index":2,"id":"accessing-the-odh-dashboard_upgradev1"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/support-requirements-and-limitations/"},"sections":[{"parentId":null,"name":"Supported browsers","level":1,"index":0,"id":"supported-browsers_requirements"},{"parentId":null,"name":"Supported services","level":1,"index":1,"id":"supported-services_requirements"},{"parentId":null,"name":"Supported packages","level":1,"index":2,"id":"supported-packages_requirements"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/upgrading-odh-v2/"},"sections":[{"parentId":null,"name":"Cleaning up resources","level":1,"index":0,"id":"cleaning-up-resources_upgradev2"},{"parentId":null,"name":"Installing Open Data Hub version 2","level":1,"index":1,"id":"installing-odh-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing the Open Data Hub Operator version 2","level":2,"index":0,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing Open Data Hub components","level":2,"index":1,"id":"installing-odh-components_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Accessing the Open Data Hub dashboard","level":2,"index":2,"id":"accessing-the-odh-dashboard_installv2"},{"parentId":null,"name":"Adding a CA bundle after upgrading","level":1,"index":2,"id":"adding-a-ca-bundle-after-upgrading_upgradev2"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-on-data-science-projects/"},"sections":[{"parentId":null,"name":"Using data science projects","level":1,"index":0,"id":"_using_data_science_projects"},{"parentId":"_using_data_science_projects","name":"Creating a data science project","level":2,"index":0,"id":"creating-a-data-science-project_nb-server"},{"parentId":"_using_data_science_projects","name":"Updating a data science project","level":2,"index":1,"id":"updating-a-data-science-project_nb-server"},{"parentId":"_using_data_science_projects","name":"Deleting a data science project","level":2,"index":2,"id":"deleting-a-data-science-project_nb-server"},{"parentId":null,"name":"Using project workbenches","level":1,"index":1,"id":"_using_project_workbenches"},{"parentId":"_using_project_workbenches","name":"Creating a project workbench","level":2,"index":0,"id":"creating-a-project-workbench_nb-server"},{"parentId":"_using_project_workbenches","name":"Starting a workbench","level":2,"index":1,"id":"starting-a-workbench_nb-server"},{"parentId":"_using_project_workbenches","name":"Updating a project workbench","level":2,"index":2,"id":"updating-a-project-workbench_nb-server"},{"parentId":"_using_project_workbenches","name":"Deleting a workbench from a data science project","level":2,"index":3,"id":"deleting-a-workbench-from-a-data-science-project_nb-server"},{"parentId":null,"name":"Using data connections","level":1,"index":2,"id":"_using_data_connections"},{"parentId":"_using_data_connections","name":"Adding a data connection to your data science project","level":2,"index":0,"id":"adding-a-data-connection-to-your-data-science-project_nb-server"},{"parentId":"_using_data_connections","name":"Deleting a data connection","level":2,"index":1,"id":"deleting-a-data-connection_nb-server"},{"parentId":"_using_data_connections","name":"Updating a connected data source","level":2,"index":2,"id":"updating-a-connected-data-source_nb-server"},{"parentId":null,"name":"Configuring cluster storage","level":1,"index":3,"id":"_configuring_cluster_storage"},{"parentId":"_configuring_cluster_storage","name":"Adding cluster storage to your data science project","level":2,"index":0,"id":"adding-cluster-storage-to-your-data-science-project_nb-server"},{"parentId":"_configuring_cluster_storage","name":"Updating cluster storage","level":2,"index":1,"id":"updating-cluster-storage_nb-server"},{"parentId":"_configuring_cluster_storage","name":"Deleting cluster storage from a data science project","level":2,"index":2,"id":"deleting-cluster-storage-from-a-data-science-project_nb-server"},{"parentId":null,"name":"Configuring data science pipelines","level":1,"index":4,"id":"_configuring_data_science_pipelines"},{"parentId":"_configuring_data_science_pipelines","name":"Configuring a pipeline server","level":2,"index":0,"id":"configuring-a-pipeline-server_nb-server"},{"parentId":"_configuring_data_science_pipelines","name":"Defining a pipeline","level":2,"index":1,"id":"defining-a-pipeline_nb-server"},{"parentId":"_configuring_data_science_pipelines","name":"Importing a data science pipeline","level":2,"index":2,"id":"importing-a-data-science-pipeline_nb-server"},{"parentId":null,"name":"Configuring access to data science projects","level":1,"index":5,"id":"_configuring_access_to_data_science_projects"},{"parentId":"_configuring_access_to_data_science_projects","name":"Configuring access to data science projects","level":2,"index":0,"id":"configuring-access-to-data-science-projects_nb-server"},{"parentId":"_configuring_access_to_data_science_projects","name":"Sharing access to a data science project","level":2,"index":1,"id":"sharing-access-to-a-data-science-project_nb-server"},{"parentId":"_configuring_access_to_data_science_projects","name":"Updating access to a data science project","level":2,"index":2,"id":"updating-access-to-a-data-science-project_nb-server"},{"parentId":"_configuring_access_to_data_science_projects","name":"Removing access to a data science project","level":2,"index":3,"id":"removing-access-to-a-data-science-project_nb-server"},{"parentId":null,"name":"Viewing Python packages installed on your notebook server","level":1,"index":6,"id":"viewing-python-packages-installed-on-your-notebook-server_nb-server"},{"parentId":null,"name":"Installing Python packages on your notebook server","level":1,"index":7,"id":"installing-python-packages-on-your-notebook-server_nb-server"},{"parentId":null,"name":"Updating notebook server settings by restarting your server","level":1,"index":8,"id":"updating-notebook-server-settings-by-restarting-your-server_nb-server"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-accelerators/"},"sections":[{"parentId":null,"name":"Overview of accelerators","level":1,"index":0,"id":"overview-of-accelerators_accelerators"},{"parentId":null,"name":"Working with accelerator profiles","level":1,"index":1,"id":"working-with-accelerator-profiles_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Creating an accelerator profile","level":2,"index":0,"id":"creating-an-accelerator-profile_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Updating an accelerator profile","level":2,"index":1,"id":"updating-an-accelerator-profile_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Deleting an accelerator profile","level":2,"index":2,"id":"deleting-an-accelerator-profile_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Viewing accelerator profiles","level":2,"index":3,"id":"viewing-accelerator-profiles_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Configuring a recommended accelerator for notebook images","level":2,"index":4,"id":"configuring-a-recommended-accelerator-for-notebook-images_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Configuring a recommended accelerator for serving runtimes","level":2,"index":5,"id":"configuring-a-recommended-accelerator-for-serving-runtimes_accelerators"},{"parentId":null,"name":"Habana Gaudi integration","level":1,"index":2,"id":"habana-gaudi-integration_accelerators"},{"parentId":"habana-gaudi-integration_accelerators","name":"Enabling Habana Gaudi devices","level":2,"index":0,"id":"enabling-habana-gaudi-devices_accelerators"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-certificates/"},"sections":[{"parentId":null,"name":"Understanding certificates in {productname-short}","level":1,"index":0,"id":"understanding-certificates_certs"},{"parentId":"understanding-certificates_certs","name":"How CA bundles are injected","level":2,"index":0,"id":"_how_ca_bundles_are_injected"},{"parentId":"understanding-certificates_certs","name":"How the ConfigMap is managed","level":2,"index":1,"id":"_how_the_configmap_is_managed"},{"parentId":null,"name":"Adding a CA bundle","level":1,"index":1,"id":"adding-a-ca-bundle_certs"},{"parentId":null,"name":"Removing a CA bundle","level":1,"index":2,"id":"removing-a-ca-bundle_certs"},{"parentId":null,"name":"Removing a CA bundle from a namespace","level":1,"index":3,"id":"removing-a-ca-bundle-from-a-namespace_certs"},{"parentId":null,"name":"Managing certificates","level":1,"index":4,"id":"managing-certificates_certs"},{"parentId":null,"name":"Using self-signed certificates with {productname-short} components","level":1,"index":5,"id":"_using_self_signed_certificates_with_productname_short_components"},{"parentId":"_using_self_signed_certificates_with_productname_short_components","name":"Using certificates with data science pipelines","level":2,"index":0,"id":"using-certificates-with-data-science-pipelines_certs"},{"parentId":"using-certificates-with-data-science-pipelines_certs","name":"Providing a CA bundle only for data science pipelines","level":3,"index":0,"id":"_providing_a_ca_bundle_only_for_data_science_pipelines"},{"parentId":"_using_self_signed_certificates_with_productname_short_components","name":"Using certificates with workbenches","level":2,"index":1,"id":"using-certificates-with-workbenches_certs"},{"parentId":"using-certificates-with-workbenches_certs","name":"Creating data science pipelines with Elyra and self-signed certificates","level":3,"index":0,"id":"_creating_data_science_pipelines_with_elyra_and_self_signed_certificates"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Enabling Data Science Pipelines 2.0","level":1,"index":0,"id":"enabling-data-science-pipelines-2_ds-pipelines"},{"parentId":"enabling-data-science-pipelines-2_ds-pipelines","name":"Installing {productname-short} with DSP 2.0","level":2,"index":0,"id":"_installing_productname_short_with_dsp_2_0"},{"parentId":"enabling-data-science-pipelines-2_ds-pipelines","name":"Upgrading to DSP 2.0","level":2,"index":1,"id":"_upgrading_to_dsp_2_0"},{"parentId":null,"name":"Managing data science pipelines","level":1,"index":1,"id":"_managing_data_science_pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Configuring a pipeline server","level":2,"index":0,"id":"configuring-a-pipeline-server_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Defining a pipeline","level":2,"index":1,"id":"defining-a-pipeline_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Importing a data science pipeline","level":2,"index":2,"id":"importing-a-data-science-pipeline_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Downloading a data science pipeline version","level":2,"index":3,"id":"downloading-a-data-science-pipeline-version_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Deleting a data science pipeline","level":2,"index":4,"id":"deleting-a-data-science-pipeline_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Deleting a pipeline server","level":2,"index":5,"id":"deleting-a-pipeline-server_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Viewing the details of a pipeline server","level":2,"index":6,"id":"viewing-the-details-of-a-pipeline-server_ds-pipelines"},{"parentId":"_managing_data_science_pipelines","name":"Viewing existing pipelines","level":2,"index":7,"id":"viewing-existing-pipelines_ds-pipelines"},{"parentId":null,"name":"Managing pipeline runs","level":1,"index":2,"id":"_managing_pipeline_runs"},{"parentId":"_managing_pipeline_runs","name":"Overview of pipeline runs","level":2,"index":0,"id":"overview-of-pipeline-runs_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Viewing active pipeline runs","level":2,"index":1,"id":"viewing-active-pipeline-runs_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Executing a pipeline run","level":2,"index":2,"id":"executing-a-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Stopping an active pipeline run","level":2,"index":3,"id":"stopping-an-active-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Duplicating an active pipeline run","level":2,"index":4,"id":"duplicating-an-active-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Viewing scheduled pipeline runs","level":2,"index":5,"id":"viewing-scheduled-pipeline-runs_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Scheduling a pipeline run using a cron job","level":2,"index":6,"id":"scheduling-a-pipeline-run-using-a-cron-job_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Scheduling a pipeline run","level":2,"index":7,"id":"scheduling-a-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Duplicating a scheduled pipeline run","level":2,"index":8,"id":"duplicating-a-scheduled-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Deleting a scheduled pipeline run","level":2,"index":9,"id":"deleting-a-scheduled-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Viewing the details of a pipeline run","level":2,"index":10,"id":"viewing-the-details-of-a-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Viewing archived pipeline runs","level":2,"index":11,"id":"viewing-archived-pipeline-runs_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Archiving a pipeline run","level":2,"index":12,"id":"archiving-a-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Restoring an archived pipeline run","level":2,"index":13,"id":"restoring-an-archived-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Deleting an archived pipeline run","level":2,"index":14,"id":"deleting-an-archived-pipeline-run_ds-pipelines"},{"parentId":"_managing_pipeline_runs","name":"Duplicating an archived pipeline run","level":2,"index":15,"id":"duplicating-an-archived-pipeline-run_ds-pipelines"},{"parentId":null,"name":"Working with pipeline logs","level":1,"index":3,"id":"_working_with_pipeline_logs"},{"parentId":"_working_with_pipeline_logs","name":"About pipeline logs","level":2,"index":0,"id":"about-pipeline-logs_ds-pipelines"},{"parentId":"_working_with_pipeline_logs","name":"Viewing pipeline step logs","level":2,"index":1,"id":"viewing-pipeline-step-logs_ds-pipelines"},{"parentId":"_working_with_pipeline_logs","name":"Downloading pipeline step logs","level":2,"index":2,"id":"downloading-pipeline-step-logs_ds-pipelines"},{"parentId":null,"name":"Working with pipelines in JupyterLab","level":1,"index":4,"id":"_working_with_pipelines_in_jupyterlab"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Overview of pipelines in JupyterLab","level":2,"index":0,"id":"overview-of-pipelines-in-jupyterlab_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Accessing the pipeline editor","level":2,"index":1,"id":"accessing-the-pipeline-editor_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Creating a runtime configuration","level":2,"index":2,"id":"creating-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Updating a runtime configuration","level":2,"index":3,"id":"updating-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Deleting a runtime configuration","level":2,"index":4,"id":"deleting-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Duplicating a runtime configuration","level":2,"index":5,"id":"duplicating-a-runtime-configuration_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Running a pipeline in JupyterLab","level":2,"index":6,"id":"running-a-pipeline-in-jupyterlab_ds-pipelines"},{"parentId":"_working_with_pipelines_in_jupyterlab","name":"Exporting a pipeline in JupyterLab","level":2,"index":7,"id":"exporting-a-pipeline-in-jupyterlab_ds-pipelines"},{"parentId":null,"name":"Additional resources","level":1,"index":5,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-deleting-users-and-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-model-serving/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-pipeline-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-authorization-token-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-jupyter-administration-interface/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-inference-endpoint-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-odh-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-pipeline-editor/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-tutorials/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-ca-bundle-after-upgrading/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-data-connection-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-model-server-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-an-application-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-an-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-cluster-storage-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-notebook-pod-tolerations/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-users-to-specialized-data-science-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/allocating-additional-resources-to-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/authenticating-trustyai-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/backing-up-storage-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/cleaning-up-after-deleting-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/cleaning-up-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/common-questions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-custom-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-recommended-accelerator-for-notebook-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-recommended-accelerator-for-serving-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-access-to-data-science-projects/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-monitoring-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-monitoring-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-distributed-workloads-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-your-ide/"},"sections":[{"parentId":null,"name":"Configuring your code-server workbench","level":1,"index":0,"id":"_configuring_your_code_server_workbench"},{"parentId":"_configuring_your_code_server_workbench","name":"Installing extensions with code-server","level":2,"index":0,"id":"_installing_extensions_with_code_server"},{"parentId":"_installing_extensions_with_code_server","name":"Extensions","level":3,"index":0,"id":"_extensions"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric/"},"sections":[{"parentId":null,"name":"Creating a bias metric by using the dashboard","level":1,"index":0,"id":"creating-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Creating a bias metric by using the CLI","level":1,"index":1,"id":"creating-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-new-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-new-project-for-your-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/defining-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/defining-data-science-admin-and-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric/"},"sections":[{"parentId":null,"name":"Deleting a bias metric by using the dashboard","level":1,"index":0,"id":"deleting-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Deleting a bias metric by using the CLI","level":1,"index":1,"id":"deleting-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-data-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-workbench-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-cluster-storage-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-model-using-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-using-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-on-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/disabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-a-data-science-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-bias-metric/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-the-dashboard-configuration-file/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-gpu-support-in-data-science/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-data-science-pipelines-2/"},"sections":[{"parentId":null,"name":"Installing {productname-short} with DSP 2.0","level":1,"index":0,"id":"_installing_productname_short_with_dsp_2_0"},{"parentId":null,"name":"Upgrading to DSP 2.0","level":1,"index":1,"id":"_upgrading_to_dsp_2_0"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-services-connected-to-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-habana-gaudi-devices/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-trustyai-service-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-trustyai-service-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-trustyai-service/"},"sections":[{"parentId":null,"name":"Enabling the TrustyAI Service by using the dashboard","level":1,"index":0,"id":"enabling-trustyai-service-using-dashboard_{context}"},{"parentId":null,"name":"Enabling the TrustyAI Service by using the CLI","level":1,"index":1,"id":"enabling-trustyai-service-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/executing-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/habana-gaudi-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/exporting-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/hiding-information-about-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/hiding-the-default-jupyter-application/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/importing-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-python-packages-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-odh-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-the-odh-operator-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/logging-in-to-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/launching-jupyter-and-starting-a-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/managing-certificates/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/making-inference-requests-to-models-deployed-on-single-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/monitoring-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/notifications-in-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/notebook-images-for-data-scientists/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/options-for-notebook-server-environments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipelines-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/preventing-users-from-adding-applications-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-user-types-and-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/pushing-project-changes-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-a-ca-bundle-from-a-namespace/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-dashboard-configuration-options/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-a-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-disabled-applications-from-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requirements-for-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/resolving-cuda-oom-errors/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/revoking-user-access-to-jupyter/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-disconnected-env/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-from-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-from-ds-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/scheduling-a-pipeline-run-using-a-cron-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/scheduling-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/sending-training-data-to-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/sharing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/showing-hiding-information-about-enabled-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-a-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-idle-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/supported-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/supported-browsers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/supported-packages/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/supported-services/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/the-open-data-hub-user-interface/"},"sections":[{"parentId":null,"name":"Global navigation","level":1,"index":0,"id":"_global_navigation"},{"parentId":null,"name":"Side navigation","level":1,"index":1,"id":"_side_navigation"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-in-jupyter-for-users/"},"sections":[{"parentId":null,"name":"I see a <strong>403: Forbidden</strong> error when I log in to Jupyter","level":1,"index":0,"id":"_i_see_a_403_forbidden_error_when_i_log_in_to_jupyter"},{"parentId":null,"name":"My notebook server does not start","level":1,"index":1,"id":"_my_notebook_server_does_not_start"},{"parentId":null,"name":"I see a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when I run my notebook cells","level":1,"index":2,"id":"_i_see_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_i_run_my_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-in-jupyter-for-administrators/"},"sections":[{"parentId":null,"name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":1,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":null,"name":"A user&#8217;s notebook server does not start","level":1,"index":1,"id":"_a_users_notebook_server_does_not_start"},{"parentId":null,"name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":1,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/understanding-certificates/"},"sections":[{"parentId":null,"name":"How CA bundles are injected","level":1,"index":0,"id":"_how_ca_bundles_are_injected"},{"parentId":null,"name":"How the ConfigMap is managed","level":1,"index":1,"id":"_how_the_configmap_is_managed"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/tutorials-for-data-scientists/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-connected-data-source/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-cluster-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-notebook-server-settings-by-restarting-your-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-the-deployment-properties-of-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-your-project-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/upgrading-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-active-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-archived-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-existing-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-http-request-metrics-for-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-metrics-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-metrics-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-performance-metrics-for-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-project-metrics-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-python-packages-installed-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-scheduled-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-status-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/working-with-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-certificates-with-workbenches/"},"sections":[{"parentId":null,"name":"Creating data science pipelines with Elyra and self-signed certificates","level":1,"index":0,"id":"_creating_data_science_pipelines_with_elyra_and_self_signed_certificates"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-deleting-users-and-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-model-serving/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-pipeline-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-authorization-token-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-odh-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-inference-endpoint-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-tutorials/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-jupyter-administration-interface/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-pipeline-editor/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-ca-bundle-after-upgrading/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-data-connection-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-model-server-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-an-application-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-an-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-cluster-storage-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/allocating-additional-resources-to-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-codeflare-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-quota-management-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/authenticating-trustyai-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/backing-up-storage-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/cleaning-up-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/common-questions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/cleaning-up-after-deleting-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-recommended-accelerator-for-notebook-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-recommended-accelerator-for-serving-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-access-to-data-science-projects/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-monitoring-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-monitoring-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-quota-management-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-codeflare-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-distributed-workloads-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-your-ide/"},"sections":[{"parentId":null,"name":"Configuring your code-server workbench","level":1,"index":0,"id":"_configuring_your_code_server_workbench"},{"parentId":"_configuring_your_code_server_workbench","name":"Installing extensions with code-server","level":2,"index":0,"id":"_installing_extensions_with_code_server"},{"parentId":"_installing_extensions_with_code_server","name":"Extensions","level":3,"index":0,"id":"_extensions"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric/"},"sections":[{"parentId":null,"name":"Creating a bias metric by using the dashboard","level":1,"index":0,"id":"creating-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Creating a bias metric by using the CLI","level":1,"index":1,"id":"creating-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-new-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-new-project-for-your-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/defining-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/defining-data-science-admin-and-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-data-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric/"},"sections":[{"parentId":null,"name":"Deleting a bias metric by using the dashboard","level":1,"index":0,"id":"deleting-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Deleting a bias metric by using the CLI","level":1,"index":1,"id":"deleting-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-workbench-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-cluster-storage-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-model-using-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-using-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-on-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/disabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-a-data-science-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-bias-metric/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-data-science-pipelines-2/"},"sections":[{"parentId":null,"name":"Installing {productname-short} with DSP 2.0","level":1,"index":0,"id":"_installing_productname_short_with_dsp_2_0"},{"parentId":null,"name":"Upgrading to DSP 2.0","level":1,"index":1,"id":"_upgrading_to_dsp_2_0"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-the-dashboard-configuration-file/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-gpu-support-in-data-science/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-habana-gaudi-devices/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-services-connected-to-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-trustyai-service-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-trustyai-service/"},"sections":[{"parentId":null,"name":"Enabling the TrustyAI Service by using the dashboard","level":1,"index":0,"id":"enabling-trustyai-service-using-dashboard_{context}"},{"parentId":null,"name":"Enabling the TrustyAI Service by using the CLI","level":1,"index":1,"id":"enabling-trustyai-service-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-trustyai-service-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/executing-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/exporting-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/habana-gaudi-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/hiding-information-about-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/hiding-the-default-jupyter-application/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/importing-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-odh-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-python-packages-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-the-odh-operator-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/launching-jupyter-and-starting-a-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/logging-in-to-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/making-inference-requests-to-models-deployed-on-single-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/managing-certificates/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/monitoring-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/notebook-images-for-data-scientists/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/notifications-in-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/options-for-notebook-server-environments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipelines-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/preventing-users-from-adding-applications-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-user-types-and-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/pushing-project-changes-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-dashboard-configuration-options/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-a-ca-bundle-from-a-namespace/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-a-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-disabled-applications-from-open-data-hub/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/resolving-cuda-oom-errors/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requirements-for-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/revoking-user-access-to-jupyter/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-disconnected-env/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-from-ds-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-from-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/scheduling-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/scheduling-a-pipeline-run-using-a-cron-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/sending-training-data-to-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/sharing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-a-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/showing-hiding-information-about-enabled-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-idle-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-notebook-servers-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/supported-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/supported-browsers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/supported-packages/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/supported-services/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-in-jupyter-for-administrators/"},"sections":[{"parentId":null,"name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":1,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":null,"name":"A user&#8217;s notebook server does not start","level":1,"index":1,"id":"_a_users_notebook_server_does_not_start"},{"parentId":null,"name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":1,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/the-open-data-hub-user-interface/"},"sections":[{"parentId":null,"name":"Global navigation","level":1,"index":0,"id":"_global_navigation"},{"parentId":null,"name":"Side navigation","level":1,"index":1,"id":"_side_navigation"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/tutorials-for-data-scientists/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-in-jupyter-for-users/"},"sections":[{"parentId":null,"name":"I see a <strong>403: Forbidden</strong> error when I log in to Jupyter","level":1,"index":0,"id":"_i_see_a_403_forbidden_error_when_i_log_in_to_jupyter"},{"parentId":null,"name":"My notebook server does not start","level":1,"index":1,"id":"_my_notebook_server_does_not_start"},{"parentId":null,"name":"I see a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when I run my notebook cells","level":1,"index":2,"id":"_i_see_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_i_run_my_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/understanding-certificates/"},"sections":[{"parentId":null,"name":"How CA bundles are injected","level":1,"index":0,"id":"_how_ca_bundles_are_injected"},{"parentId":null,"name":"How the ConfigMap is managed","level":1,"index":1,"id":"_how_the_configmap_is_managed"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-connected-data-source/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-cluster-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-notebook-server-settings-by-restarting-your-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-the-deployment-properties-of-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-your-project-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/upgrading-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-the-command-line-interface/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-certificates-with-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Providing a CA bundle only for data science pipelines","level":1,"index":0,"id":"_providing_a_ca_bundle_only_for_data_science_pipelines"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-certificates-with-workbenches/"},"sections":[{"parentId":null,"name":"Creating data science pipelines with Elyra and self-signed certificates","level":1,"index":0,"id":"_creating_data_science_pipelines_with_elyra_and_self_signed_certificates"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-active-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-archived-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-existing-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-http-request-metrics-for-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-metrics-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-metrics-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-performance-metrics-for-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-project-metrics-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-python-packages-installed-on-your-notebook-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-scheduled-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-status-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/working-with-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-users-to-specialized-data-science-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-custom-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-notebook-pod-tolerations/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-certificates-with-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Providing a CA bundle only for data science pipelines","level":1,"index":0,"id":"_providing_a_ca_bundle_only_for_data_science_pipelines"}]}}}]},"asciidoc":{"html":"<div id=\"preamble\">\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>As a data scientist, you can enhance your data science projects on {productname-short} by building portable machine learning (ML) workflows with data science pipelines, using Docker containers. This enables you to standardize and automate machine learning workflows to enable you to develop and deploy your data science models.</p>\n</div>\n<div class=\"paragraph\">\n<p>For example, the steps in a machine learning workflow might include items such as data extraction, data processing, feature extraction, model training, model validation, and model serving. Automating these activities enables your organization to develop a continuous process of retraining and updating a model based on newly received data. This can help resolve challenges related to building an integrated machine learning deployment and continuously operating it in production.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can also use the Elyra JupyterLab extension to create and run data science pipelines within JupyterLab. For more information, see <a href=\"{odhdocshome}/working-on-data-science-projects/#_working_with_pipelines_in_jupyterlab\">Working with pipelines in JupyterLab</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>From {productname-long} version 2.10.0, data science pipelines are based on <a href=\"https://www.kubeflow.org/docs/components/pipelines/v2/\">KubeFlow Pipelines (KFP) version 2.0</a>. For more information, see <a href=\"{odhdocshome}/working-on-data-science-projects/#enabling-data-science-pipelines-2_ds-pipelines\">Enabling Data Science Pipelines 2.0</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>A data science pipeline in {productname-short} consists of the following components:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Pipeline server: A server that is attached to your data science project and hosts your data science pipeline.</p>\n</li>\n<li>\n<p>Pipeline: A pipeline defines the configuration of your machine learning workflow and the relationship between each component in the workflow.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Pipeline code: A definition of your pipeline in a YAML file.</p>\n</li>\n<li>\n<p>Pipeline graph: A graphical illustration of the steps executed in a pipeline run and the relationship between them.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Pipeline run: An execution of your pipeline.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Active run: A pipeline run that is in its execution phase, or is stopped.</p>\n</li>\n<li>\n<p>Scheduled run: A pipeline run scheduled to execute at least once.</p>\n</li>\n<li>\n<p>Archived run: A pipeline run that resides in the run archive and is no longer required.</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>This feature is based on Kubeflow Pipelines 2.0. Use the latest Kubeflow Pipelines 2.0 SDK to build your data science pipeline in Python code. After you have built your pipeline, use the SDK to compile it into an Intermediate Representation (IR) YAML file. The {productname-short} user interface enables you to track and manage pipelines and pipeline runs.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can store your pipeline artifacts in an S3-compatible object storage bucket so that you do not consume local storage. To do this, you must first configure write access to your S3 bucket on your storage account.</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"enabling-data-science-pipelines-2_ds-pipelines\">Enabling Data Science Pipelines 2.0</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>From {productname-long} version 2.10.0, data science pipelines are based on <a href=\"https://www.kubeflow.org/docs/components/pipelines/v2/\">KubeFlow Pipelines (KFP) version 2.0</a>. DSP 2.0 is enabled and deployed by default in {productname-short}.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>The <code>PipelineConf</code> class is deprecated, and there is no KFP 2.0 equivalent.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Data Science Pipelines (DSP) 2.0 contains an installation of Argo Workflows. {productname-short} does not support direct customer usage of this installation of Argo Workflows.</p>\n</div>\n<div class=\"paragraph\">\n<p>To install or upgrade to {productname-short} 2.10.0 with DSP, ensure that your cluster does not have an existing installation of Argo Workflows that is not installed by {productname-short}.</p>\n</div>\n<div class=\"paragraph\">\n<p>Argo Workflows resources that are created by {productname-short} have the following labels in the OpenShift Console under <strong>Administration &gt; CustomResourceDefinitions</strong>, in the <code>argoproj.io</code> group:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code> labels:\n    app.kubernetes.io/part-of: data-science-pipelines-operator\n    app.opendatahub.io/data-science-pipelines-operator: 'true'</code></pre>\n</div>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_installing_productname_short_with_dsp_2_0\">Installing {productname-short} with DSP 2.0</h3>\n<div class=\"paragraph\">\n<p>To install {productname-short} 2.10.0, ensure that there is no installation of Argo Workflows that is not installed by DSP on your cluster, and follow the installation steps described in <a href=\"{odhdocshome}/installing-open-data-hub/\">Installing {productname-short}</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>If there is an existing installation of Argo Workflows that is not installed by DSP on your cluster, DSP will be disabled after you install {productname-short} 2.10.0 with DSP.</p>\n</div>\n<div class=\"paragraph\">\n<p>To enable data science pipelines, remove the separate installation of Argo Workflows from your cluster. Data Science Pipelines will be enabled automatically.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_upgrading_to_dsp_2_0\">Upgrading to DSP 2.0</h3>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>After you upgrade to {productname-short} 2.10.0, pipelines created with DSP 1.0 will continue to run, but will be inaccessible from the {productname-short} dashboard. We recommend that current DSP users do not upgrade to {productname-short} 2.10.0 until you are ready to migrate to the new pipelines solution.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>To upgrade to {productname-short} 2.10.0 with DSP 2.0, ensure that there is no installation of Argo Workflows that is not installed by DSP on your cluster, and follow the upgrade steps described in <a href=\"{odhdocshome}/upgrading-open-data-hub/\">Upgrading {productname-short}</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>If you upgrade to {productname-short} 2.10.0 with DSP enabled and an Argo Workflows installation that is not installed by DSP exists on your cluster, {productname-short} components will not be upgraded. To complete the component upgrade, disable DSP or remove the separate installation of Argo Workflows. The component upgrade will complete automatically.</p>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"_managing_data_science_pipelines\">Managing data science pipelines</h2>\n<div class=\"sectionbody\">\n<div class=\"sect2\">\n<h3 id=\"configuring-a-pipeline-server_ds-pipelines\">Configuring a pipeline server</h3>\n<div class=\"paragraph _abstract\">\n<p>Before you can successfully create a pipeline in {productname-short}, you must configure a pipeline server. This task includes configuring where your pipeline artifacts and data are stored.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>You are not required to specify any storage directories when configuring a data connection for your pipeline server. When you import a pipeline, the <code>/pipelines</code> folder is created in the <code>root</code> folder of the bucket, containing a YAML file for the pipeline. If you upload a new version of the same pipeline, a new YAML file with a different ID is added to the <code>/pipelines</code> folder.</p>\n</div>\n<div class=\"paragraph\">\n<p>When you run a pipeline, the artifacts are stored in the <code>/pipeline-name</code> folder in the <code>root</code> folder of the bucket.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you use an external MySQL database and upgrade to {productname-short} 2.10.0, the database is migrated to DSP 2.0 format, making it incompatible with earlier versions of {productname-short}.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that you can add a pipeline server to.</p>\n</li>\n<li>\n<p>You have an existing S3-compatible object storage bucket and you have configured write access to your S3 bucket on your storage account.</p>\n</li>\n<li>\n<p>If you are configuring a pipeline server with an external MySQL database, your database must use MySQL version 5.x.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Projects</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Projects</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the name of the project that you want to configure a pipeline server for.</p>\n<div class=\"paragraph\">\n<p>A project details page opens.</p>\n</div>\n</li>\n<li>\n<p>Click the <strong>Pipelines</strong> tab.</p>\n</li>\n<li>\n<p>Click <strong>Configure pipeline server</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Configure pipeline server</strong> dialog appears.</p>\n</div>\n</li>\n<li>\n<p>In the <strong>Object storage connection</strong> section, provide values for the mandatory fields:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Access key</strong> field, enter the access key ID for the S3-compatible object storage provider.</p>\n</li>\n<li>\n<p>In the <strong>Secret key</strong> field, enter the secret access key for the S3-compatible object storage account that you specified.</p>\n</li>\n<li>\n<p>In the <strong>Endpoint</strong> field, enter the endpoint of your S3-compatible object storage bucket.</p>\n</li>\n<li>\n<p>In the <strong>Region</strong> field, enter the default region of your S3-compatible object storage account.</p>\n</li>\n<li>\n<p>In the <strong>Bucket</strong> field, enter the name of your S3-compatible object storage bucket.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you specify incorrect data connection settings, you cannot update these settings on the same pipeline server. Therefore, you must delete the pipeline server and configure another one.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>In the <strong>Database</strong> section, click <strong>Show advanced database options</strong> to specify the database to store your pipeline data and select one of the following sets of actions:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Select <strong>Use default database stored on your cluster</strong> to deploy a MariaDB database in your project.</p>\n</li>\n<li>\n<p>Select <strong>Connect to external MySQL database</strong> to add a new connection to an external database that your pipeline server can access.</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Host</strong> field, enter the database&#8217;s host name.</p>\n</li>\n<li>\n<p>In the <strong>Port</strong> field, enter the database&#8217;s port.</p>\n</li>\n<li>\n<p>In the <strong>Username</strong> field, enter the default user name that is connected to the database.</p>\n</li>\n<li>\n<p>In the <strong>Password</strong> field, enter the password for the default user account.</p>\n</li>\n<li>\n<p>In the <strong>Database</strong> field, enter the database name.</p>\n</li>\n</ol>\n</div>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Click <strong>Configure pipeline server</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>In the <strong>Pipelines</strong> tab for the project:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>The <strong>Import pipeline</strong> button is available.</p>\n</li>\n<li>\n<p>When you click the action menu (<strong>&#8942;</strong>) and then click <strong>View pipeline server configuration</strong>, the pipeline server details are displayed.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"defining-a-pipeline_ds-pipelines\">Defining a pipeline</h3>\n<div class=\"paragraph _abstract\">\n<p>The Kubeflow Pipelines SDK enables you to define end-to-end machine learning and data pipelines. Use the latest Kubeflow Pipelines 2.0 SDK to build your data science pipeline in Python code. After you have built your pipeline, use the SDK to compile it into an Intermediate Representation (IR) YAML file. After defining the pipeline, you can import the YAML file to the {productname-short} dashboard to enable you to configure its execution settings.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can also use the Elyra JupyterLab extension to create and run data science pipelines within JupyterLab. For more information about the Elyra JupyterLab extension, see <a href=\"https://elyra.readthedocs.io/en/stable/getting_started/overview.html\">Elyra Documentation</a>.</p>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://www.kubeflow.org/docs/components/pipelines/v2/\">Kubeflow Pipelines 2.0 Documentation</a></p>\n</li>\n<li>\n<p><a href=\"https://elyra.readthedocs.io/en/stable/getting_started/overview.html\">Elyra Documentation</a></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"importing-a-data-science-pipeline_ds-pipelines\">Importing a data science pipeline</h3>\n<div class=\"paragraph _abstract\">\n<p>To help you begin working with data science pipelines in {productname-short}, you can import a YAML file containing your pipeline&#8217;s code to an active pipeline server, or you can import the YAML file from a URL. This file contains a Kubeflow pipeline compiled by using the Kubeflow compiler. After you have imported the pipeline to a pipeline server, you can execute the pipeline by creating a pipeline run.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have compiled your pipeline with the Kubeflow compiler and you have access to the resulting YAML file.</p>\n</li>\n<li>\n<p>If you are uploading your pipeline from a URL, the URL is publicly accessible.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Pipelines</strong> page, select the <strong>project</strong> that you want to import a pipeline to.</p>\n</li>\n<li>\n<p>Click <strong>Import pipeline</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Import pipeline</strong> dialog, enter the details for the pipeline that you are importing.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Pipeline name</strong> field, enter a name for the pipeline that you are importing.</p>\n</li>\n<li>\n<p>In the <strong>Pipeline description</strong> field, enter a description for the pipeline that you are importing.</p>\n</li>\n<li>\n<p>Select where you want to import your pipeline from by performing one of the following actions:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Select <strong>Upload a file</strong> to upload your pipeline from your local machine&#8217;s file system. Import your pipeline by clicking <strong>upload</strong> or by dragging and dropping a file.</p>\n</li>\n<li>\n<p>Select <strong>Import by url</strong> to upload your pipeline from a URL and then enter the URL into the text box.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Click <strong>Import pipeline</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline that you imported appears on the <strong>Pipelines</strong> page and on the <strong>Pipelines</strong> tab on the project details page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"downloading-a-data-science-pipeline-version_ds-pipelines\">Downloading a data science pipeline version</h3>\n<div class=\"paragraph _abstract\">\n<p>To make further changes to a data science pipeline version that you previously uploaded to {productname-short}, you can download pipeline version code from the user interface.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have created and imported a pipeline to an active pipeline server that is available to download.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Pipelines</strong> page, select the <strong>project</strong> that contains the version that you want to download.</p>\n</li>\n<li>\n<p>For a pipeline that contains the version that you want to download, click <strong>Expand</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhoai-expand-icon.png\" alt=\"rhoai expand icon\"></span>).</p>\n</li>\n<li>\n<p>Click the pipeline version that you want to download.</p>\n</li>\n<li>\n<p>On the <strong>Pipeline details</strong> page, click the <strong>YAML</strong> tab.</p>\n</li>\n<li>\n<p>Click the <strong>Download</strong> button (<span class=\"image\"><img src=\"/static/docs/images/rhoai-download-icon.png\" alt=\"rhoai download icon\"></span>) to download the YAML file containing your pipeline version code to your local machine.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline version code downloads to your browser&#8217;s default directory for downloaded files.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"deleting-a-data-science-pipeline_ds-pipelines\">Deleting a data science pipeline</h3>\n<div class=\"paragraph _abstract\">\n<p>If you no longer require access to your data science pipeline on the dashboard, you can delete it so that it does not appear on the <strong>Data Science Pipelines</strong> page.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n<li>\n<p>There are active pipelines available on the <strong>Pipelines</strong> page.</p>\n</li>\n<li>\n<p>The pipeline that you want to delete does not contain any pipeline versions.</p>\n</li>\n<li>\n<p>The pipeline that you want to delete does not contain any pipeline versions. For more information, see <a href=\"{odhdocshome}/working-on-data-science-projects/#_managing_data_science_pipelines\">Deleting a pipeline version</a>.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Pipelines</strong> page, select the project that contains the pipeline that you want to delete from the <strong>Project</strong> list.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the pipeline that you want to delete and click <strong>Delete pipeline</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Delete pipeline</strong> dialog, enter the pipeline name in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete pipeline</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The data science pipeline that you deleted no longer appears on the <strong>Pipelines</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"deleting-a-pipeline-server_ds-pipelines\">Deleting a pipeline server</h3>\n<div class=\"paragraph _abstract\">\n<p>After you have finished running your data science pipelines, you can delete the pipeline server. Deleting a pipeline server automatically deletes all of its associated pipelines, pipeline versions, and runs. If your pipeline data is stored in a database, the database is also deleted along with its meta-data. In addition, after deleting a pipeline server, you cannot create new pipelines or pipeline runs until you create another pipeline server.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Pipelines</strong> page, select the <strong>project</strong> for the pipeline server that you want to delete.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline server actions</strong> list, select <strong>Delete pipeline server</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Delete pipeline server</strong> dialog, enter the pipeline server&#8217;s name in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Pipelines previously assigned to the deleted pipeline server no longer appears on the <strong>Pipelines</strong> page for the relevant data science project.</p>\n</li>\n<li>\n<p>Pipeline runs previously assigned to the deleted pipeline server no longer appears on the <strong>Runs</strong> page for the relevant data science project.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-the-details-of-a-pipeline-server_ds-pipelines\">Viewing the details of a pipeline server</h3>\n<div class=\"paragraph _abstract\">\n<p>You can view the details of pipeline servers configured in {productname-short}, such as the pipeline&#8217;s data connection details and where its data is stored.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>You have previously created a data science project that contains an active and available pipeline server.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Pipelines</strong> page opens, select the <strong>project</strong> whose pipeline server you want to view.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline server actions</strong> list, select <strong>View pipeline server configuration</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>You can view the relevant pipeline server details in the <strong>View pipeline server</strong> dialog.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-existing-pipelines_ds-pipelines\">Viewing existing pipelines</h3>\n<div class=\"paragraph _abstract\">\n<p>You can view the details of pipelines that you have imported to {productname-long}, such as the pipeline&#8217;s last run, when it was created, the pipeline&#8217;s executed runs, and details of any associated pipeline versions.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>Existing pipelines are available.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Pipelines</strong> page, select the relevant <strong>project</strong> for the pipelines you want to view.</p>\n</li>\n<li>\n<p>Study the pipelines on the list.</p>\n</li>\n<li>\n<p>Optional: Click <strong>Expand</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhoai-expand-icon.png\" alt=\"rhoai expand icon\"></span>) on the relevant row to view details of any pipeline versions associated with the pipeline.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>A list of previously created data science pipelines appears on the <strong>Pipelines</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"_managing_pipeline_runs\">Managing pipeline runs</h2>\n<div class=\"sectionbody\">\n<div class=\"sect2\">\n<h3 id=\"overview-of-pipeline-runs_ds-pipelines\">Overview of pipeline runs</h3>\n<div class=\"paragraph _abstract\">\n<p>A pipeline run is a single execution of a data science pipeline. As data scientist, you can use {productname-short} to define, manage, and track executions of a data science pipeline. You can view a record of previously executed, scheduled, and archived runs from the <strong>Runs</strong> page in the {productname-short} user interface.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can optimize your use of pipeline runs for portability. You can clone your pipeline runs to reproduce and scale them accordingly, or archive them when you want to retain a record of their execution, but no longer require them. You can delete archived runs that you no longer want to retain, or you can restore them to their former state.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can execute a run once, that is, immediately after its creation, or on a recurring basis. Recurring runs consist of a copy of a pipeline with all of its parameter values and a run trigger. A run trigger indicates when a recurring run executes. You can define the following run triggers:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Periodic: used for scheduling runs to execute in intervals.</p>\n</li>\n<li>\n<p>Cron: used for scheduling runs as a cron job.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>You can also configure multiple instances of the same run to execute concurrently, from a range of one to ten. When executed, you can track the run&#8217;s progress from the run <strong>Details</strong> page on the {productname-short} user interface. From here, you can view the run&#8217;s graph, and output artifacts. A pipeline run can be in one of the following states:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Scheduled run: A pipeline run scheduled to execute at least once.</p>\n</li>\n<li>\n<p>Active run: A pipeline run that is in its execution phase, or is stopped.</p>\n</li>\n<li>\n<p>Archived run: A pipeline run that resides in the run archive and is no longer required.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>You can use catch up runs to ensure your pipeline runs do not permanently fall behind schedule when paused. For example, if you re-enable a paused recurring run, the run scheduler backfills each missed run interval. If you disable catch up runs, and you have a scheduled run interval ready to execute, the run scheduler only schedules the run execution for the latest run interval. Catch up runs are enabled by default. However, if your pipeline handles backfill internally, Red Hat recommends that you disable catch up runs to avoid duplicate backfill.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can review and analyze logs for each step in an active pipeline run. With the log viewer, you can search for specific log messages, view the log for each step, and download the step logs to your local machine.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-active-pipeline-runs_ds-pipelines\">Viewing active pipeline runs</h3>\n<div class=\"paragraph _abstract\">\n<p>You can view a list of pipeline runs that were previously executed in {productname-short}. From this list, you can view details relating to your pipeline runs, such as the pipeline version that the run belongs to, along with the run status, duration, and execution start time.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and has a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have previously executed a pipeline run.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, select the <strong>project</strong> for the active pipeline runs that you want to view.</p>\n</li>\n<li>\n<p>On the <strong>Run details</strong> page, click the <strong>Active</strong> tab.</p>\n<div class=\"paragraph\">\n<p>After a run has completed its execution, the run&#8217;s status appears in the <strong>Status</strong> column in the table, indicating whether the run has succeeded or failed.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>A list of active runs appears in the <strong>Active</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"executing-a-pipeline-run_ds-pipelines\">Executing a pipeline run</h3>\n<div class=\"paragraph _abstract\">\n<p>You can instantiate a single execution of a pipeline by creating an active pipeline run that executes immediately after creation.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, select the <strong>project</strong> that you want to create a run for.</p>\n</li>\n<li>\n<p>Click <strong>Create run</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Create run</strong> page, configure the run:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Name</strong> field, enter a name for the run.</p>\n</li>\n<li>\n<p>In the <strong>Description</strong> field, enter a description for the run.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline</strong> list, select the pipeline that you want to create a run for. Alternatively, to create a new pipeline, click <strong>Create new pipeline</strong> and complete the relevant fields in the <strong>Import pipeline</strong> dialog.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline version</strong> list, select the pipeline version to create a run for. Alternatively, to upload a new version, click <strong>Upload new version</strong> and complete the relevant fields in the <strong>Upload new version</strong> dialog.</p>\n</li>\n<li>\n<p>Configure the input parameters for the run by selecting the parameters from the list.</p>\n</li>\n<li>\n<p>Click <strong>Create run</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline run that you created appears in the <strong>Active</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"stopping-an-active-pipeline-run_ds-pipelines\">Stopping an active pipeline run</h3>\n<div class=\"paragraph _abstract\">\n<p>If you no longer require an active pipeline run to continue executing, you can stop the run before its defined end date.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n<li>\n<p>There is a previously created data science project available that contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>An active pipeline run is currently executing.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, select the <strong>project</strong> that contains the pipeline whose active run you want to stop.</p>\n</li>\n<li>\n<p>In the <strong>Active</strong> tab, click the action menu (<strong>&#8942;</strong>) beside the active run that you want to delete and click <strong>Stop</strong>.</p>\n<div class=\"paragraph\">\n<p>There might be a short delay while the run stops.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>In the list of active runs, the status of the run is \"stopped\".</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"duplicating-an-active-pipeline-run_ds-pipelines\">Duplicating an active pipeline run</h3>\n<div class=\"paragraph _abstract\">\n<p>To make it easier to quickly execute pipeline runs with the same configuration, you can duplicate them.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>An active run is available to duplicate in the <strong>Active</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, select the <strong>project</strong> that has the pipeline run that you want to duplicate.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the relevant active run and click <strong>Duplicate</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Duplicate run</strong> page, configure the duplicate run:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Name</strong> field, enter a name for the duplicate run.</p>\n</li>\n<li>\n<p>In the <strong>Description</strong> field, enter a description for the duplicate run.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline</strong> list, select the pipeline to contain the duplicate run.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline version</strong> list, select the pipeline version to contain the duplicate run.</p>\n</li>\n<li>\n<p>In the <strong>Parameters</strong> section, configure the input parameters for the run that you are duplicating by selecting the appropriate parameters from the list.</p>\n</li>\n<li>\n<p>Click <strong>Create run</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The duplicate pipeline run appears in the <strong>Active</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-scheduled-pipeline-runs_ds-pipelines\">Viewing scheduled pipeline runs</h3>\n<div class=\"paragraph _abstract\">\n<p>You can view a list of pipeline runs that are scheduled for execution in {productname-short}. From this list, you can view details relating to your pipeline runs, such as the pipeline version that the run belongs to. You can also view the run status, execution frequency, and schedule.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have scheduled a pipeline run that is available to view.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Runs</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project whose scheduled pipeline runs you want to view.</p>\n</li>\n<li>\n<p>Click the <strong>Schedules</strong> tab.</p>\n</li>\n<li>\n<p>Study the table showing a list of scheduled runs.</p>\n<div class=\"paragraph\">\n<p>After a run has been scheduled, the run&#8217;s status indicates whether the run is ready for execution or unavailable for execution. To change its execution availability, click the run&#8217;s Status icon.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>A list of scheduled runs appears in the <strong>Schedules</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"scheduling-a-pipeline-run-using-a-cron-job_ds-pipelines\">Scheduling a pipeline run using a cron job</h3>\n<div class=\"paragraph _abstract\">\n<p>You can use a cron job to schedule a pipeline run to execute at a specific time. Cron jobs are useful for creating periodic and recurring tasks, and can also schedule individual tasks for a specific time, such as if you want to schedule a run for a low activity period. To successfully execute runs in {productname-short}, you must use the supported format. See <a href=\"https://pkg.go.dev/github.com/robfig/cron#hdr-CRON_Expression_Format\">Cron Expression Format</a> for more information.</p>\n</div>\n<div class=\"paragraph\">\n<p>The following examples show the correct format:</p>\n</div>\n<table class=\"tableblock frame-all grid-all stretch\">\n<colgroup>\n<col style=\"width: 50%;\">\n<col style=\"width: 50%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Run occurrence</th>\n<th class=\"tableblock halign-left valign-top\">Cron format</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Every five minutes</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">@every 5m</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Every 10 minutes</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">0 */10 * * * *</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Daily at 16:16 UTC</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">0 16 16 * * *</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Daily every quarter of the hour</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">0 0,15,30,45 * * * *</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">On Monday and Tuesday at 15:40 UTC</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">0 40 15 * * MON,TUE</p></td>\n</tr>\n</tbody>\n</table>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://pkg.go.dev/github.com/robfig/cron#hdr-CRON_Expression_Format\">Cron Expression Format</a></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"scheduling-a-pipeline-run_ds-pipelines\">Scheduling a pipeline run</h3>\n<div class=\"paragraph _abstract\">\n<p>To repeatedly run a pipeline, you can create a scheduled pipeline run.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Runs</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project that you want to schedule a run for.</p>\n</li>\n<li>\n<p>Click the <strong>Schedules</strong> tab.</p>\n</li>\n<li>\n<p>Click <strong>Schedule run</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Schedule run</strong> page, configure the run that you are scheduling:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Name</strong> field, enter a name for the run.</p>\n</li>\n<li>\n<p>In the <strong>Description</strong> field, enter a description for the run.</p>\n</li>\n<li>\n<p>From the <strong>Trigger type</strong> list, select one of the following options:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Select <strong>Periodic</strong> to specify an execution frequency. In the <strong>Run every</strong> field, enter a numerical value and select an execution frequency from the list.</p>\n</li>\n<li>\n<p>Select <strong>Cron</strong> to specify the execution schedule in <code>cron</code> format. This creates a cron job to execute the run. Click the <strong>Copy</strong> button (<span class=\"image\"><img src=\"/static/docs/images/osd-copy.png\" alt=\"osd copy\"></span>) to copy the cron job schedule to the clipboard. The field furthest to the left represents seconds. For more information about scheduling tasks using the supported <code>cron</code> format, see <a href=\"https://pkg.go.dev/github.com/robfig/cron#hdr-CRON_Expression_Format\">Cron Expression Format</a>.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>In the <strong>Maximum concurrent runs</strong> field, specify the number of runs that can execute concurrently, from a range of one to ten.</p>\n</li>\n<li>\n<p>For <strong>Start date</strong>, specify a start date for the run. Select a start date using the <strong>Calendar</strong> and the start time from the list of times.</p>\n</li>\n<li>\n<p>For <strong>End date</strong>, specify an end date for the run. Select an end date using the <strong>Calendar</strong> and the end time from the list of times.</p>\n</li>\n<li>\n<p>For <strong>Catch up</strong>, enable or disable catch up runs. You can use catch up runs to ensure your pipeline runs do not permanently fall behind schedule when paused. For example, if you re-enable a paused recurring run, the run scheduler backfills each missed run interval.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline</strong> list, select the pipeline that you want to create a run for. Alternatively, to create a new pipeline, click <strong>Create new pipeline</strong> and complete the relevant fields in the <strong>Import pipeline</strong> dialog.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline version</strong> list, select the pipeline version to create a run for. Alternatively, to upload a new version, click <strong>Upload new version</strong> and complete the relevant fields in the <strong>Upload new version</strong> dialog.</p>\n</li>\n<li>\n<p>Configure the input parameters for the run by selecting the parameters from the list.</p>\n</li>\n<li>\n<p>Click <strong>Schedule run</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline run that you created appears in the <strong>Schedules</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"duplicating-a-scheduled-pipeline-run_ds-pipelines\">Duplicating a scheduled pipeline run</h3>\n<div class=\"paragraph _abstract\">\n<p>To make it easier to schedule runs to execute as part of your pipeline configuration, you can duplicate existing scheduled runs.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>A scheduled run is available to duplicate in the <strong>Schedules</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, select the <strong>project</strong> that has the pipeline run that you want to duplicate.</p>\n</li>\n<li>\n<p>Click the <strong>Schedules</strong> tab.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the run that you want to duplicate and click <strong>Duplicate</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Duplicate schedule</strong> page, configure the duplicate run:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Name</strong> field, enter a name for the duplicate run.</p>\n</li>\n<li>\n<p>In the <strong>Description</strong> field, enter a description for the duplicate run.</p>\n</li>\n<li>\n<p>From the <strong>Trigger type</strong> list, select one of the following options:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Select <strong>Periodic</strong> to specify an execution frequency. In the <strong>Run every</strong> field, enter a numerical value and select an execution frequency from the list.</p>\n</li>\n<li>\n<p>Select <strong>Cron</strong> to specify the execution schedule in <code>cron</code> format. This creates a cron job to execute the run. Click the <strong>Copy</strong> button (<span class=\"image\"><img src=\"/static/docs/images/osd-copy.png\" alt=\"osd copy\"></span>) to copy the cron job schedule to the clipboard. The field furthest to the left represents seconds. For more information about scheduling tasks using the supported <code>cron</code> format, see <a href=\"https://pkg.go.dev/github.com/robfig/cron#hdr-CRON_Expression_Format\">Cron Expression Format</a>.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>For <strong>Maximum concurrent runs</strong>, specify the number of runs that can execute concurrently, from a range of one to ten.</p>\n</li>\n<li>\n<p>For <strong>Start date</strong>, specify a start date for the duplicate run. Select a start date using the <strong>Calendar</strong> and the start time from the list of times.</p>\n</li>\n<li>\n<p>For <strong>End date</strong>, specify an end date for the duplicate run. Select an end date using the <strong>Calendar</strong> and the end time from the list of times.</p>\n</li>\n<li>\n<p>For <strong>Catch up</strong>, enable or disable catch up runs. You can use catch up runs to ensure your pipeline runs do not permanently fall behind schedule when paused. For example, if you re-enable a paused recurring run, the run scheduler backfills each missed run interval.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline</strong> list, select the pipeline that you want to create a duplicate run for. Alternatively, to create a new pipeline, click <strong>Create new pipeline</strong> and complete the relevant fields in the <strong>Import pipeline</strong> dialog.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline version</strong> list, select the pipeline version to create a duplicate run for. Alternatively, to upload a new version, click <strong>Upload new version</strong> and complete the relevant fields in the <strong>Upload new version</strong> dialog.</p>\n</li>\n<li>\n<p>Configure the input parameters for the run by selecting the parameters from the list.</p>\n</li>\n<li>\n<p>Click <strong>Schedule run</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline run that you duplicated appears in the <strong>Schedules</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"deleting-a-scheduled-pipeline-run_ds-pipelines\">Deleting a scheduled pipeline run</h3>\n<div class=\"paragraph _abstract\">\n<p>To discard pipeline runs that you previously scheduled, but no longer require, you can delete them so that they do not appear on the <strong>Runs</strong> page.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have previously scheduled a run that is available to delete.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Runs</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>From the <strong>Project</strong> list, select the project that contains the pipeline whose scheduled run you want to delete.</p>\n<div class=\"paragraph\">\n<p>The page refreshes to show the pipeline&#8217;s scheduled runs on the <strong>Schedules</strong> tab.</p>\n</div>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the scheduled run that you want to delete and click <strong>Delete</strong>.</p>\n<div class=\"paragraph\">\n<p>The <strong>Delete schedule</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Enter the run&#8217;s name in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The run that you deleted no longer appears on the <strong>Schedules</strong> tab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-the-details-of-a-pipeline-run_ds-pipelines\">Viewing the details of a pipeline run</h3>\n<div class=\"paragraph _abstract\">\n<p>To gain a clearer understanding of your pipeline runs, you can view the details of a previously triggered pipeline run, such as its graph, execution details, and run output.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have previously triggered a pipeline run.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Pipelines</strong> page, select the <strong>project</strong> that you want to view run details for.</p>\n</li>\n<li>\n<p>For a pipeline that you want to view run details for, click <strong>Expand</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhoai-expand-icon.png\" alt=\"rhoai expand icon\"></span>).</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) for the pipeline version and then click <strong>View runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, click the name of the run that you want to view the details of.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>On the <strong>Run details</strong> page, you can view the run&#8217;s graph, execution details, input parameters, step logs, and run output.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-archived-pipeline-runs_ds-pipelines\">Viewing archived pipeline runs</h3>\n<div class=\"paragraph _abstract\">\n<p>You can view a list of pipeline runs that you have archived. You can view details for your archived pipeline runs, such as the pipeline version, run status, duration, and execution start date.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and has a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>An archived pipeline run exists.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, select the <strong>project</strong> for the archived pipeline runs that you want to view.</p>\n</li>\n<li>\n<p>Click the <strong>Archived</strong> tab.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>A list of archived runs appears in the <strong>Archived</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"archiving-a-pipeline-run_ds-pipelines\">Archiving a pipeline run</h3>\n<div class=\"paragraph _abstract\">\n<p>You can retain records of your pipeline runs by archiving them. If required, you can restore runs from your archive to reuse, or delete runs that are no longer required.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and has a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have previously executed a pipeline run that is available.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, select the project for the pipeline run that you want to archive from the <strong>Project</strong> list.</p>\n</li>\n<li>\n<p>On the <strong>Run details</strong> page, click the action menu (<strong>&#8942;</strong>) beside the run that you want to archive and then click <strong>Archive</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Archiving run</strong> dialog, enter the run name in the text field to confirm that you intend to archive it.</p>\n</li>\n<li>\n<p>Click <strong>Archive</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The archived run does not appear in the <strong>Active</strong> tab and instead appears in the <strong>Archived</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"restoring-an-archived-pipeline-run_ds-pipelines\">Restoring an archived pipeline run</h3>\n<div class=\"paragraph _abstract\">\n<p>You can restore an archived run to the active state.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and has a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>An archived run exists in your project.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, select the <strong>project</strong> for the archived pipeline run that you want to restore.</p>\n</li>\n<li>\n<p>On the <strong>Run details</strong> page, click the <strong>Archived</strong> tab.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the run that you want to restore and click <strong>Restore</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Restore run</strong> dialog, enter the run name in the text field to confirm that you intend to restore it.</p>\n</li>\n<li>\n<p>Click <strong>Restore</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The restored run appears in the <strong>Active</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"deleting-an-archived-pipeline-run_ds-pipelines\">Deleting an archived pipeline run</h3>\n<div class=\"paragraph _abstract\">\n<p>You can delete pipeline runs from the {productname-short} run archive.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and has a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have previously archived a pipeline run.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, select the <strong>project</strong> for the archived pipeline run you want to delete.</p>\n</li>\n<li>\n<p>In the <strong>Run details</strong> page, click <strong>Archived</strong>.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the run that you want to delete and click <strong>Delete</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Delete run</strong> dialog, enter the run name in the text field to confirm that you intend to delete it.</p>\n</li>\n<li>\n<p>Click <strong>Delete</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The archived run that you deleted no longer appears in the <strong>Archived</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"duplicating-an-archived-pipeline-run_ds-pipelines\">Duplicating an archived pipeline run</h3>\n<div class=\"paragraph _abstract\">\n<p>To make it easier to reproduce runs with the same configuration as runs in your archive, you can duplicate them.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a configured pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>An archived run is available to duplicate in the <strong>Archived</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, select the <strong>project</strong> that has the pipeline run that you want to duplicate.</p>\n</li>\n<li>\n<p>Click the <strong>Archived</strong> tab.</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) beside the relevant archived run and click <strong>Duplicate</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Duplicate run</strong> page, configure the duplicate run:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Name</strong> field, enter a name for the duplicate run.</p>\n</li>\n<li>\n<p>In the <strong>Description</strong> field, enter a description for the duplicate run.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline</strong> list, select the pipeline to contain the duplicate run.</p>\n</li>\n<li>\n<p>From the <strong>Pipeline version</strong> list, select the pipeline version to contain the duplicate run.</p>\n</li>\n<li>\n<p>In the <strong>Parameters</strong> section, configure the input parameters for the run that you are duplicating by selecting the appropriate parameters from the list.</p>\n</li>\n<li>\n<p>Click <strong>Create run</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The duplicate pipeline run appears in the <strong>Active</strong> tab on the <strong>Runs</strong> page.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"_working_with_pipeline_logs\">Working with pipeline logs</h2>\n<div class=\"sectionbody\">\n<div class=\"sect2\">\n<h3 id=\"about-pipeline-logs_ds-pipelines\">About pipeline logs</h3>\n<div class=\"paragraph _abstract\">\n<p>You can review and analyze step logs for each step in a triggered pipeline run.</p>\n</div>\n<div class=\"paragraph\">\n<p>To help you troubleshoot and audit your pipelines, you can review and analyze these step logs by using the log viewer in the {productname-short} dashboard. From here, you can search for specific log messages, view the log for each step, and download the step logs to your local machine.</p>\n</div>\n<div class=\"paragraph\">\n<p>If the step log file exceeds its capacity, a warning appears above the log viewer stating that the log window displays partial content. Expanding the warning displays further information, such as how the log viewer refreshes every three seconds, and that each step log displays the last 500 lines of log messages received. In addition, you can click <strong>download all step logs</strong> to download all step logs to your local machine.</p>\n</div>\n<div class=\"paragraph\">\n<p>Each step has a set of container logs. You can view these container logs by selecting a container from the <strong>Steps</strong> list in the log viewer. The <code>Step-main</code> container log consists of the log output for the step. The <code>step-copy-artifact</code> container log consists of output relating to artifact data sent to s3-compatible storage. If the data transferred between the steps in your pipeline is larger than 3 KB, five container logs are typically available. These logs contain output relating to data transferred between your persistent volume claims (PVCs).</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-pipeline-step-logs_ds-pipelines\">Viewing pipeline step logs</h3>\n<div class=\"paragraph _abstract\">\n<p>To help you troubleshoot and audit your pipelines, you can review and analyze the log of each pipeline step using the log viewer. From here, you can search for specific log messages and download the logs for each step in your pipeline. If the pipeline is running, you can also pause and resume the log from the log viewer.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have previously triggered a pipeline run.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Pipelines</strong> page, select the <strong>project</strong> that you want to view logs for.</p>\n</li>\n<li>\n<p>For the pipeline that you want to view logs for, click <strong>Expand</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhoai-expand-icon.png\" alt=\"rhoai expand icon\"></span>).</p>\n</li>\n<li>\n<p>Click the action menu (<strong>&#8942;</strong>) on the row containing the project version that you want to view pipeline logs for and click <strong>View runs</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, click the name of the run that you want to view logs for.</p>\n</li>\n<li>\n<p>On the graph on the <strong>Run details</strong> page , click the pipeline step that you want to view logs for.</p>\n</li>\n<li>\n<p>Click the <strong>Logs</strong> tab.</p>\n</li>\n<li>\n<p>To view the logs of another pipeline step, from the <strong>Steps</strong> list, select the step that you want to view logs for.</p>\n</li>\n<li>\n<p>Analyze the log using the log viewer.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>To search for a specific log message, enter at least part of the message in the search bar.</p>\n</li>\n<li>\n<p>To view the full log in a separate browser window, click the action menu (&#8942;) and select <strong>View raw logs</strong>. Alternatively, to expand the size of the log viewer, click the action menu (&#8942;) and select <strong>Expand</strong>.</p>\n</li>\n</ul>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>You can view the logs for each step in your pipeline.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"downloading-pipeline-step-logs_ds-pipelines\">Downloading pipeline step logs</h3>\n<div class=\"paragraph _abstract\">\n<p>Instead of viewing the step logs of a pipeline run using the log viewer on the {productname-short} dashboard, you can download them for further analysis. You can choose to download the logs belonging to all steps in your pipeline, or you can download the log only for the step log displayed in the log viewer.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n<li>\n<p>You have previously created a data science project that is available and contains a pipeline server.</p>\n</li>\n<li>\n<p>You have imported a pipeline to an active pipeline server.</p>\n</li>\n<li>\n<p>You have previously triggered a pipeline run.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the {productname-short} dashboard, click <strong>Data Science Pipelines</strong> &#8594; <strong>Pipelines</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Pipelines</strong> page, select the <strong>project</strong> that you want to download logs for.</p>\n</li>\n<li>\n<p>For the pipeline that you want to download logs for, click <strong>Expand</strong> (<span class=\"image\"><img src=\"/static/docs/images/rhoai-expand-icon.png\" alt=\"rhoai expand icon\"></span>).</p>\n</li>\n<li>\n<p>Click <strong>View runs</strong> on the row containing the pipeline version that you want to download logs for.</p>\n</li>\n<li>\n<p>On the <strong>Runs</strong> page, click the name of the run that you want to download logs for.</p>\n</li>\n<li>\n<p>On the graph on the <strong>Run details</strong> page, click the pipeline step that you want to download logs for.</p>\n</li>\n<li>\n<p>Click the <strong>Logs</strong> tab.</p>\n</li>\n<li>\n<p>In the log viewer, click the <strong>Download</strong> button (<span class=\"image\"><img src=\"/static/docs/images/rhoai-download-icon.png\" alt=\"rhoai download icon\"></span>).</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Select <strong>Download current stop log</strong> to download the log for the current pipeline step.</p>\n</li>\n<li>\n<p>Select <strong>Download all step logs</strong> to download the logs for all steps in your pipeline run.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The step logs download to your browser&#8217;s default directory for downloaded files.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"_working_with_pipelines_in_jupyterlab\">Working with pipelines in JupyterLab</h2>\n<div class=\"sectionbody\">\n<div class=\"sect2\">\n<h3 id=\"overview-of-pipelines-in-jupyterlab_ds-pipelines\">Overview of pipelines in JupyterLab</h3>\n<div class=\"paragraph _abstract\">\n<p>You can use Elyra to create visual end-to-end pipeline workflows in JupyterLab. Elyra is an extension for JupyterLab that provides you with a Pipeline Editor to create pipeline workflows that can be executed in {productname-short}.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can access the Elyra extension within JupyterLab when you create the most recent version of one of the following notebook images:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Standard Data Science</p>\n</li>\n<li>\n<p>PyTorch</p>\n</li>\n<li>\n<p>TensorFlow</p>\n</li>\n<li>\n<p>TrustyAI</p>\n</li>\n<li>\n<p>HabanaAI</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>When you use the Pipeline Editor to visually design your pipelines, minimal coding is required to create and run pipelines. For more information about Elyra, see <a href=\"https://elyra.readthedocs.io/en/stable/getting_started/overview.html\">Elyra Documentation</a>. For more information about the Pipeline Editor, see <a href=\"https://elyra.readthedocs.io/en/stable/user_guide/jupyterlab-interface.html#visual-pipeline-editor\">Visual Pipeline Editor</a>. After you have created your pipeline, you can run it locally in JupyterLab, or remotely using data science pipelines in {productname-short}.</p>\n</div>\n<div class=\"paragraph\">\n<p>The pipeline creation process consists of the following tasks:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Create a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>Create a pipeline server.</p>\n</li>\n<li>\n<p>Create a new pipeline in the Pipeline Editor in JupyterLab.</p>\n</li>\n<li>\n<p>Develop your pipeline by adding Python notebooks or Python scripts and defining their runtime properties.</p>\n</li>\n<li>\n<p>Define execution dependencies.</p>\n</li>\n<li>\n<p>Run or export your pipeline.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>Before you can run a pipeline in JupyterLab, your pipeline instance must contain a runtime configuration. A runtime configuration defines connectivity information for your pipeline instance and S3-compatible cloud storage.</p>\n</div>\n<div class=\"paragraph\">\n<p>If you create a workbench as part of a data science project, a default runtime configuration is created automatically. However, if you create a notebook from the Jupyter tile in the {productname-short} dashboard, you must create a runtime configuration before you can run your pipeline in JupyterLab. For more information about runtime configurations, see <a href=\"https://elyra.readthedocs.io/en/stable/user_guide/runtime-conf.html\">Runtime Configuration</a>. As a prerequisite, before you create a workbench, ensure that you have created and configured a pipeline server within the same data science project as your workbench.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can use S3-compatible cloud storage to make data available to your notebooks and scripts while they are executed. Your cloud storage must be accessible from the machine in your deployment that runs JupyterLab and from the cluster that hosts Data Science Pipelines. Before you create and run pipelines in JupyterLab, ensure that you have your s3-compatible storage credentials readily available.</p>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://elyra.readthedocs.io/en/stable/getting_started/overview.html\">Elyra Documentation</a></p>\n</li>\n<li>\n<p><a href=\"https://elyra.readthedocs.io/en/stable/user_guide/jupyterlab-interface.html#visual-pipeline-editor\">Visual Pipeline Editor</a></p>\n</li>\n<li>\n<p><a href=\"https://elyra.readthedocs.io/en/stable/user_guide/runtime-conf.html\">Runtime Configuration</a>.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"accessing-the-pipeline-editor_ds-pipelines\">Accessing the pipeline editor</h3>\n<div class=\"paragraph _abstract\">\n<p>You can use Elyra to create visual end-to-end pipeline workflows in JupyterLab. Elyra is an extension for JupyterLab that provides you with a Pipeline Editor to create pipeline workflows that can execute in {productname-short}.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project.</p>\n</li>\n<li>\n<p>You have created a workbench with the <strong>Standard Data Science</strong> notebook image.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, PyTorch, or HabanaAI).</p>\n</li>\n<li>\n<p>You have access to S3-compatible storage.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>After you open JupyterLab, confirm that the JupyterLab launcher is automatically displayed.</p>\n</li>\n<li>\n<p>In the <strong>Elyra</strong> section of the JupyterLab launcher, click the <strong>Pipeline Editor</strong> tile.</p>\n<div class=\"paragraph\">\n<p>The Pipeline Editor opens.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>You can view the Pipeline Editor in JupyterLab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"creating-a-runtime-configuration_ds-pipelines\">Creating a runtime configuration</h3>\n<div class=\"paragraph _abstract\">\n<p>If you create a workbench as part of a data science project, a default runtime configuration is created automatically. However, if you create a notebook from the Jupyter tile in the {productname-short} dashboard, you must create a runtime configuration before you can run your pipeline in JupyterLab. This enables you to specify connectivity information for your pipeline instance and S3-compatible cloud storage.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n<li>\n<p>You have access to S3-compatible cloud storage.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, PyTorch, or HabanaAI).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left sidebar of JupyterLab, click <strong>Runtimes</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>).</p>\n</li>\n<li>\n<p>Click the <strong>Create new runtime configuration</strong> button (<span class=\"image\"><img src=\"/static/docs/images/jupyter-create-runtime.png\" alt=\"Create new runtime configuration\"></span>).</p>\n<div class=\"paragraph\">\n<p>The <strong>Add new Data Science Pipelines runtime configuration</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Complete the relevant fields to define your runtime configuration.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Display Name</strong> field, enter a name for your runtime configuration.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Description</strong> field, enter a description to define your runtime configuration.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Tags</strong> field, click <strong>Add Tag</strong> to define a category for your pipeline instance. Enter a name for the tag and press Enter.</p>\n</li>\n<li>\n<p>Define the credentials of your data science pipeline:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint</strong> field, enter the API endpoint of your data science pipeline. Do not specify the pipelines namespace in this field.</p>\n</li>\n<li>\n<p>In the <strong>Public Data Science Pipelines API Endpoint</strong> field, enter the public API endpoint of your data science pipeline.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>You can obtain the Data Science Pipelines API endpoint from the <strong>Data Science Pipelines</strong> &#8594; <strong>Runs</strong> page in the dashboard. Copy the relevant end point and enter it in the <strong>Public Data Science Pipelines API Endpoint</strong> field.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Optional: In the <strong>Data Science Pipelines User Namespace</strong> field, enter the relevant user namespace to run pipelines.</p>\n</li>\n<li>\n<p>From the <strong>Authentication Type</strong> list, select the authentication type required to authenticate your pipeline.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you created a notebook directly from the Jupyter tile on the dashboard, select <code>EXISTING_BEARER_TOKEN</code> from the <strong>Authentication Type</strong> list.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint Username</strong> field, enter the user name required for the authentication type.</p>\n</li>\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint Password Or Token</strong>, enter the password or token required for the authentication type.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>To obtain the Data Science Pipelines API endpoint token, in the upper-right corner of the OpenShift web console, click your user name and select <strong>Copy login command</strong>. After you have logged in, click <strong>Display token</strong> and copy the value of <code>--token=</code> from the <strong>Log in with this token</strong> command.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Define the connectivity information of your S3-compatible storage:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Cloud Object Storage Endpoint</strong> field, enter the endpoint of your S3-compatible storage. For more information about Amazon s3 endpoints, see <a href=\"https://docs.aws.amazon.com/general/latest/gr/s3.html\">Amazon Simple Storage Service endpoints and quotas</a>.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Public Cloud Object Storage Endpoint</strong> field, enter the URL of your S3-compatible storage.</p>\n</li>\n<li>\n<p>In the <strong>Cloud Object Storage Bucket Name</strong> field, enter the name of the bucket where your pipeline artifacts are stored. If the bucket name does not exist, it is created automatically.</p>\n</li>\n<li>\n<p>From the <strong>Cloud Object Storage Authentication Type</strong> list, select the authentication type required to access to your S3-compatible cloud storage. If you use AWS S3 buckets, select <code>KUBERNETES_SECRET</code> from the list.</p>\n</li>\n<li>\n<p>In the <strong>Cloud Object Storage Credentials Secret</strong> field, enter the secret that contains the storage user name and password. This secret is defined in the relevant user namespace, if applicable. In addition, it must be stored on the cluster that hosts your pipeline runtime.</p>\n</li>\n<li>\n<p>In the <strong>Cloud Object Storage Username</strong> field, enter the user name to connect to your S3-compatible cloud storage, if applicable. If you use AWS S3 buckets, enter your AWS Secret Access Key ID.</p>\n</li>\n<li>\n<p>In the <strong>Cloud Object Storage Password</strong> field, enter the password to connect to your S3-compatible cloud storage, if applicable. If you use AWS S3 buckets, enter your AWS Secret Access Key.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Save &amp; Close</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The runtime configuration that you created is shown in the <strong>Runtimes</strong> tab (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>) in the left sidebar of JupyterLab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"updating-a-runtime-configuration_ds-pipelines\">Updating a runtime configuration</h3>\n<div class=\"paragraph _abstract\">\n<p>To ensure that your runtime configuration is accurate and updated, you can change the settings of an existing runtime configuration.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n<li>\n<p>You have access to S3-compatible storage.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>A previously created runtime configuration is available in the JupyterLab interface.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, PyTorch, or HabanaAI).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left sidebar of JupyterLab, click <strong>Runtimes</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>).</p>\n</li>\n<li>\n<p>Hover the cursor over the runtime configuration that you want to update and click the <strong>Edit</strong> button (<span class=\"image\"><img src=\"/static/docs/images/rhoai-edit-icon.png\" alt=\"Edit runtime configuration\"></span>).</p>\n<div class=\"paragraph\">\n<p>The <strong>Data Science Pipelines runtime configuration</strong> page opens.</p>\n</div>\n</li>\n<li>\n<p>Fill in the relevant fields to update your runtime configuration.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Display Name</strong> field, update name for your runtime configuration, if applicable.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Description</strong> field, update the description of your runtime configuration, if applicable.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Tags</strong> field, click <strong>Add Tag</strong> to define a category for your pipeline instance. Enter a name for the tag and press Enter.</p>\n</li>\n<li>\n<p>Define the credentials of your data science pipeline:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint</strong> field, update the API endpoint of your data science pipeline, if applicable. Do not specify the pipelines namespace in this field.</p>\n</li>\n<li>\n<p>In the <strong>Public Data Science Pipelines API Endpoint</strong> field, update the API endpoint of your data science pipeline, if applicable.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Data Science Pipelines User Namespace</strong> field, update the relevant user namespace to run pipelines, if applicable.</p>\n</li>\n<li>\n<p>From the <strong>Authentication Type</strong> list, select a new authentication type required to authenticate your pipeline, if applicable.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you created a notebook directly from the Jupyter tile on the dashboard, select <code>EXISTING_BEARER_TOKEN</code> from the <strong>Authentication Type</strong> list.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint Username</strong> field, update the user name required for the authentication type, if applicable.</p>\n</li>\n<li>\n<p>In the <strong>Data Science Pipelines API Endpoint Password Or Token</strong>, update the password or token required for the authentication type, if applicable.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>To obtain the Data Science Pipelines API endpoint token, in the upper-right corner of the OpenShift web console, click your user name and select <strong>Copy login command</strong>. After you have logged in, click <strong>Display token</strong> and copy the value of <code>--token=</code> from the <strong>Log in with this token</strong> command.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Define the connectivity information of your S3-compatible storage:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p>In the <strong>Cloud Object Storage Endpoint</strong> field, update the endpoint of your S3-compatible storage, if applicable. For more information about Amazon s3 endpoints, see <a href=\"https://docs.aws.amazon.com/general/latest/gr/s3.html\">Amazon Simple Storage Service endpoints and quotas</a>.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Public Cloud Object Storage Endpoint</strong> field, update the URL of your S3-compatible storage, if applicable.</p>\n</li>\n<li>\n<p>In the <strong>Cloud Object Storage Bucket Name</strong> field, update the name of the bucket where your pipeline artifacts are stored, if applicable. If the bucket name does not exist, it is created automatically.</p>\n</li>\n<li>\n<p>From the <strong>Cloud Object Storage Authentication Type</strong> list, update the authentication type required to access to your S3-compatible cloud storage, if applicable. If you use AWS S3 buckets, you must select <code>USER_CREDENTIALS</code> from the list.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Cloud Object Storage Credentials Secret</strong> field, update the secret that contains the storage user name and password, if applicable. This secret is defined in the relevant user namespace. You must save the secret on the cluster that hosts your pipeline runtime.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Cloud Object Storage Username</strong> field, update the user name to connect to your S3-compatible cloud storage, if applicable. If you use AWS S3 buckets, update your AWS Secret Access Key ID.</p>\n</li>\n<li>\n<p>Optional: In the <strong>Cloud Object Storage Password</strong> field, update the password to connect to your S3-compatible cloud storage, if applicable. If you use AWS S3 buckets, update your AWS Secret Access Key.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Save &amp; Close</strong>.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The runtime configuration that you updated is shown in the <strong>Runtimes</strong> tab (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>) in the left sidebar of JupyterLab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"deleting-a-runtime-configuration_ds-pipelines\">Deleting a runtime configuration</h3>\n<div class=\"paragraph _abstract\">\n<p>After you have finished using your runtime configuration, you can delete it from the JupyterLab interface. After deleting a runtime configuration, you cannot run pipelines in JupyterLab until you create another runtime configuration.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>A previously created runtime configuration is visible in the JupyterLab interface.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, PyTorch, or HabanaAI).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left sidebar of JupyterLab, click <strong>Runtimes</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>).</p>\n</li>\n<li>\n<p>Hover the cursor over the runtime configuration that you want to delete and click the <strong>Delete Item</strong> button (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-trash-button.png\" alt=\"Delete item\"></span>).</p>\n<div class=\"paragraph\">\n<p>A dialog box appears prompting you to confirm the deletion of your runtime configuration.</p>\n</div>\n</li>\n<li>\n<p>Click <strong>OK</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The runtime configuration that you deleted is no longer shown in the <strong>Runtimes</strong> tab (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>) in the left sidebar of JupyterLab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"duplicating-a-runtime-configuration_ds-pipelines\">Duplicating a runtime configuration</h3>\n<div class=\"paragraph _abstract\">\n<p>To prevent you from re-creating runtime configurations with similar values in their entirety, you can duplicate an existing runtime configuration in the JupyterLab interface.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>A previously created runtime configuration is visible in the JupyterLab interface.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, PyTorch, or HabanaAI).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the left sidebar of JupyterLab, click <strong>Runtimes</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>).</p>\n</li>\n<li>\n<p>Hover the cursor over the runtime configuration that you want to duplicate and click the <strong>Duplicate</strong> button (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-duplicate.png\" alt=\"Duplicate\"></span>).</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The runtime configuration that you duplicated is shown in the <strong>Runtimes</strong> tab (<span class=\"image\"><img src=\"/static/docs/images/jupyter-runtimes-sidebar.png\" alt=\"The Runtimes icon\"></span>) in the left sidebar of JupyterLab.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"running-a-pipeline-in-jupyterlab_ds-pipelines\">Running a pipeline in JupyterLab</h3>\n<div class=\"paragraph _abstract\">\n<p>You can run pipelines that you have created in JupyterLab from the Pipeline Editor user interface. Before you can run a pipeline, you must create a data science project and a pipeline server. After you create a pipeline server, you must create a workbench within the same project as your pipeline server.\nYour pipeline instance in JupyterLab must contain a runtime configuration. If you create a workbench as part of a data science project, a default runtime configuration is created automatically. However, if you create a notebook from the Jupyter tile in the {productname-short} dashboard, you must create a runtime configuration before you can run your pipeline in JupyterLab. A runtime configuration defines connectivity information for your pipeline instance and S3-compatible cloud storage.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n<li>\n<p>You have access to S3-compatible storage.</p>\n</li>\n<li>\n<p>You have created a pipeline in JupyterLab.</p>\n</li>\n<li>\n<p>You have opened your pipeline in the Pipeline Editor in JupyterLab.</p>\n</li>\n<li>\n<p>Your pipeline instance contains a runtime configuration.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, PyTorch, or HabanaAI).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the Pipeline Editor user interface, click <strong>Run Pipeline</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-run-pipeline-button.png\" alt=\"The Runtimes icon\"></span>).</p>\n<div class=\"paragraph\">\n<p>The <strong>Run Pipeline</strong> dialog appears. The <strong>Pipeline Name</strong> field is automatically populated with the pipeline file name.</p>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>You must enter a unique pipeline name. The pipeline name that you enter must not match the name of any previously executed pipelines.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Define the settings for your pipeline run.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>From the <strong>Runtime Configuration</strong> list, select the relevant runtime configuration to run your pipeline.</p>\n</li>\n<li>\n<p>Optional: Configure your pipeline parameters, if applicable. If your pipeline contains nodes that reference pipeline parameters, you can change the default parameter values. If a parameter is required and has no default value, you must enter a value.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>OK</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>You can view the output artifacts of your pipeline run. The artifacts are stored in your designated object storage bucket.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"exporting-a-pipeline-in-jupyterlab_ds-pipelines\">Exporting a pipeline in JupyterLab</h3>\n<div class=\"paragraph _abstract\">\n<p>You can export pipelines that you have created in JupyterLab. When you export a pipeline, the pipeline is prepared for later execution, but is not uploaded or executed immediately. During the export process, any package dependencies are uploaded to S3-compatible storage. Also, pipeline code is generated for the target runtime.</p>\n</div>\n<div class=\"paragraph\">\n<p>Before you can export a pipeline, you must create a data science project and a pipeline server. After you create a pipeline server, you must create a workbench within the same project as your pipeline server. In addition, your pipeline instance in JupyterLab must contain a runtime configuration. If you create a workbench as part of a data science project, a default runtime configuration is created automatically. However, if you create a notebook from the Jupyter tile in the {productname-short} dashboard, you must create a runtime configuration before you can export your pipeline in JupyterLab. A runtime configuration defines connectivity information for your pipeline instance and S3-compatible cloud storage.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.</p>\n</li>\n<li>\n<p>You have created a data science project that contains a workbench.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the data science project that contains your workbench.</p>\n</li>\n<li>\n<p>You have access to S3-compatible storage.</p>\n</li>\n<li>\n<p>You have a created a pipeline in JupyterLab.</p>\n</li>\n<li>\n<p>You have opened your pipeline in the Pipeline Editor in JupyterLab.</p>\n</li>\n<li>\n<p>Your pipeline instance contains a runtime configuration.</p>\n</li>\n<li>\n<p>You have created and launched a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, PyTorch, or HabanaAI).</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the Pipeline Editor user interface, click <strong>Export Pipeline</strong> (<span class=\"image\"><img src=\"/static/docs/images/jupyterlab-export-pipeline-button.png\" alt=\"Export pipeline\"></span>).</p>\n<div class=\"paragraph\">\n<p>The <strong>Export Pipeline</strong> dialog appears. The <strong>Pipeline Name</strong> field is automatically populated with the pipeline file name.</p>\n</div>\n</li>\n<li>\n<p>Define the settings to export your pipeline.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>From the <strong>Runtime Configuration</strong> list, select the relevant runtime configuration to export your pipeline.</p>\n</li>\n<li>\n<p>From the <strong>Export Pipeline as</strong> select an appropriate file format</p>\n</li>\n<li>\n<p>In the <strong>Export Filename</strong> field, enter a file name for the exported pipeline.</p>\n</li>\n<li>\n<p>Select the <strong>Replace if file already exists</strong> check box to replace an existing file of the same name as the pipeline you are exporting.</p>\n</li>\n<li>\n<p>Optional: Configure your pipeline parameters, if applicable. If your pipeline contains nodes that reference pipeline parameters, you can change the default parameter values. If a parameter is required and has no default value, you must enter a value.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>OK</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>You can view the file containing the pipeline that you exported in your designated object storage bucket.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1 _additional-resources\">\n<h2 id=\"_additional_resources\">Additional resources</h2>\n<div class=\"sectionbody\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p><a href=\"https://pypi.org/project/kfp/\">KubeFlow Pipelines SDK</a></p>\n</li>\n<li>\n<p><a href=\"https://www.kubeflow.org/docs/components/pipelines/v2/\">Kubeflow Pipelines 2.0 Documentation</a>\n<a href=\"{odhdocshome}/working-on-data-science-projects/working-with-data-science-pipelines_ds-pipelines#working_with_pipelines_in_jupyterlab\">Working with pipelines in JupyterLab</a>.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>","id":"650137f6-3c6a-5d64-821b-48d3f5246053","document":{"title":"Working with data science pipelines"}},"markdownRemark":null},"pageContext":{"id":"650137f6-3c6a-5d64-821b-48d3f5246053"}},"staticQueryHashes":["2604506565"],"slicesMap":{}}